<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>üåå</text></svg>">
    <title>Gravitation¬≥ - Math & Physics Tutorials</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="nav.css">
    <script defer src="../shared/browser-support.js"></script>
    <script defer src="../shared/site-nav.js"></script>
    
    <!-- MathJax Configuration -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
    
    <style>
        body { margin: 0; padding: 0; }
        .docs-container { min-height: calc(100vh - 70px); margin-top: 70px; padding: 4rem 2rem; background: radial-gradient(ellipse at center, #1a1f3a 0%, #0a0e27 100%); }
        .docs-content { max-width: 1200px; margin: 0 auto; }
        .docs-header { text-align: center; margin-bottom: 2rem; }
        .docs-title { font-size: 3rem; font-weight: 700; background: var(--gradient-accent); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; margin-bottom: 1rem; animation: fadeInUp 0.6s ease 0.3s both; }
        .docs-subtitle { font-size: 1.2rem; color: var(--text-secondary); animation: fadeInUp 0.6s ease 0.5s both; margin-bottom: 2rem; }
        
        /* Main Navigation Tabs */
        .tutorial-tabs {
            display: flex;
            gap: 0.5rem;
            justify-content: center;
            flex-wrap: wrap;
            padding: 1rem 0;
            margin-bottom: 2rem;
            animation: fadeInUp 0.6s ease 0.7s both;
        }
        
        .tutorial-tab {
            background: var(--bg-overlay, rgba(255, 255, 255, 0.05));
            border: 1px solid var(--border-color);
            border-radius: var(--radius-md);
            padding: 0.75rem 1.5rem;
            color: var(--text-secondary);
            cursor: pointer;
            transition: all 0.3s ease;
            font-size: 1rem;
            font-weight: 500;
        }
        
        .tutorial-tab:hover {
            background: var(--bg-overlay, rgba(255, 255, 255, 0.1));
            border-color: var(--accent-blue);
            color: var(--text-primary);
        }
        
        .tutorial-tab.active {
            background: var(--gradient-accent);
            border-color: var(--accent-blue);
            color: white;
            box-shadow: 0 4px 20px rgba(0, 212, 255, 0.3);
        }
        
        /* Content Sections */
        .tutorial-content {
            display: none;
            animation: fadeIn 0.5s ease;
        }
        
        .tutorial-content.active {
            display: block;
        }
        
        /* Accordion Subsections */
        .subsection-accordion {
            margin-bottom: 1.5rem;
        }
        
        .subsection-header {
            background: rgba(255, 255, 255, 0.08);
            border: 1px solid var(--border-color);
            border-radius: var(--radius-md);
            padding: 1rem 1.5rem;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
            transition: all 0.3s ease;
            margin-bottom: 0;
        }
        
        .subsection-header:hover {
            background: rgba(255, 255, 255, 0.12);
            border-color: var(--accent-blue);
        }
        
        .subsection-header.active {
            background: rgba(0, 212, 255, 0.15);
            border-color: var(--accent-blue);
            border-bottom-left-radius: 0;
            border-bottom-right-radius: 0;
            margin-bottom: 0;
        }
        
        .subsection-title {
            font-size: 1.3rem;
            font-weight: 600;
            color: var(--text-primary);
            display: flex;
            align-items: center;
            gap: 0.75rem;
        }
        
        .subsection-number {
            background: var(--gradient-accent);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: var(--radius-sm);
            font-size: 0.9rem;
            font-weight: 700;
        }
        
        .subsection-arrow {
            font-size: 1.5rem;
            color: var(--accent-blue);
            transition: transform 0.3s ease;
        }
        
        .subsection-header.active .subsection-arrow {
            transform: rotate(90deg);
        }
        
        .subsection-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.4s ease, opacity 0.3s ease;
            background: var(--bg-overlay, rgba(255, 255, 255, 0.05));
            border: 1px solid var(--border-color);
            border-top: none;
            border-radius: 0 0 var(--radius-md) var(--radius-md);
            opacity: 0;
        }
        
        .subsection-content.active {
            max-height: 50000px;
            opacity: 1;
        }
        
        .subsection-inner {
            padding: 2rem;
            animation: slideIn 0.4s ease;
        }
        
        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(-20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        .docs-section { margin-bottom: 2rem; }
        .docs-section h2 { font-size: 2rem; font-weight: 600; color: var(--text-primary); margin-bottom: 1rem; border-bottom: 2px solid var(--accent-blue); padding-bottom: 0.5rem; }
        .docs-section h3 { font-size: 1.5rem; font-weight: 600; color: var(--accent-blue); margin-top: 1.5rem; margin-bottom: 0.75rem; }
        .docs-section p { font-size: 1.05rem; color: var(--text-secondary); line-height: 1.8; margin-bottom: 1rem; }
        .docs-section ul { color: var(--text-secondary); line-height: 1.8; margin-left: 1.5rem; margin-bottom: 1rem; }
        .docs-section ol { color: var(--text-secondary); line-height: 1.8; margin-left: 2rem; margin-bottom: 1rem; padding-left: 0; }
        .docs-section li { margin-bottom: 0.5rem; }
        .math-block { background: rgba(0, 0, 0, 0.3); border: 1px solid var(--border-color); border-radius: var(--radius-sm); padding: 1.5rem; margin: 1.5rem 0; overflow-x: auto; text-align: center; }
        .info-box { background: rgba(0, 212, 255, 0.1); border-left: 4px solid var(--accent-blue); padding: 1rem; margin: 1rem 0; border-radius: var(--radius-sm); }
        .info-box p { margin: 0; }
        .example-box { background: rgba(76, 175, 80, 0.1); border-left: 4px solid #4caf50; padding: 1rem; margin: 1rem 0; border-radius: var(--radius-sm); }
        .example-box p { margin: 0; color: var(--text-secondary); }
        
        /* MathJax styling */
        mjx-container { color: #a8dadc !important; }
        
        /* Collapsible details animation */
        details {
            overflow: hidden;
        }
        
        details[open] summary ~ * {
            animation: details-open 0.3s ease;
        }
        
        @keyframes details-open {
            0% {
                opacity: 0;
                transform: translateY(-10px);
            }
            100% {
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        details:not([open]) summary ~ * {
            animation: details-close 0.3s ease;
        }
        
        @keyframes details-close {
            0% {
                opacity: 1;
                transform: translateY(0);
            }
            100% {
                opacity: 0;
                transform: translateY(-10px);
            }
        }
        
        @keyframes fadeInUp { from { opacity: 0; transform: translateY(30px); } to { opacity: 1; transform: translateY(0); } }
        @keyframes fadeIn { from { opacity: 0; } to { opacity: 1; } }
        
        @media (max-width: 768px) { 
            .docs-title { font-size: 2rem; } 
            .tutorial-tabs { gap: 0.25rem; }
            .tutorial-tab { padding: 0.5rem 1rem; font-size: 0.9rem; }
            .subsection-title { font-size: 1.1rem; }
        }
    </style>
</head>
<body>
    <nav class="nav-ribbon">
        <div class="nav-left">
            <a href="index.html" class="nav-logo"><h1>Gravitation<sup>3</sup></h1></a>
            <ul class="nav-links">
                <li class="nav-link"><a href="explore.html"><span class="nav-icon">üî≠</span><span>Explore Sims</span></a></li>
                <li class="nav-link"><a href="ai.html"><span class="nav-icon">ü§ñ</span><span>AI & ML</span></a></li>
            </ul>
        </div>
        <div class="nav-right">
            <ul class="nav-links">
                <li class="nav-link"><a href="about.html"><span class="nav-icon">‚ÑπÔ∏è</span><span>About</span></a></li>
                <li class="nav-link"><a href="docs.html"><span class="nav-icon">üìö</span><span>Documentation and Guides</span></a></li>
                <li class="nav-link"><a href="mission.html"><span class="nav-icon">üéØ</span><span>Mission</span></a></li>
                <li class="nav-link"><a href="support.html"><span class="nav-icon">üí¨</span><span>Support</span></a></li>
            </ul>
            <button class="mobile-menu-toggle">‚ò∞</button>
        </div>
    </nav>

    <div class="docs-container">
        <div class="docs-content">
            <div class="docs-header">
                <h1 class="docs-title">Math & Physics Tutorials</h1>
                <p class="docs-subtitle">Comprehensive guide from fundamentals to advanced concepts</p>
            </div>

            <!-- Main Navigation Tabs -->
            <div class="tutorial-tabs">
                <button class="tutorial-tab active" data-tutorial="vectors">üìê Vector Mathematics</button>
                <button class="tutorial-tab" data-tutorial="calculus">üìä Calculus Fundamentals</button>
                <button class="tutorial-tab" data-tutorial="vector-calc">‚àá Vector Calculus</button>
                <button class="tutorial-tab" data-tutorial="diffeq">üîÑ Differential Equations</button>
                <button class="tutorial-tab" data-tutorial="advanced">üéì Advanced Concepts</button>
            </div>

            <!-- Section 1: Vector Mathematics -->
            <div class="tutorial-content active" id="vectors">
                
                <!-- Subsection 1.1 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">1.1</span>
                            <span>Introduction to Vectors</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>What is a Vector?</h2>
                                
                                <p>Imagine you're standing at a street corner, and a friend calls asking for directions to the coffee shop. You could say "It's 500 meters away" - but that wouldn't help them much, would it? They could walk 500 meters in any direction and still miss it completely. What they really need to know is: "Walk 500 meters <em>north</em>." Now you've given them two pieces of information: a distance (500 meters) and a direction (north). This combination of magnitude and direction is exactly what we call a <strong>vector</strong>.</p>

                                <p>In physics and mathematics, we encounter quantities every day that need both magnitude and direction to be fully described. Think about wind: saying "the wind is blowing at 20 miles per hour" gives you some information, but it's incomplete. Is it blowing toward you? Away? From the side? You need to know both how <em>fast</em> (magnitude) and <em>which way</em> (direction). That's a vector!</p>

                                <h3>The Two Essential Properties of Vectors</h3>
                                
                                <p><strong>1. Magnitude:</strong> This is the "size" of the vector - how much, how far, how strong. It's always a positive number (or zero). For a velocity vector, the magnitude is the speed. For a force vector, it's the strength of the push or pull. For a displacement vector, it's the distance traveled. You can think of magnitude as answering the question "How much?"</p>

                                <p><strong>2. Direction:</strong> This tells us which way the vector points. It could be north, east, upward, at a 45-degree angle, or any direction in three-dimensional space. Direction answers the question "Which way?" Without direction, we don't have a complete picture of what's happening.</p>

                                <h3>Scalars: The Simpler Cousin</h3>
                                
                                <p>In contrast to vectors, <strong>scalars</strong> are quantities that only have magnitude - no direction needed. Temperature is a perfect example: when the thermometer reads 25¬∞C, that's all the information you need. You don't say "25¬∞C to the northwest" - temperature is just a number. Mass is another scalar: if something weighs 5 kilograms, that's it. The 5 kg doesn't point anywhere.</p>

                                <p>Here's a helpful way to remember: if you can answer the question "How much?" with just a number, it's a scalar. If you need to add "in which direction?" to give a complete answer, it's a vector.</p>

                                <div class="info-box">
                                    <p><strong>Key Insight:</strong> Speed vs. Velocity perfectly illustrates the scalar/vector distinction. Speed (scalar) is just "60 mph" - it tells you how fast. Velocity (vector) is "60 mph north" - it tells you how fast AND which way. Same magnitude, but velocity carries critical directional information that speed lacks.</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>Common Vectors and Scalars in Physics</h2>
                                
                                <p>Let's look at real examples you encounter every day. Understanding which quantities are vectors and which are scalars is crucial for analyzing motion, forces, and energy in our simulations.</p>

                                <h3>Scalar Quantities (Magnitude Only):</h3>
                                <ul>
                                    <li><strong>Temperature (25¬∞C):</strong> Your coffee can be hot or cold, but its temperature doesn't have a direction. It's just a measurement.</li>
                                    <li><strong>Mass (5 kg):</strong> An object's mass is the same regardless of where you place it or how you orient it. Mass is intrinsic to the object.</li>
                                    <li><strong>Speed (60 mph):</strong> This tells you how fast something is moving, but not where it's going. Your car's speedometer shows speed, not velocity.</li>
                                    <li><strong>Time (3 seconds):</strong> Time flows forward, but we don't describe it with spatial direction. Three seconds is three seconds, period.</li>
                                    <li><strong>Energy (100 joules):</strong> An object can have kinetic or potential energy, but the energy itself doesn't point anywhere.</li>
                                    <li><strong>Distance (50 meters):</strong> The total path length traveled, without regard to direction. You could walk 50 meters in circles and you've still traveled 50 meters.</li>
                                </ul>
                                
                                <h3>Vector Quantities (Magnitude + Direction):</h3>
                                <ul>
                                    <li><strong>Velocity (60 mph north):</strong> Both the speed (60 mph) and direction (north) are essential. Change the direction even slightly, and you have a different velocity!</li>
                                    <li><strong>Force (10 N downward):</strong> Forces always act in a specific direction. Gravity pulls downward. You push the door forward. The strength (10 N) matters, but so does the direction of push/pull.</li>
                                    <li><strong>Displacement (5 m east):</strong> Unlike distance, displacement cares about start and end points. If you walk 5 meters east, you've displaced 5 m east - even if you walked in a zigzag pattern.</li>
                                    <li><strong>Acceleration (9.8 m/s¬≤ downward):</strong> When gravity accelerates a falling object, it speeds up in the downward direction. The rate of change (9.8 m/s¬≤) and the direction (down) both matter.</li>
                                    <li><strong>Momentum (mv in direction of motion):</strong> A moving object's momentum depends on both its mass and velocity, pointing in the direction of motion.</li>
                                    <li><strong>Electric Field (strength + direction):</strong> Electric fields show which way a positive charge would be pushed, and how strongly.</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Real-World Example:</strong> Imagine two airplanes both flying at 500 mph (same speed/magnitude). Plane A flies north, Plane B flies south. Even though they have the same speed, they have opposite velocities! After one hour, Plane A is 500 miles north of its starting point, while Plane B is 500 miles south. The direction makes all the difference - that's why velocity is a vector and speed is a scalar.</p>
                                </div>

                                <h3>Why This Matters for Our Simulations</h3>
                                
                                <p>In the Gravitation¬≥ simulations, almost everything you see is based on vectors. The Lorenz attractor traces a path through 3D space - that's a position vector changing over time. The three-body problem requires tracking position AND velocity vectors for each body. Fluid simulations calculate velocity vectors at every point in space, showing how the fluid flows. Understanding vectors isn't just abstract math - it's the fundamental language that describes motion, forces, and change in the natural world.</p>

                                <p>When you see a particle moving through space in our simulations, its behavior is governed by vector equations. The force acting on it (a vector) causes it to accelerate (also a vector), which changes its velocity (another vector), which changes its position (yet another vector). Every step is about vectors interacting with vectors!</p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 1.2 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">1.2</span>
                            <span>Vector Operations</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>How Do We Write Vectors?</h2>
                                
                                <p>Before we can work with vectors, we need a way to write them down clearly. Mathematicians and physicists have developed several notations, each useful in different contexts:</p>

                                <p><strong>Arrow Notation:</strong> $\vec{v}$ - The little arrow on top reminds us this is a vector, not a scalar. When you see $\vec{v}$, you know immediately it has both magnitude and direction.</p>

                                <p><strong>Bold Notation:</strong> <strong>v</strong> - In textbooks, vectors are often written in bold. This is harder to do when writing by hand, but it's common in printed materials.</p>

                                <p><strong>Component Notation:</strong> $\vec{v} = (v_x, v_y, v_z)$ or $\vec{v} = v_x\hat{i} + v_y\hat{j} + v_z\hat{k}$ - This breaks the vector into its parts along each coordinate axis. The values $v_x$, $v_y$, and $v_z$ tell us how much the vector points in each direction. The symbols $\hat{i}$, $\hat{j}$, and $\hat{k}$ are unit vectors pointing along the x, y, and z axes respectively.</p>

                                <h3>Calculating Vector Magnitude</h3>

                                <p>The magnitude of a vector is its length - how far it reaches from start to finish. If you imagine the vector as an arrow, the magnitude is the length of that arrow. We calculate it using the three-dimensional version of the Pythagorean theorem:</p>

                                <div class="math-block">
                                    $$|\vec{v}| = \sqrt{v_x^2 + v_y^2 + v_z^2}$$
                                </div>

                                <p>Why does this work? Think of the vector components as forming a 3D box. The vector is the diagonal line from one corner to the opposite corner. The Pythagorean theorem in 3D tells us that this diagonal length is the square root of the sum of the squared side lengths.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> If $\vec{v} = (3, 4, 0)$, then $|\vec{v}| = \sqrt{3^2 + 4^2 + 0^2} = \sqrt{9 + 16} = \sqrt{25} = 5$ meters. This is the classic 3-4-5 right triangle! The vector points 3 units in the x-direction, 4 units in the y-direction, and has total length 5.</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>Vector Addition: Following Directions</h2>

                                <p>Adding vectors is like following a series of directions. If you walk 3 blocks east, then 4 blocks north, where do you end up? You add the displacement vectors! Vector addition is done component by component:</p>

                                <div class="math-block">
                                    $$\vec{a} + \vec{b} = (a_x + b_x, a_y + b_y, a_z + b_z)$$
                                </div>

                                <p>This means: add all the x-components together, add all the y-components together, and add all the z-components together. The result is a new vector that represents the total displacement.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> If $\vec{a} = (1, 2, 3)$ represents walking 1m east, 2m north, and 3m up, and $\vec{b} = (4, 5, 6)$ represents another displacement, then $\vec{a} + \vec{b} = (5, 7, 9)$ is your total displacement. You moved 5m east, 7m north, and 9m up overall.</p>
                                </div>

                                <h3>Scalar Multiplication: Changing Size</h3>

                                <p>When you multiply a vector by a scalar (a regular number), you change its magnitude without changing its direction (unless the scalar is negative, which reverses the direction):</p>

                                <div class="math-block">
                                    $$c\vec{v} = (cv_x, cv_y, cv_z)$$
                                </div>

                                <p>For example, $2\vec{v}$ is a vector pointing the same direction as $\vec{v}$ but twice as long. And $-\vec{v}$ points in the exact opposite direction with the same magnitude. This is useful in physics: if a force $\vec{F}$ acts on an object, then $2\vec{F}$ is twice as strong but pushes in the same direction.</p>
                            </div>

                            <div class="docs-section">
                                <h2>The Dot Product: Measuring Alignment</h2>

                                <p>The dot product (also called the inner product or scalar product) takes two vectors and produces a single number. It measures how much the vectors point in the same direction:</p>

                                <div class="math-block">
                                    $$\vec{a} \cdot \vec{b} = a_xb_x + a_yb_y + a_zb_z = |\vec{a}||\vec{b}|\cos\theta$$
                                </div>

                                <p>There are two ways to calculate it. The algebraic way: multiply corresponding components and add them up. The geometric way: multiply the magnitudes and the cosine of the angle between them.</p>

                                <h3>Proof: Why the Two Formulas Are Equivalent</h3>

                                <p>How can $a_xb_x + a_yb_y + a_zb_z$ equal $|\vec{a}||\vec{b}|\cos\theta$? Let's prove this remarkable connection between algebra and geometry.</p>

                                <p><strong>Starting point:</strong> Consider two vectors $\vec{a}$ and $\vec{b}$ with an angle $\theta$ between them. Using the law of cosines on the triangle formed by $\vec{a}$, $\vec{b}$, and their difference $\vec{c} = \vec{b} - \vec{a}$:</p>

                                <div class="math-block">
                                    $$|\vec{c}|^2 = |\vec{a}|^2 + |\vec{b}|^2 - 2|\vec{a}||\vec{b}|\cos\theta$$
                                </div>

                                <p>But we can also calculate $|\vec{c}|^2$ using components. Since $\vec{c} = \vec{b} - \vec{a} = (b_x - a_x, b_y - a_y, b_z - a_z)$:</p>

                                <div class="math-block">
                                    $$|\vec{c}|^2 = (b_x - a_x)^2 + (b_y - a_y)^2 + (b_z - a_z)^2$$
                                </div>

                                <p>Expanding the squares:</p>

                                <div class="math-block">
                                    $$|\vec{c}|^2 = b_x^2 - 2a_xb_x + a_x^2 + b_y^2 - 2a_yb_y + a_y^2 + b_z^2 - 2a_zb_z + a_z^2$$
                                </div>

                                <p>Regrouping:</p>

                                <div class="math-block">
                                    $$|\vec{c}|^2 = (a_x^2 + a_y^2 + a_z^2) + (b_x^2 + b_y^2 + b_z^2) - 2(a_xb_x + a_yb_y + a_zb_z)$$
                                    $$|\vec{c}|^2 = |\vec{a}|^2 + |\vec{b}|^2 - 2(a_xb_x + a_yb_y + a_zb_z)$$
                                </div>

                                <p>Now we have two expressions for $|\vec{c}|^2$. Setting them equal:</p>

                                <div class="math-block">
                                    $$|\vec{a}|^2 + |\vec{b}|^2 - 2(a_xb_x + a_yb_y + a_zb_z) = |\vec{a}|^2 + |\vec{b}|^2 - 2|\vec{a}||\vec{b}|\cos\theta$$
                                </div>

                                <p>Subtracting $|\vec{a}|^2 + |\vec{b}|^2$ from both sides and dividing by -2:</p>

                                <div class="math-block">
                                    $$a_xb_x + a_yb_y + a_zb_z = |\vec{a}||\vec{b}|\cos\theta$$
                                </div>

                                <p><strong>QED.</strong> This proof reveals why the dot product is so powerful - it connects the algebraic (component-wise) view with the geometric (angle and magnitude) view!</p>

                                <h3>Properties of the Dot Product</h3>

                                <p>The dot product has several important mathematical properties:</p>

                                <ol>
                                    <li><strong>Commutative:</strong> $\vec{a} \cdot \vec{b} = \vec{b} \cdot \vec{a}$</li>
                                    <li><strong>Distributive:</strong> $\vec{a} \cdot (\vec{b} + \vec{c}) = \vec{a} \cdot \vec{b} + \vec{a} \cdot \vec{c}$</li>
                                    <li><strong>Scalar Multiplication:</strong> $(c\vec{a}) \cdot \vec{b} = c(\vec{a} \cdot \vec{b})$</li>
                                    <li><strong>Positive Definite:</strong> $\vec{a} \cdot \vec{a} = |\vec{a}|^2 \geq 0$, with equality only when $\vec{a} = \vec{0}$</li>
                                </ol>

                                <p><strong>Why is this useful?</strong> The dot product tells you about the relationship between vectors:</p>
                                <ul>
                                    <li>If $\vec{a} \cdot \vec{b} > 0$: The vectors point generally in the same direction (angle less than 90¬∞)</li>
                                    <li>If $\vec{a} \cdot \vec{b} = 0$: The vectors are perpendicular (90¬∞ angle) - very important in physics!</li>
                                    <li>If $\vec{a} \cdot \vec{b} < 0$: The vectors point generally in opposite directions (angle greater than 90¬∞)</li>
                                </ul>

                                <h3>Finding the Angle Between Vectors</h3>

                                <p>Since $\vec{a} \cdot \vec{b} = |\vec{a}||\vec{b}|\cos\theta$, we can solve for the angle:</p>

                                <div class="math-block">
                                    $$\theta = \arccos\left(\frac{\vec{a} \cdot \vec{b}}{|\vec{a}||\vec{b}|}\right)$$
                                </div>

                                <p>This formula is incredibly useful! Given any two vectors in component form, we can find the exact angle between them.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Find the angle between $\vec{a} = (1, 2, 0)$ and $\vec{b} = (2, 1, 2)$.<br>
                                    $\vec{a} \cdot \vec{b} = 1(2) + 2(1) + 0(2) = 4$<br>
                                    $|\vec{a}| = \sqrt{1^2 + 2^2 + 0^2} = \sqrt{5}$<br>
                                    $|\vec{b}| = \sqrt{2^2 + 1^2 + 2^2} = 3$<br>
                                    $\theta = \arccos\left(\frac{4}{\sqrt{5} \cdot 3}\right) = \arccos\left(\frac{4}{3\sqrt{5}}\right) \approx 53.3¬∞$</p>
                                </div>

                                <div class="info-box">
                                    <p><strong>Physics Application:</strong> Work = Force ¬∑ Displacement. When you push a box, the work you do is the dot product of the force vector and displacement vector. If you push perpendicular to the motion, you do zero work! That's why pushing down on your shopping cart as you roll it forward doesn't help it move.</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Let $\vec{a} = (1, 0, 0)$ and $\vec{b} = (0, 1, 0)$. Then $\vec{a} \cdot \vec{b} = 1(0) + 0(1) + 0(0) = 0$. These vectors are perpendicular - one points along the x-axis, the other along the y-axis. They're at a 90¬∞ angle.</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>The Cross Product: Finding Perpendiculars</h2>

                                <p>Unlike the dot product which gives a number, the cross product of two vectors produces a third vector that is perpendicular to both input vectors. This is incredibly useful in three-dimensional problems:</p>

                                <div class="math-block">
                                    $$\vec{a} \times \vec{b} = (a_yb_z - a_zb_y, a_zb_x - a_xb_z, a_xb_y - a_yb_x)$$
                                </div>

                                <p>The formula looks complicated, but there's a pattern. Each component involves terms from the other two axes. The result is a vector pointing perpendicular to the plane containing $\vec{a}$ and $\vec{b}$.</p>

                                <h3>Understanding the Formula with Determinants</h3>

                                <p>The cross product can be understood using the determinant of a 3√ó3 matrix. This might look intimidating, but it's actually a clever mnemonic device:</p>

                                <div class="math-block">
                                    $$\vec{a} \times \vec{b} = \begin{vmatrix} \hat{i} & \hat{j} & \hat{k} \\ a_x & a_y & a_z \\ b_x & b_y & b_z \end{vmatrix}$$
                                </div>

                                <p>Expanding this determinant along the first row gives us the formula above. Each component is found by:</p>

                                <ul>
                                    <li><strong>x-component:</strong> $\hat{i}(a_yb_z - a_zb_y)$ - use the 2√ó2 submatrix excluding $\hat{i}$'s row and column</li>
                                    <li><strong>y-component:</strong> $-\hat{j}(a_xb_z - a_zb_x)$ - note the negative sign! This gives us $\hat{j}(a_zb_x - a_xb_z)$</li>
                                    <li><strong>z-component:</strong> $\hat{k}(a_xb_y - a_yb_x)$ - use the 2√ó2 submatrix with x and y components</li>
                                </ul>

                                <h3>Proof: Why Is It Perpendicular?</h3>

                                <p>Let's prove that $\vec{a} \times \vec{b}$ is indeed perpendicular to both $\vec{a}$ and $\vec{b}$. Two vectors are perpendicular if their dot product is zero, so we need to show:</p>

                                <p><strong>1. $\vec{a} \cdot (\vec{a} \times \vec{b}) = 0$</strong></p>

                                <div class="math-block">
                                    $$\vec{a} \cdot (\vec{a} \times \vec{b}) = a_x(a_yb_z - a_zb_y) + a_y(a_zb_x - a_xb_z) + a_z(a_xb_y - a_yb_x)$$
                                </div>

                                <p>Expanding:</p>

                                <div class="math-block">
                                    $$= a_xa_yb_z - a_xa_zb_y + a_ya_zb_x - a_ya_xb_z + a_za_xb_y - a_za_yb_x$$
                                </div>

                                <p>Notice that every term appears twice with opposite signs! For example, $a_xa_yb_z$ and $-a_ya_xb_z$ cancel (since multiplication is commutative). All six terms cancel in pairs, giving us:</p>

                                <div class="math-block">
                                    $$\vec{a} \cdot (\vec{a} \times \vec{b}) = 0$$
                                </div>

                                <p><strong>2. $\vec{b} \cdot (\vec{a} \times \vec{b}) = 0$</strong></p>

                                <p>The proof is identical - all terms cancel in pairs. Therefore, $\vec{a} \times \vec{b}$ is perpendicular to both $\vec{a}$ and $\vec{b}$. <strong>QED.</strong></p>

                                <h3>The Right-Hand Rule</h3>

                                <p><strong>The Right-Hand Rule:</strong> Point your right hand's fingers along $\vec{a}$, curl them toward $\vec{b}$, and your thumb points along $\vec{a} \times \vec{b}$. This tells you which of the two perpendicular directions you get.</p>

                                <p><strong>Important:</strong> The cross product is <em>anti-commutative</em>: $\vec{a} \times \vec{b} = -(\vec{b} \times \vec{a})$. Swapping the order reverses the direction!</p>

                                <h3>Magnitude and Geometric Interpretation</h3>

                                <p><strong>Magnitude Formula:</strong> The length of the cross product is:</p>

                                <div class="math-block">
                                    $$|\vec{a} \times \vec{b}| = |\vec{a}||\vec{b}|\sin\theta$$
                                </div>

                                <p>where $\theta$ is the angle between the vectors. This equals the <strong>area of the parallelogram</strong> formed by $\vec{a}$ and $\vec{b}$!</p>

                                <h3>Proof: Magnitude Formula</h3>

                                <p>Let's prove that $|\vec{a} \times \vec{b}|^2 = |\vec{a}|^2|\vec{b}|^2\sin^2\theta$. Starting with the component formula:</p>

                                <div class="math-block">
                                    $$|\vec{a} \times \vec{b}|^2 = (a_yb_z - a_zb_y)^2 + (a_zb_x - a_xb_z)^2 + (a_xb_y - a_yb_x)^2$$
                                </div>

                                <p>Expanding each term:</p>

                                <div class="math-block">
                                    $$= a_y^2b_z^2 - 2a_ya_zb_yb_z + a_z^2b_y^2 + a_z^2b_x^2 - 2a_xa_zb_xb_z + a_x^2b_z^2$$
                                    $$+ a_x^2b_y^2 - 2a_xa_yb_xb_y + a_y^2b_x^2$$
                                </div>

                                <p>Regrouping by collecting $a$ and $b$ terms:</p>

                                <div class="math-block">
                                    $$= (a_x^2 + a_y^2 + a_z^2)(b_x^2 + b_y^2 + b_z^2) - (a_xb_x + a_yb_y + a_zb_z)^2$$
                                    $$= |\vec{a}|^2|\vec{b}|^2 - (\vec{a} \cdot \vec{b})^2$$
                                </div>

                                <p>Since $\vec{a} \cdot \vec{b} = |\vec{a}||\vec{b}|\cos\theta$:</p>

                                <div class="math-block">
                                    $$= |\vec{a}|^2|\vec{b}|^2 - |\vec{a}|^2|\vec{b}|^2\cos^2\theta$$
                                    $$= |\vec{a}|^2|\vec{b}|^2(1 - \cos^2\theta)$$
                                    $$= |\vec{a}|^2|\vec{b}|^2\sin^2\theta$$
                                </div>

                                <p>Taking the square root of both sides:</p>

                                <div class="math-block">
                                    $$|\vec{a} \times \vec{b}| = |\vec{a}||\vec{b}|\sin\theta$$
                                </div>

                                <p><strong>QED.</strong> This beautiful result connects the algebraic and geometric views of the cross product!</p>

                                <h3>Properties of the Cross Product</h3>

                                <ol>
                                    <li><strong>Anti-commutative:</strong> $\vec{a} \times \vec{b} = -(\vec{b} \times \vec{a})$</li>
                                    <li><strong>Distributive:</strong> $\vec{a} \times (\vec{b} + \vec{c}) = \vec{a} \times \vec{b} + \vec{a} \times \vec{c}$</li>
                                    <li><strong>Scalar Multiplication:</strong> $(c\vec{a}) \times \vec{b} = c(\vec{a} \times \vec{b}) = \vec{a} \times (c\vec{b})$</li>
                                    <li><strong>Zero Vector:</strong> $\vec{a} \times \vec{a} = \vec{0}$ (a vector crossed with itself is zero)</li>
                                    <li><strong>Parallel Vectors:</strong> If $\vec{a} \parallel \vec{b}$, then $\vec{a} \times \vec{b} = \vec{0}$</li>
                                </ol>

                                <h3>Why Area of Parallelogram?</h3>

                                <p>The parallelogram formed by $\vec{a}$ and $\vec{b}$ has:</p>
                                <ul>
                                    <li>Base = $|\vec{a}|$</li>
                                    <li>Height = $|\vec{b}|\sin\theta$ (the perpendicular distance from $\vec{b}$ to the line containing $\vec{a}$)</li>
                                    <li>Area = base √ó height = $|\vec{a}||\vec{b}|\sin\theta$ = $|\vec{a} \times \vec{b}|$</li>
                                </ul>

                                <p>So the magnitude of the cross product literally gives you the area! This is why the cross product is so useful in geometry and physics.</p>

                                <div class="info-box">
                                    <p><strong>Physics Applications:</strong> The cross product appears everywhere in physics:<br>
                                    ‚Ä¢ <strong>Torque:</strong> $\vec{\tau} = \vec{r} \times \vec{F}$ (how much a force causes rotation)<br>
                                    ‚Ä¢ <strong>Angular momentum:</strong> $\vec{L} = \vec{r} \times \vec{p}$<br>
                                    ‚Ä¢ <strong>Magnetic force:</strong> $\vec{F} = q\vec{v} \times \vec{B}$ (force on a charged particle in a magnetic field)<br>
                                    ‚Ä¢ <strong>Angular velocity:</strong> The velocity of a point on a rotating object is $\vec{v} = \vec{\omega} \times \vec{r}$<br>
                                    In fluid dynamics, the curl operation (which we'll see later) is based on the cross product and describes rotation in fluids!</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 1:</strong> Let $\vec{a} = (1, 0, 0)$ (pointing along x-axis) and $\vec{b} = (0, 1, 0)$ (pointing along y-axis). Then:<br>
                                    $\vec{a} \times \vec{b} = (0 \cdot 0 - 0 \cdot 1, 0 \cdot 0 - 1 \cdot 0, 1 \cdot 1 - 0 \cdot 0) = (0, 0, 1)$<br>
                                    The result points along the z-axis, perpendicular to both input vectors!<br>
                                    Magnitude: $|\vec{a} \times \vec{b}| = 1$, which is the area of the unit square formed by $\vec{a}$ and $\vec{b}$.</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 2:</strong> Find $\vec{a} \times \vec{b}$ where $\vec{a} = (2, 1, -1)$ and $\vec{b} = (1, 3, 2)$.<br>
                                    $\vec{a} \times \vec{b} = (1 \cdot 2 - (-1) \cdot 3, (-1) \cdot 1 - 2 \cdot 2, 2 \cdot 3 - 1 \cdot 1)$<br>
                                    $= (2 + 3, -1 - 4, 6 - 1) = (5, -5, 5)$<br>
                                    Check: $\vec{a} \cdot (5, -5, 5) = 2(5) + 1(-5) + (-1)(5) = 10 - 5 - 5 = 0$ ‚úì<br>
                                    And: $\vec{b} \cdot (5, -5, 5) = 1(5) + 3(-5) + 2(5) = 5 - 15 + 10 = 0$ ‚úì</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 1.3 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">1.3</span>
                            <span>Coordinate Systems</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Understanding Coordinate Systems</h2>
                                
                                <p>When we work with vectors, we need a way to describe their position and direction. A coordinate system is like giving addresses to points in space. Just as street addresses help you locate buildings in a city, coordinate systems help us locate points and describe vectors in mathematical space.</p>

                                <h3>Cartesian Coordinates (Rectangular)</h3>
                                
                                <p>The <strong>Cartesian coordinate system</strong> is the most familiar system. Named after mathematician Ren√© Descartes, it uses perpendicular axes to define positions. In 2D, we have x (horizontal) and y (vertical) axes. In 3D, we add a z-axis perpendicular to both.</p>

                                <p><strong>2D Cartesian Coordinates:</strong> Any point in a plane can be described by two numbers $(x, y)$. The first number tells you how far to move horizontally from the origin, and the second tells you how far to move vertically.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> The point $(3, 4)$ means: start at the origin, move 3 units right along the x-axis, then move 4 units up along the y-axis. If you have negative coordinates like $(-2, 5)$, you move 2 units left and 5 units up.</p>
                                </div>

                                <p><strong>3D Cartesian Coordinates:</strong> In three dimensions, we use $(x, y, z)$. The z-axis typically points upward (or out of the page). This system is perfect for describing positions in real 3D space.</p>

                                <div class="info-box">
                                    <p><strong>In Our Simulations:</strong> The Lorenz attractor, three-body problem, and most fluid dynamics simulations use Cartesian coordinates because they make it easy to calculate distances, velocities, and forces in straightforward x, y, z components.</p>
                                </div>

                                <h3>Polar Coordinates (2D)</h3>
                                
                                <p>While Cartesian coordinates use distances along perpendicular axes, <strong>polar coordinates</strong> describe a point's location using a distance and an angle. Instead of saying "go 3 right and 4 up," polar coordinates say "go 5 units at a 53¬∞ angle."</p>

                                <p>A point in polar coordinates is written as $(r, \theta)$ where:</p>
                                <ul>
                                    <li><strong>r (radius):</strong> The distance from the origin to the point</li>
                                    <li><strong>Œ∏ (theta):</strong> The angle measured counterclockwise from the positive x-axis</li>
                                </ul>

                                <div class="math-block">
                                    $$x = r\cos\theta$$
                                    $$y = r\sin\theta$$
                                </div>

                                <p>These formulas convert from polar to Cartesian coordinates. Going the other way:</p>

                                <div class="math-block">
                                    $$r = \sqrt{x^2 + y^2}$$
                                    $$\theta = \arctan\left(\frac{y}{x}\right)$$
                                </div>

                                <div class="example-box">
                                    <p><strong>Example:</strong> The point $(r=5, \theta=53.13¬∞)$ in polar coordinates is the same as $(x=3, y=4)$ in Cartesian coordinates. You can verify: $x = 5\cos(53.13¬∞) \approx 3$ and $y = 5\sin(53.13¬∞) \approx 4$.</p>
                                </div>

                                <p><strong>When to use polar coordinates:</strong> They're perfect for circular or rotational motion. Describing planets orbiting a star, a pendulum swinging, or vortices in a fluid is often simpler in polar coordinates than Cartesian.</p>

                                <h3>Cylindrical Coordinates (3D)</h3>
                                
                                <p><strong>Cylindrical coordinates</strong> extend polar coordinates into 3D by adding a height component. They're written as $(r, \theta, z)$ where $r$ and $\theta$ work like 2D polar coordinates in the horizontal plane, and $z$ gives the vertical height.</p>

                                <div class="math-block">
                                    $$x = r\cos\theta$$
                                    $$y = r\sin\theta$$
                                    $$z = z$$
                                </div>

                                <p><strong>Use cases:</strong> Cylindrical coordinates are ideal for problems with cylindrical symmetry - like flow in pipes, rotating shafts, or any situation where objects naturally arrange themselves around a central axis. The Taylor-Couette flow simulation in Gravitation¬≥, which models fluid between rotating cylinders, naturally fits cylindrical coordinates.</p>

                                <h3>Spherical Coordinates (3D)</h3>
                                
                                <p><strong>Spherical coordinates</strong> describe points using a radius and two angles, perfect for spherical symmetry. They're written as $(r, \theta, \phi)$ or $(\rho, \theta, \phi)$ where:</p>
                                <ul>
                                    <li><strong>r or œÅ (rho):</strong> Distance from the origin</li>
                                    <li><strong>Œ∏ (theta):</strong> Azimuthal angle in the xy-plane (like longitude)</li>
                                    <li><strong>œÜ (phi):</strong> Polar angle from the z-axis (like latitude, but measured from the north pole)</li>
                                </ul>

                                <div class="math-block">
                                    $$x = r\sin\phi\cos\theta$$
                                    $$y = r\sin\phi\sin\theta$$
                                    $$z = r\cos\phi$$
                                </div>

                                <p><strong>Use cases:</strong> Spherical coordinates shine in problems with spherical symmetry - gravitational fields around planets, electromagnetic radiation from antennas, or electron orbitals in atoms. When dealing with orbital mechanics or planetary motion, spherical coordinates can dramatically simplify the mathematics.</p>

                                <div class="info-box">
                                    <p><strong>Choosing the Right System:</strong> The best coordinate system matches the symmetry of your problem. Use Cartesian for rectangular boundaries, polar/cylindrical for rotation around an axis, and spherical for radiation from a point. Many physics problems that seem impossibly complex in one coordinate system become manageable in another!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 1.4 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">1.4</span>
                            <span>Vector Fields</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>What is a Vector Field?</h2>
                                
                                <p>Imagine standing in a river. At your feet, the water flows at a certain speed and direction. Take a step to the side, and the flow might be different - maybe faster, maybe in a slightly different direction. Now imagine you could see arrows at every point in the river showing the water's velocity at that location. This collection of vectors, one at each point in space, is called a <strong>vector field</strong>.</p>

                                <p>Mathematically, a vector field is a function that assigns a vector to every point in space. We write it as $\vec{F}(x, y, z)$ or $\vec{F}(\vec{r})$, meaning: "tell me a position, and I'll tell you the vector at that position."</p>

                                <h3>Examples of Vector Fields</h3>
                                
                                <p><strong>1. Velocity Fields:</strong> In fluid dynamics, the velocity field $\vec{v}(x, y, z, t)$ tells you how fast and in what direction the fluid is moving at each point in space and time. When you see swirling patterns in our lid-driven cavity simulation or Von K√°rm√°n vortex street, you're seeing the velocity vector field visualized.</p>

                                <p><strong>2. Gravitational Fields:</strong> Near Earth, the gravitational field $\vec{g}$ points downward everywhere, always toward Earth's center. Its magnitude is strongest at the surface and weakens with distance. The three-body simulation shows how gravitational fields from multiple objects combine.</p>

                                <p><strong>3. Electric and Magnetic Fields:</strong> Electric fields $\vec{E}$ point away from positive charges and toward negative charges. Magnetic fields $\vec{B}$ circulate around current-carrying wires. These fields control how charged particles move.</p>

                                <p><strong>4. Force Fields:</strong> More generally, any field that describes forces acting throughout space is a force field. Spring forces, drag forces, electromagnetic forces - they can all be described as vector fields.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> A simple velocity field might be $\vec{v}(x, y) = (-y, x)$. At point $(3, 0)$, the velocity is $(0, 3)$ pointing upward. At $(0, 3)$, it's $(-3, 0)$ pointing left. This creates a counterclockwise circular flow pattern around the origin!</p>
                                </div>

                                <h3>Visualizing Vector Fields</h3>
                                
                                <p>Vector fields are typically visualized in two ways:</p>

                                <p><strong>1. Arrow Plots:</strong> Draw arrows at regularly spaced points, with each arrow's length representing the vector magnitude and its direction showing the vector direction. This gives an immediate sense of the field's structure.</p>

                                <p><strong>2. Streamlines:</strong> Draw curves that are always tangent to the vector field. Think of them as paths that particles would follow if carried by the field. In our fluid simulations, you're essentially seeing streamlines when particles trace out paths through the velocity field.</p>

                                <div class="info-box">
                                    <p><strong>In Gravitation¬≥:</strong> Nearly all our simulations involve vector fields! Fluid simulations compute and display velocity fields. Attractor simulations show how position vectors evolve. Understanding vector fields is the key to understanding how these complex systems behave.</p>
                                </div>

                                <h3>Field Lines and Flow</h3>
                                
                                <p>When we follow field lines (streamlines) through a vector field, we can learn a lot about the system's behavior:</p>
                                
                                <ul>
                                    <li><strong>Convergence:</strong> Field lines coming together indicate a sink - particles flowing toward a point</li>
                                    <li><strong>Divergence:</strong> Field lines spreading apart indicate a source - particles flowing away from a point</li>
                                    <li><strong>Circulation:</strong> Field lines forming closed loops indicate rotational flow - like vortices in fluids</li>
                                    <li><strong>Steady vs. Unsteady:</strong> If the field doesn't change with time, streamlines show particle paths. If it changes with time, streamlines show instantaneous flow direction but not actual paths.</li>
                                </ul>

                                <p>These concepts - divergence, curl, and how fields change - are so important that we have special mathematical operations for them, which we'll explore in the Vector Calculus section!</p>
                            </div>
                        </div>
                    </div>
                </div>

            </div>

            <!-- Section 2: Calculus Fundamentals -->
            <div class="tutorial-content" id="calculus">
                
                <!-- Subsection 2.1 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">2.1</span>
                            <span>Limits and Continuity</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>What is a Limit?</h2>
                                
                                <p>Imagine you're driving toward a stop sign. As you get closer and closer, you can see more details - the letters become clearer, you can read the text more easily. A limit in mathematics is similar: it describes what happens to a function as we get closer and closer to a particular point, even if we never quite reach it.</p>

                                <p>Mathematically, we write: "the limit of $f(x)$ as $x$ approaches $a$ equals $L$" as:</p>

                                <div class="math-block">
                                    $$\lim_{x \to a} f(x) = L$$
                                </div>

                                <p>This means: as $x$ gets arbitrarily close to $a$ (but isn't necessarily equal to $a$), the function $f(x)$ gets arbitrarily close to $L$.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Consider $f(x) = 2x + 1$. What is $\lim_{x \to 3} f(x)$?<br>
                                    As $x$ approaches 3: when $x = 2.9$, $f(x) = 6.8$; when $x = 2.99$, $f(x) = 6.98$; when $x = 2.999$, $f(x) = 6.998$.<br>
                                    Clearly, as $x$ gets closer to 3, $f(x)$ gets closer to 7. So $\lim_{x \to 3} (2x + 1) = 7$.</p>
                                </div>

                                <h3>Why Do Limits Matter?</h3>
                                
                                <p>Limits are the foundation of calculus. They allow us to:</p>
                                <ul>
                                    <li>Define instantaneous rates of change (derivatives)</li>
                                    <li>Calculate areas under curves (integrals)</li>
                                    <li>Understand behavior near points where functions aren't defined</li>
                                    <li>Analyze infinite processes and sequences</li>
                                </ul>

                                <h3>One-Sided Limits</h3>
                                
                                <p>Sometimes we need to specify which direction we're approaching from:</p>
                                
                                <p><strong>Right-hand limit:</strong> $\lim_{x \to a^+} f(x)$ means approaching $a$ from the right (larger values)</p>
                                <p><strong>Left-hand limit:</strong> $\lim_{x \to a^-} f(x)$ means approaching $a$ from the left (smaller values)</p>

                                <p>For a limit to exist at a point, both one-sided limits must exist and be equal.</p>

                                <div class="info-box">
                                    <p><strong>In Physics:</strong> Limits help us understand instantaneous velocity. When you look at your speedometer, it shows your instantaneous speed - the limit of average velocity as the time interval approaches zero!</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>Continuity</h2>
                                
                                <p>A function is <strong>continuous</strong> at a point if you can draw it without lifting your pencil. More formally, $f(x)$ is continuous at $x = a$ if:</p>

                                <ol>
                                    <li>$f(a)$ is defined (the function has a value at $a$)</li>
                                    <li>$\lim_{x \to a} f(x)$ exists (the limit exists)</li>
                                    <li>$\lim_{x \to a} f(x) = f(a)$ (the limit equals the function value)</li>
                                </ol>

                                <p>If any of these conditions fail, the function has a <strong>discontinuity</strong> at that point.</p>

                                <h3>Types of Discontinuities</h3>
                                
                                <p><strong>1. Removable Discontinuity:</strong> There's a hole in the graph, but we could "fill it in" by redefining the function at that point. The limit exists but doesn't equal the function value (or the function isn't defined there).</p>

                                <p><strong>2. Jump Discontinuity:</strong> The function suddenly jumps from one value to another. The left and right-hand limits exist but aren't equal.</p>

                                <p><strong>3. Infinite Discontinuity:</strong> The function shoots off to infinity. Common at vertical asymptotes like in $f(x) = \frac{1}{x}$ at $x = 0$.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> The function $f(x) = \frac{x^2 - 4}{x - 2}$ has a removable discontinuity at $x = 2$. We can't just plug in $x = 2$ (we'd get $\frac{0}{0}$), but we can factor: $f(x) = \frac{(x-2)(x+2)}{x-2} = x + 2$ for $x \neq 2$. The limit as $x \to 2$ is 4, but $f(2)$ is undefined.</p>
                                </div>

                                <div class="info-box">
                                    <p><strong>In Our Simulations:</strong> Fluid velocity fields are continuous functions - the velocity changes smoothly from point to point. Discontinuous jumps in velocity would create infinite forces! Ensuring continuity is crucial for realistic physics simulations.</p>
                                </div>

                                <h3>Limit Laws</h3>
                                
                                <p>If $\lim_{x \to a} f(x) = L$ and $\lim_{x \to a} g(x) = M$, then:</p>

                                <div class="math-block">
                                    $$\lim_{x \to a} [f(x) + g(x)] = L + M$$
                                    $$\lim_{x \to a} [f(x) \cdot g(x)] = L \cdot M$$
                                    $$\lim_{x \to a} \frac{f(x)}{g(x)} = \frac{L}{M} \text{ (if } M \neq 0\text{)}$$
                                </div>

                                <p>These laws let us break complex limits into simpler pieces!</p>
                            </div>

                            <div class="docs-section">
                                <h2>L'H√¥pital's Rule</h2>

                                <p>Sometimes when we try to evaluate a limit, we get an indeterminate form like $\frac{0}{0}$ or $\frac{\infty}{\infty}$. <strong>L'H√¥pital's Rule</strong> gives us a powerful technique for evaluating such limits.</p>

                                <h3>The Rule</h3>

                                <p>If $\lim_{x \to a} f(x) = 0$ and $\lim_{x \to a} g(x) = 0$ (or both approach $\pm\infty$), and if $\lim_{x \to a} \frac{f'(x)}{g'(x)}$ exists, then:</p>

                                <div class="math-block">
                                    $$\lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}$$
                                </div>

                                <p><strong>Key point:</strong> We take the derivative of the numerator and denominator <em>separately</em>, not using the quotient rule! This only works when we have an indeterminate form.</p>

                                <h3>When to Use L'H√¥pital's Rule</h3>

                                <p>L'H√¥pital's Rule applies to these indeterminate forms:</p>
                                <ul>
                                    <li>$\frac{0}{0}$ - Both numerator and denominator approach zero</li>
                                    <li>$\frac{\infty}{\infty}$ - Both approach infinity</li>
                                </ul>

                                <p>For other indeterminate forms like $0 \cdot \infty$, $\infty - \infty$, $0^0$, $1^\infty$, or $\infty^0$, we can often rearrange them into $\frac{0}{0}$ or $\frac{\infty}{\infty}$ forms.</p>

                                <div class="example-box">
                                    <p><strong>Example 1:</strong> Evaluate $\lim_{x \to 0} \frac{\sin x}{x}$<br>
                                    Direct substitution gives $\frac{0}{0}$ - indeterminate! Apply L'H√¥pital's Rule:<br>
                                    $\lim_{x \to 0} \frac{\sin x}{x} = \lim_{x \to 0} \frac{\cos x}{1} = \frac{1}{1} = 1$<br>
                                    This is a famous limit in calculus!</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 2:</strong> Evaluate $\lim_{x \to \infty} \frac{e^x}{x^2}$<br>
                                    As $x \to \infty$, both numerator and denominator approach $\infty$, giving $\frac{\infty}{\infty}$. Apply L'H√¥pital's Rule:<br>
                                    $\lim_{x \to \infty} \frac{e^x}{x^2} = \lim_{x \to \infty} \frac{e^x}{2x}$<br>
                                    Still $\frac{\infty}{\infty}$! Apply L'H√¥pital's Rule again:<br>
                                    $= \lim_{x \to \infty} \frac{e^x}{2} = \infty$<br>
                                    Exponential functions grow faster than polynomials!</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 3:</strong> Evaluate $\lim_{x \to 0^+} x\ln x$ (Form: $0 \cdot \infty$)<br>
                                    Rewrite as: $\lim_{x \to 0^+} \frac{\ln x}{1/x}$ (Form: $\frac{\infty}{\infty}$)<br>
                                    Apply L'H√¥pital's Rule:<br>
                                    $\lim_{x \to 0^+} \frac{1/x}{-1/x^2} = \lim_{x \to 0^+} \frac{x^2}{-x} = \lim_{x \to 0^+} (-x) = 0$</p>
                                </div>

                                <h3>Proof Sketch of L'H√¥pital's Rule</h3>

                                <p>The proof uses the <strong>Cauchy Mean Value Theorem</strong>, a generalization of the regular Mean Value Theorem. For the $\frac{0}{0}$ case:</p>

                                <p>Assume $f(a) = g(a) = 0$. By the Cauchy MVT, there exists a point $c$ between $x$ and $a$ such that:</p>

                                <div class="math-block">
                                    $$\frac{f(x) - f(a)}{g(x) - g(a)} = \frac{f'(c)}{g'(c)}$$
                                </div>

                                <p>Since $f(a) = g(a) = 0$:</p>

                                <div class="math-block">
                                    $$\frac{f(x)}{g(x)} = \frac{f'(c)}{g'(c)}$$
                                </div>

                                <p>As $x \to a$, we also have $c \to a$, so:</p>

                                <div class="math-block">
                                    $$\lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{c \to a} \frac{f'(c)}{g'(c)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}$$
                                </div>

                                <p>This elegant result transforms difficult limits into (often) easier derivative calculations!</p>

                                <div class="info-box">
                                    <p><strong>Common Mistake:</strong> Don't use L'H√¥pital's Rule when you don't have an indeterminate form! For example, $\lim_{x \to 0} \frac{x^2 + 1}{x + 1} = \frac{1}{1} = 1$ directly - no need for L'H√¥pital's Rule.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 2.2 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">2.2</span>
                            <span>Derivatives</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>What is a Derivative?</h2>
                                
                                <p>You're driving down a highway and glance at your speedometer: 60 mph. That number represents your <em>instantaneous rate of change</em> of position - exactly what a derivative measures! The derivative tells us how fast a function is changing at any given instant.</p>

                                <p>If $f(x)$ represents position over time, then the derivative $f'(x)$ represents velocity - the rate of change of position. If we have velocity, its derivative gives us acceleration - the rate of change of velocity.</p>

                                <h3>The Definition of a Derivative</h3>
                                
                                <p>The derivative of $f(x)$ at point $x$ is defined as:</p>

                                <div class="math-block">
                                    $$f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}$$
                                </div>

                                <p>This formula captures the idea of instantaneous rate of change. The fraction $\frac{f(x + h) - f(x)}{h}$ is the slope of the line connecting $(x, f(x))$ and $(x+h, f(x+h))$ - an average rate of change. As $h \to 0$, this becomes the instantaneous rate of change.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Let's find the derivative of $f(x) = x^2$ using the definition:<br>
                                    $f'(x) = \lim_{h \to 0} \frac{(x+h)^2 - x^2}{h} = \lim_{h \to 0} \frac{x^2 + 2xh + h^2 - x^2}{h} = \lim_{h \to 0} \frac{2xh + h^2}{h} = \lim_{h \to 0} (2x + h) = 2x$<br>
                                    So the derivative of $x^2$ is $2x$!</p>
                                </div>

                                <h3>Notation for Derivatives</h3>
                                
                                <p>There are several equivalent notations for derivatives:</p>
                                <ul>
                                    <li>$f'(x)$ - Lagrange notation (pronounced "f prime of x")</li>
                                    <li>$\frac{df}{dx}$ - Leibniz notation (pronounced "df dx")</li>
                                    <li>$\frac{dy}{dx}$ - when $y = f(x)$</li>
                                    <li>$Df(x)$ - operator notation</li>
                                </ul>

                                <p>Each notation has its uses. Leibniz notation $\frac{dy}{dx}$ is especially helpful because it reminds us we're looking at a rate of change of $y$ with respect to $x$.</p>

                                <h3>Geometric Interpretation</h3>
                                
                                <p>Geometrically, the derivative at a point is the <strong>slope of the tangent line</strong> to the function at that point. A positive derivative means the function is increasing, negative means decreasing, and zero indicates a local maximum, minimum, or inflection point.</p>

                                <div class="info-box">
                                    <p><strong>Physical Interpretation:</strong> In physics, derivatives are everywhere!<br>
                                    ‚Ä¢ Position ‚Üí Velocity (first derivative)<br>
                                    ‚Ä¢ Velocity ‚Üí Acceleration (second derivative)<br>
                                    ‚Ä¢ Electric potential ‚Üí Electric field<br>
                                    Our simulations constantly compute derivatives to update positions, velocities, and forces!</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>Basic Differentiation Rules and Common Derivatives</h2>
                                
                                <p>Let's learn the fundamental rules for differentiation and catalog the derivatives of common functions. These form the foundation for all derivative calculations.</p>

                                <h3>Basic Algebraic Rules</h3>
                                
                                <p><strong>Constant Rule:</strong> If $f(x) = c$ (a constant), then $f'(x) = 0$</p>
                                <p><strong>Power Rule:</strong> $\frac{d}{dx}(x^n) = nx^{n-1}$</p>
                                <p><strong>Constant Multiple Rule:</strong> If $f(x) = c \cdot g(x)$, then $f'(x) = c \cdot g'(x)$</p>
                                <p><strong>Sum/Difference Rule:</strong> $\frac{d}{dx}[f(x) \pm g(x)] = f'(x) \pm g'(x)$</p>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof of Power Rule</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>Let's prove that if $f(x) = x^n$, then $f'(x) = nx^{n-1}$ for positive integers $n$.</p>
                                        <p>Using the limit definition of the derivative:</p>
                                        <div class="math-block">
                                            $$f'(x) = \lim_{h \to 0} \frac{(x+h)^n - x^n}{h}$$
                                        </div>
                                        <p>We use the binomial theorem to expand $(x+h)^n$:</p>
                                        <div class="math-block">
                                            $$(x+h)^n = x^n + nx^{n-1}h + \frac{n(n-1)}{2}x^{n-2}h^2 + \cdots + h^n$$
                                        </div>
                                        <p>Substituting into the limit:</p>
                                        <div class="math-block">
                                            $$f'(x) = \lim_{h \to 0} \frac{x^n + nx^{n-1}h + \frac{n(n-1)}{2}x^{n-2}h^2 + \cdots + h^n - x^n}{h}$$
                                        </div>
                                        <p>The $x^n$ terms cancel:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \frac{nx^{n-1}h + \frac{n(n-1)}{2}x^{n-2}h^2 + \cdots + h^n}{h}$$
                                        </div>
                                        <p>Factor out $h$ from every term in the numerator:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \left(nx^{n-1} + \frac{n(n-1)}{2}x^{n-2}h + \cdots + h^{n-1}\right)$$
                                        </div>
                                        <p>As $h \to 0$, all terms containing $h$ vanish:</p>
                                        <div class="math-block">
                                            $$f'(x) = nx^{n-1}$$
                                        </div>
                                        <p><strong>QED.</strong> This proof extends to all real exponents using more advanced techniques!</p>
                                    </div>
                                </details>

                                <div class="example-box">
                                    <p><strong>Examples:</strong> $\frac{d}{dx}(5) = 0$, $\frac{d}{dx}(x^3) = 3x^2$, $\frac{d}{dx}(7x^4) = 28x^3$, $\frac{d}{dx}(3x^2 - 5x + 2) = 6x - 5$</p>
                                </div>

                                <h3>Exponential and Logarithmic Functions</h3>
                                
                                <div class="math-block">
                                    $$\frac{d}{dx}(e^x) = e^x$$
                                    $$\frac{d}{dx}(a^x) = a^x \ln(a) \quad (a > 0, a \neq 1)$$
                                    $$\frac{d}{dx}(\ln x) = \frac{1}{x}$$
                                    $$\frac{d}{dx}(\log_a x) = \frac{1}{x \ln(a)} \quad (a > 0, a \neq 1)$$
                                </div>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof: Derivative of e^x</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>This proof uses the fundamental property that $e$ is defined as the unique number where $\lim_{h \to 0} \frac{e^h - 1}{h} = 1$.</p>
                                        <p>Using the limit definition of the derivative:</p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}(e^x) = \lim_{h \to 0} \frac{e^{x+h} - e^x}{h}$$
                                        </div>
                                        <p>Factor out $e^x$:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \frac{e^x \cdot e^h - e^x}{h} = \lim_{h \to 0} e^x \cdot \frac{e^h - 1}{h}$$
                                        </div>
                                        <p>Since $e^x$ doesn't depend on $h$, we can pull it out of the limit:</p>
                                        <div class="math-block">
                                            $$= e^x \cdot \lim_{h \to 0} \frac{e^h - 1}{h} = e^x \cdot 1 = e^x$$
                                        </div>
                                        <p><strong>QED.</strong> This remarkable property - that $e^x$ is its own derivative - makes it the most important function in calculus!</p>
                                    </div>
                                </details>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof: Derivative of ln x</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>We'll use the definition of the derivative and properties of logarithms.</p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}(\ln x) = \lim_{h \to 0} \frac{\ln(x+h) - \ln x}{h}$$
                                        </div>
                                        <p>Using the logarithm property $\ln a - \ln b = \ln(a/b)$:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \frac{1}{h}\ln\left(\frac{x+h}{x}\right) = \lim_{h \to 0} \frac{1}{h}\ln\left(1 + \frac{h}{x}\right)$$
                                        </div>
                                        <p>Using the logarithm property $\ln(a^b) = b\ln a$:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \ln\left(1 + \frac{h}{x}\right)^{1/h}$$
                                        </div>
                                        <p>Let $u = h/x$, so as $h \to 0$, we also have $u \to 0$, and $h = ux$:</p>
                                        <div class="math-block">
                                            $$= \lim_{u \to 0} \ln\left(1 + u\right)^{1/(ux)} = \lim_{u \to 0} \frac{1}{x}\ln\left(1 + u\right)^{1/u}$$
                                        </div>
                                        <p>The limit $\lim_{u \to 0}(1 + u)^{1/u} = e$ by definition of $e$:</p>
                                        <div class="math-block">
                                            $$= \frac{1}{x}\ln(e) = \frac{1}{x}$$
                                        </div>
                                        <p><strong>QED.</strong> The natural logarithm and exponential functions are perfect inverses!</p>
                                    </div>
                                </details>

                                <h3>Trigonometric Functions</h3>
                                
                                <div class="math-block">
                                    $$\frac{d}{dx}(\sin x) = \cos x$$
                                    $$\frac{d}{dx}(\cos x) = -\sin x$$
                                    $$\frac{d}{dx}(\tan x) = \sec^2 x$$
                                    $$\frac{d}{dx}(\cot x) = -\csc^2 x$$
                                    $$\frac{d}{dx}(\sec x) = \sec x \tan x$$
                                    $$\frac{d}{dx}(\csc x) = -\csc x \cot x$$
                                </div>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof: Derivatives of sin x and cos x</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>These proofs rely on two important limits: $\lim_{h \to 0} \frac{\sin h}{h} = 1$ and $\lim_{h \to 0} \frac{\cos h - 1}{h} = 0$.</p>
                                        <p><strong>For sin x:</strong></p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}(\sin x) = \lim_{h \to 0} \frac{\sin(x+h) - \sin x}{h}$$
                                        </div>
                                        <p>Using the angle addition formula $\sin(x+h) = \sin x \cos h + \cos x \sin h$:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \frac{\sin x \cos h + \cos x \sin h - \sin x}{h}$$
                                        </div>
                                        <p>Rearranging:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \left[\sin x \cdot \frac{\cos h - 1}{h} + \cos x \cdot \frac{\sin h}{h}\right]$$
                                        </div>
                                        <p>Using our two limits:</p>
                                        <div class="math-block">
                                            $$= \sin x \cdot 0 + \cos x \cdot 1 = \cos x$$
                                        </div>
                                        <p><strong>For cos x:</strong></p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}(\cos x) = \lim_{h \to 0} \frac{\cos(x+h) - \cos x}{h}$$
                                        </div>
                                        <p>Using $\cos(x+h) = \cos x \cos h - \sin x \sin h$:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \frac{\cos x \cos h - \sin x \sin h - \cos x}{h}$$
                                            $$= \lim_{h \to 0} \left[\cos x \cdot \frac{\cos h - 1}{h} - \sin x \cdot \frac{\sin h}{h}\right]$$
                                            $$= \cos x \cdot 0 - \sin x \cdot 1 = -\sin x$$
                                        </div>
                                        <p><strong>QED.</strong> Notice the beautiful symmetry: differentiating sin gives cos, and differentiating cos gives -sin!</p>
                                    </div>
                                </details>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proofs: Derivatives of All Trig Functions</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p><strong>Derivative of tan x:</strong> Since $\tan x = \frac{\sin x}{\cos x}$, use the quotient rule:</p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}(\tan x) = \frac{\cos x \cdot \cos x - \sin x \cdot (-\sin x)}{\cos^2 x} = \frac{\cos^2 x + \sin^2 x}{\cos^2 x} = \frac{1}{\cos^2 x} = \sec^2 x$$
                                        </div>
                                        
                                        <p><strong>Derivative of cot x:</strong> Since $\cot x = \frac{\cos x}{\sin x}$, use the quotient rule:</p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}(\cot x) = \frac{-\sin x \cdot \sin x - \cos x \cdot \cos x}{\sin^2 x} = \frac{-(\sin^2 x + \cos^2 x)}{\sin^2 x} = \frac{-1}{\sin^2 x} = -\csc^2 x$$
                                        </div>
                                        
                                        <p><strong>Derivative of sec x:</strong> Since $\sec x = \frac{1}{\cos x}$, use the quotient rule:</p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}(\sec x) = \frac{0 \cdot \cos x - 1 \cdot (-\sin x)}{\cos^2 x} = \frac{\sin x}{\cos^2 x} = \frac{1}{\cos x} \cdot \frac{\sin x}{\cos x} = \sec x \tan x$$
                                        </div>
                                        
                                        <p><strong>Derivative of csc x:</strong> Since $\csc x = \frac{1}{\sin x}$, use the quotient rule:</p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}(\csc x) = \frac{0 \cdot \sin x - 1 \cdot \cos x}{\sin^2 x} = \frac{-\cos x}{\sin^2 x} = -\frac{1}{\sin x} \cdot \frac{\cos x}{\sin x} = -\csc x \cot x$$
                                        </div>
                                        <p><strong>QED.</strong> All trig derivatives follow from sin, cos, and the quotient rule!</p>
                                    </div>
                                </details>

                                <h3>Inverse Trigonometric Functions</h3>
                                
                                <div class="math-block">
                                    $$\frac{d}{dx}(\arcsin x) = \frac{1}{\sqrt{1-x^2}}$$
                                    $$\frac{d}{dx}(\arccos x) = -\frac{1}{\sqrt{1-x^2}}$$
                                    $$\frac{d}{dx}(\arctan x) = \frac{1}{1+x^2}$$
                                    $$\frac{d}{dx}(\text{arccot } x) = -\frac{1}{1+x^2}$$
                                    $$\frac{d}{dx}(\text{arcsec } x) = \frac{1}{|x|\sqrt{x^2-1}}$$
                                    $$\frac{d}{dx}(\text{arccsc } x) = -\frac{1}{|x|\sqrt{x^2-1}}$$
                                </div>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proofs: Derivatives of All Inverse Trig Functions</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p><strong>Derivative of arcsin x:</strong> Let $y = \arcsin x$, so $\sin y = x$. Differentiate implicitly:</p>
                                        <div class="math-block">
                                            $$\cos y \cdot \frac{dy}{dx} = 1 \implies \frac{dy}{dx} = \frac{1}{\cos y} = \frac{1}{\sqrt{1-\sin^2 y}} = \frac{1}{\sqrt{1-x^2}}$$
                                        </div>
                                        
                                        <p><strong>Derivative of arccos x:</strong> Let $y = \arccos x$, so $\cos y = x$. Differentiate implicitly:</p>
                                        <div class="math-block">
                                            $$-\sin y \cdot \frac{dy}{dx} = 1 \implies \frac{dy}{dx} = \frac{-1}{\sin y} = \frac{-1}{\sqrt{1-\cos^2 y}} = \frac{-1}{\sqrt{1-x^2}}$$
                                        </div>
                                        
                                        <p><strong>Derivative of arctan x:</strong> Let $y = \arctan x$, so $\tan y = x$. Differentiate implicitly:</p>
                                        <div class="math-block">
                                            $$\sec^2 y \cdot \frac{dy}{dx} = 1 \implies \frac{dy}{dx} = \frac{1}{\sec^2 y} = \frac{1}{1+\tan^2 y} = \frac{1}{1+x^2}$$
                                        </div>
                                        
                                        <p><strong>Derivative of arccot x:</strong> Let $y = \text{arccot } x$, so $\cot y = x$. Differentiate implicitly:</p>
                                        <div class="math-block">
                                            $$-\csc^2 y \cdot \frac{dy}{dx} = 1 \implies \frac{dy}{dx} = \frac{-1}{\csc^2 y} = \frac{-1}{1+\cot^2 y} = \frac{-1}{1+x^2}$$
                                        </div>
                                        
                                        <p><strong>Derivative of arcsec x:</strong> Let $y = \text{arcsec } x$, so $\sec y = x$. Differentiate implicitly:</p>
                                        <div class="math-block">
                                            $$\sec y \tan y \cdot \frac{dy}{dx} = 1 \implies \frac{dy}{dx} = \frac{1}{\sec y \tan y} = \frac{1}{x\sqrt{\sec^2 y - 1}} = \frac{1}{|x|\sqrt{x^2-1}}$$
                                        </div>
                                        
                                        <p><strong>Derivative of arccsc x:</strong> Let $y = \text{arccsc } x$, so $\csc y = x$. Differentiate implicitly:</p>
                                        <div class="math-block">
                                            $$-\csc y \cot y \cdot \frac{dy}{dx} = 1 \implies \frac{dy}{dx} = \frac{-1}{\csc y \cot y} = \frac{-1}{x\sqrt{\csc^2 y - 1}} = \frac{-1}{|x|\sqrt{x^2-1}}$$
                                        </div>
                                        <p><strong>QED.</strong> All inverse trig derivatives use implicit differentiation and trig identities!</p>
                                    </div>
                                </details>

                                <h3>Hyperbolic Functions</h3>
                                
                                <div class="math-block">
                                    $$\frac{d}{dx}(\sinh x) = \cosh x$$
                                    $$\frac{d}{dx}(\cosh x) = \sinh x$$
                                    $$\frac{d}{dx}(\tanh x) = \text{sech}^2 x$$
                                </div>

                                <p><strong>Definitions:</strong> $\sinh x = \frac{e^x - e^{-x}}{2}$, $\cosh x = \frac{e^x + e^{-x}}{2}$, $\tanh x = \frac{\sinh x}{\cosh x}$</p>

                                <h3>Inverse Hyperbolic Functions</h3>
                                
                                <div class="math-block">
                                    $$\frac{d}{dx}(\text{arsinh } x) = \frac{1}{\sqrt{x^2+1}}$$
                                    $$\frac{d}{dx}(\text{arcosh } x) = \frac{1}{\sqrt{x^2-1}}$$
                                    $$\frac{d}{dx}(\text{artanh } x) = \frac{1}{1-x^2}$$
                                </div>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proofs: Derivatives of Inverse Hyperbolic Functions</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p><strong>Derivative of arsinh x:</strong> Let $y = \text{arsinh } x$, so $\sinh y = x$. Differentiate implicitly:</p>
                                        <div class="math-block">
                                            $$\cosh y \cdot \frac{dy}{dx} = 1 \implies \frac{dy}{dx} = \frac{1}{\cosh y}$$
                                        </div>
                                        <p>Using the identity $\cosh^2 y - \sinh^2 y = 1$, we get $\cosh y = \sqrt{1 + \sinh^2 y} = \sqrt{1 + x^2}$:</p>
                                        <div class="math-block">
                                            $$\frac{dy}{dx} = \frac{1}{\sqrt{x^2+1}}$$
                                        </div>
                                        
                                        <p><strong>Derivative of arcosh x:</strong> Let $y = \text{arcosh } x$, so $\cosh y = x$. Differentiate implicitly:</p>
                                        <div class="math-block">
                                            $$\sinh y \cdot \frac{dy}{dx} = 1 \implies \frac{dy}{dx} = \frac{1}{\sinh y}$$
                                        </div>
                                        <p>Using $\sinh y = \sqrt{\cosh^2 y - 1} = \sqrt{x^2 - 1}$ (taking positive root for the principal branch):</p>
                                        <div class="math-block">
                                            $$\frac{dy}{dx} = \frac{1}{\sqrt{x^2-1}}$$
                                        </div>
                                        
                                        <p><strong>Derivative of artanh x:</strong> Let $y = \text{artanh } x$, so $\tanh y = x$. Differentiate implicitly:</p>
                                        <div class="math-block">
                                            $$\text{sech}^2 y \cdot \frac{dy}{dx} = 1 \implies \frac{dy}{dx} = \frac{1}{\text{sech}^2 y}$$
                                        </div>
                                        <p>Using the identity $\text{sech}^2 y = 1 - \tanh^2 y = 1 - x^2$:</p>
                                        <div class="math-block">
                                            $$\frac{dy}{dx} = \frac{1}{1-x^2}$$
                                        </div>
                                        <p><strong>QED.</strong> Inverse hyperbolic derivatives mirror inverse trig, but with different algebraic expressions!</p>
                                    </div>
                                </details>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof of Power Rule</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>Let's prove that if $f(x) = x^n$, then $f'(x) = nx^{n-1}$ for positive integers $n$.</p>
                                        <p>Using the limit definition of the derivative:</p>
                                        <div class="math-block">
                                            $$f'(x) = \lim_{h \to 0} \frac{(x+h)^n - x^n}{h}$$
                                        </div>
                                        <p>We use the binomial theorem to expand $(x+h)^n$:</p>
                                        <div class="math-block">
                                            $$(x+h)^n = x^n + nx^{n-1}h + \frac{n(n-1)}{2}x^{n-2}h^2 + \cdots + h^n$$
                                        </div>
                                        <p>Substituting into the limit:</p>
                                        <div class="math-block">
                                            $$f'(x) = \lim_{h \to 0} \frac{x^n + nx^{n-1}h + \frac{n(n-1)}{2}x^{n-2}h^2 + \cdots + h^n - x^n}{h}$$
                                        </div>
                                        <p>The $x^n$ terms cancel:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \frac{nx^{n-1}h + \frac{n(n-1)}{2}x^{n-2}h^2 + \cdots + h^n}{h}$$
                                        </div>
                                        <p>Factor out $h$ from every term in the numerator:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \left(nx^{n-1} + \frac{n(n-1)}{2}x^{n-2}h + \cdots + h^{n-1}\right)$$
                                        </div>
                                        <p>As $h \to 0$, all terms containing $h$ vanish:</p>
                                        <div class="math-block">
                                            $$f'(x) = nx^{n-1}$$
                                        </div>
                                        <p><strong>QED.</strong> This proof extends to all real exponents using more advanced techniques!</p>
                                    </div>
                                </details>

                                <div class="example-box">
                                    <p>$\frac{d}{dx}(x^3) = 3x^2$, $\frac{d}{dx}(x^{10}) = 10x^9$, $\frac{d}{dx}(\sqrt{x}) = \frac{d}{dx}(x^{1/2}) = \frac{1}{2}x^{-1/2} = \frac{1}{2\sqrt{x}}$</p>
                                </div>

                                <p><strong>Constant Rule:</strong> If $f(x) = c$ (a constant), then $f'(x) = 0$</p>
                                <p><strong>Constant Multiple Rule:</strong> If $f(x) = c \cdot g(x)$, then $f'(x) = c \cdot g'(x)$</p>
                                <p><strong>Sum/Difference Rule:</strong> $\frac{d}{dx}[f(x) \pm g(x)] = f'(x) \pm g'(x)$</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Find the derivative of $f(x) = 3x^4 - 5x^2 + 7$<br>
                                    $f'(x) = 3(4x^3) - 5(2x) + 0 = 12x^3 - 10x$</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>Advanced Differentiation Rules</h2>
                                
                                <p>Now that we know the basic rules and common derivatives, let's learn advanced rules for combining functions. These rules are essential for differentiating complex expressions.</p>

                                <h3>Product Rule</h3>
                                
                                <p><strong>Product Rule:</strong> If $f(x) = u(x) \cdot v(x)$, then:</p>
                                <div class="math-block">
                                    $$f'(x) = u'(x) \cdot v(x) + u(x) \cdot v'(x)$$
                                </div>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof of Product Rule</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>Starting from the limit definition:</p>
                                        <div class="math-block">
                                            $$f'(x) = \lim_{h \to 0} \frac{u(x+h)v(x+h) - u(x)v(x)}{h}$$
                                        </div>
                                        <p>The clever trick is to add and subtract $u(x+h)v(x)$:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \frac{u(x+h)v(x+h) - u(x+h)v(x) + u(x+h)v(x) - u(x)v(x)}{h}$$
                                        </div>
                                        <p>Factoring:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \frac{u(x+h)[v(x+h) - v(x)] + v(x)[u(x+h) - u(x)]}{h}$$
                                        </div>
                                        <p>Splitting into two limits:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} u(x+h) \cdot \frac{v(x+h) - v(x)}{h} + \lim_{h \to 0} v(x) \cdot \frac{u(x+h) - u(x)}{h}$$
                                        </div>
                                        <p>Since $u$ is differentiable, it's continuous, so $\lim_{h \to 0} u(x+h) = u(x)$:</p>
                                        <div class="math-block">
                                            $$= u(x) \cdot v'(x) + v(x) \cdot u'(x) = u'(x)v(x) + u(x)v'(x)$$
                                        </div>
                                        <p><strong>QED.</strong> This elegant proof shows why we get both terms in the product rule!</p>
                                    </div>
                                </details>

                                <div class="example-box">
                                    <p><strong>Example:</strong> $f(x) = x^2 \sin(x)$<br>
                                    $f'(x) = 2x \sin(x) + x^2 \cos(x)$</p>
                                </div>

                                <h3>Quotient Rule</h3>
                                
                                <p><strong>Quotient Rule:</strong> If $f(x) = \frac{u(x)}{v(x)}$, then:</p>
                                <div class="math-block">
                                    $$f'(x) = \frac{u'(x) \cdot v(x) - u(x) \cdot v'(x)}{[v(x)]^2}$$
                                </div>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof of Quotient Rule</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>We can derive the quotient rule from the product rule. Let $f(x) = u(x) \cdot \frac{1}{v(x)}$.</p>
                                        <p>First, find the derivative of $\frac{1}{v(x)}$ using the limit definition:</p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}\left[\frac{1}{v(x)}\right] = \lim_{h \to 0} \frac{\frac{1}{v(x+h)} - \frac{1}{v(x)}}{h}$$
                                        </div>
                                        <p>Combining fractions in the numerator:</p>
                                        <div class="math-block">
                                            $$= \lim_{h \to 0} \frac{v(x) - v(x+h)}{h \cdot v(x+h) \cdot v(x)} = \lim_{h \to 0} \frac{-[v(x+h) - v(x)]}{h \cdot v(x+h) \cdot v(x)}$$
                                        </div>
                                        <div class="math-block">
                                            $$= \frac{-v'(x)}{[v(x)]^2}$$
                                        </div>
                                        <p>Now apply the product rule to $f(x) = u(x) \cdot \frac{1}{v(x)}$:</p>
                                        <div class="math-block">
                                            $$f'(x) = u'(x) \cdot \frac{1}{v(x)} + u(x) \cdot \frac{-v'(x)}{[v(x)]^2}$$
                                        </div>
                                        <p>Finding a common denominator:</p>
                                        <div class="math-block">
                                            $$= \frac{u'(x) \cdot v(x)}{[v(x)]^2} - \frac{u(x) \cdot v'(x)}{[v(x)]^2} = \frac{u'(x) \cdot v(x) - u(x) \cdot v'(x)}{[v(x)]^2}$$
                                        </div>
                                        <p><strong>QED.</strong> The quotient rule follows naturally from the product rule!</p>
                                    </div>
                                </details>

                                <div class="example-box">
                                    <p><strong>Example:</strong> $f(x) = \frac{x^2 + 1}{x - 1}$<br>
                                    $f'(x) = \frac{2x(x-1) - (x^2+1)(1)}{(x-1)^2} = \frac{2x^2 - 2x - x^2 - 1}{(x-1)^2} = \frac{x^2 - 2x - 1}{(x-1)^2}$</p>
                                </div>

                                <h3>Chain Rule</h3>
                                
                                <p>The chain rule handles composite functions - functions inside other functions. If $f(x) = g(h(x))$, then:</p>
                                <div class="math-block">
                                    $$f'(x) = g'(h(x)) \cdot h'(x)$$
                                </div>

                                <p>In Leibniz notation: if $y = f(u)$ and $u = g(x)$, then $\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$</p>

                                <p>The Leibniz notation makes the chain rule intuitive - it looks like the $du$'s "cancel," though this is not rigorous. What it really means is: the rate of change of $y$ with respect to $x$ equals the rate of change of $y$ with respect to $u$, multiplied by the rate of change of $u$ with respect to $x$.</p>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof of Chain Rule</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>Let $f(x) = g(h(x))$ where both $g$ and $h$ are differentiable. We want to prove $f'(x) = g'(h(x)) \cdot h'(x)$.</p>
                                        <p>Starting from the limit definition:</p>
                                        <div class="math-block">
                                            $$f'(x) = \lim_{\Delta x \to 0} \frac{f(x + \Delta x) - f(x)}{\Delta x} = \lim_{\Delta x \to 0} \frac{g(h(x + \Delta x)) - g(h(x))}{\Delta x}$$
                                        </div>
                                        <p>Let $u = h(x)$ and $\Delta u = h(x + \Delta x) - h(x)$. Then:</p>
                                        <div class="math-block">
                                            $$f'(x) = \lim_{\Delta x \to 0} \frac{g(u + \Delta u) - g(u)}{\Delta x}$$
                                        </div>
                                        <p>Multiply and divide by $\Delta u$ (assuming $\Delta u \neq 0$ for now):</p>
                                        <div class="math-block">
                                            $$= \lim_{\Delta x \to 0} \frac{g(u + \Delta u) - g(u)}{\Delta u} \cdot \frac{\Delta u}{\Delta x}$$
                                        </div>
                                        <p>Now here's the key insight: as $\Delta x \to 0$, we also have $\Delta u \to 0$ (since $h$ is continuous, being differentiable). So we can separate this into two limits:</p>
                                        <div class="math-block">
                                            $$= \lim_{\Delta u \to 0} \frac{g(u + \Delta u) - g(u)}{\Delta u} \cdot \lim_{\Delta x \to 0} \frac{\Delta u}{\Delta x}$$
                                        </div>
                                        <p>But these limits are exactly the definitions of derivatives!</p>
                                        <div class="math-block">
                                            $$= g'(u) \cdot h'(x) = g'(h(x)) \cdot h'(x)$$
                                        </div>
                                        <p><strong>QED.</strong> The chain rule emerges naturally from the definition of the derivative!</p>
                                        <p><strong>Intuition:</strong> If you change $x$ by a tiny amount, $u$ changes by roughly $h'(x) \cdot \Delta x$. Then $g(u)$ changes by roughly $g'(u)$ times that change in $u$. Multiplying these rates gives the total rate of change: $g'(u) \cdot h'(x)$.</p>
                                    </div>
                                </details>

                                <div class="example-box">
                                    <p><strong>Example 1:</strong> Find the derivative of $f(x) = (3x^2 + 1)^5$<br>
                                    Let $u = 3x^2 + 1$, so $f = u^5$<br>
                                    $\frac{df}{dx} = \frac{df}{du} \cdot \frac{du}{dx} = 5u^4 \cdot 6x = 5(3x^2 + 1)^4 \cdot 6x = 30x(3x^2 + 1)^4$</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 2:</strong> Find the derivative of $f(x) = \sin(x^3)$<br>
                                    Let $u = x^3$, so $f = \sin(u)$<br>
                                    $\frac{df}{dx} = \frac{df}{du} \cdot \frac{du}{dx} = \cos(u) \cdot 3x^2 = 3x^2\cos(x^3)$</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 3 (Multiple Chains):</strong> Find the derivative of $f(x) = e^{\sin(x^2)}$<br>
                                    This requires the chain rule twice! Let $v = x^2$, $u = \sin(v)$, and $f = e^u$<br>
                                    $\frac{df}{dx} = \frac{df}{du} \cdot \frac{du}{dv} \cdot \frac{dv}{dx} = e^u \cdot \cos(v) \cdot 2x = 2xe^{\sin(x^2)}\cos(x^2)$</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>Applications of Derivatives</h2>
                                
                                <h3>Finding Extrema</h3>
                                
                                <p>To find maximum and minimum values of a function:</p>
                                <ol>
                                    <li>Find where $f'(x) = 0$ (critical points)</li>
                                    <li>Test each critical point and endpoints</li>
                                    <li>Use the second derivative test: if $f''(x) > 0$, it's a minimum; if $f''(x) < 0$, it's a maximum</li>
                                </ol>

                                <div class="info-box">
                                    <p><strong>In Optimization:</strong> Many physics problems involve finding extrema - minimum energy states, maximum efficiency, shortest paths. Our double pendulum simulation, for example, naturally seeks configurations that minimize potential energy!</p>
                                </div>

                                <h3>Related Rates</h3>
                                
                                <p>When multiple quantities change over time and are related by an equation, we can use derivatives to find how their rates of change are related. This is crucial in dynamics!</p>

                                <h3>Linear Approximation</h3>
                                
                                <p>The derivative gives us the best linear approximation to a function near a point:</p>
                                <div class="math-block">
                                    $$f(x) \approx f(a) + f'(a)(x - a)$$
                                </div>

                                <p>This is the tangent line approximation, and it's the basis for many numerical methods used in simulations!</p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 2.3 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">2.3</span>
                            <span>Integrals</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>What is an Integral?</h2>
                                
                                <p>If derivatives measure rates of change, integrals measure accumulation. Imagine you're driving and your speedometer shows your velocity at each moment. If you want to know how far you've traveled, you need to "add up" all those little bits of distance over time. That's exactly what an integral does!</p>

                                <p>Integrals have two main interpretations:</p>
                                <ul>
                                    <li><strong>Geometric:</strong> The area under a curve</li>
                                    <li><strong>Physical:</strong> The accumulation of a quantity over an interval</li>
                                </ul>

                                <h3>The Definite Integral</h3>
                                
                                <p>The definite integral of $f(x)$ from $a$ to $b$ is written:</p>
                                <div class="math-block">
                                    $$\int_a^b f(x) \, dx$$
                                </div>

                                <p>This represents the signed area between the curve $y = f(x)$ and the x-axis, from $x = a$ to $x = b$. Area above the axis is positive, below is negative.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> $\int_0^3 2x \, dx$ represents the area under the line $y = 2x$ from $x = 0$ to $x = 3$. This forms a triangle with base 3 and height 6, so the area is $\frac{1}{2}(3)(6) = 9$.</p>
                                </div>

                                <h3>Riemann Sums</h3>
                                
                                <p>How do we actually calculate integrals? The formal definition uses a <strong>Riemann sum</strong> - we split the interval into $n$ rectangles, add up their areas, and take the limit as $n \to \infty$:</p>

                                <div class="math-block">
                                    $$\int_a^b f(x) \, dx = \lim_{n \to \infty} \sum_{i=1}^n f(x_i) \Delta x$$
                                </div>

                                <p>where $\Delta x = \frac{b-a}{n}$ is the width of each rectangle, and $x_i$ is a point in the $i$-th interval.</p>

                                <div class="info-box">
                                    <p><strong>In Simulations:</strong> Computers can't take infinite limits, so they use finite Riemann sums - this is numerical integration! Our simulations use sophisticated versions of this idea to evolve systems forward in time.</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>The Fundamental Theorem of Calculus</h2>
                                
                                <p>The most important theorem in calculus connects derivatives and integrals. It has two parts:</p>

                                <h3>Part 1: Differentiation of Integrals</h3>
                                
                                <p>If $F(x) = \int_a^x f(t) \, dt$, then $F'(x) = f(x)$</p>

                                <p>In other words, if you integrate a function and then differentiate the result, you get back the original function!</p>

                                <h3>Part 2: Integration by Antiderivatives</h3>
                                
                                <p>If $F(x)$ is an antiderivative of $f(x)$ (meaning $F'(x) = f(x)$), then:</p>

                                <div class="math-block">
                                    $$\int_a^b f(x) \, dx = F(b) - F(a)$$
                                </div>

                                <p>This is huge! Instead of calculating limits of Riemann sums, we just need to find an antiderivative and evaluate it at the endpoints.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Calculate $\int_1^3 x^2 \, dx$<br>
                                    The antiderivative of $x^2$ is $F(x) = \frac{x^3}{3}$<br>
                                    $\int_1^3 x^2 \, dx = F(3) - F(1) = \frac{27}{3} - \frac{1}{3} = 9 - \frac{1}{3} = \frac{26}{3}$</p>
                                </div>

                                <h3>Why Is This So Important?</h3>
                                
                                <p>The Fundamental Theorem tells us that differentiation and integration are inverse operations - one undoes the other! This deep connection between rates of change and accumulation is at the heart of calculus and physics.</p>

                                <div class="info-box">
                                    <p><strong>Physics Example:</strong> If velocity is the derivative of position, then position is the integral of velocity! Given $v(t) = 3t^2$, we can find position: $x(t) = \int 3t^2 \, dt = t^3 + C$, where $C$ depends on initial position.</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>Common Integrals</h2>
                                
                                <p>Before learning integration techniques, let's catalog the integrals of common functions. These are the inverse operations of the derivatives we learned earlier, and they're essential building blocks for more complex integrals.</p>

                                <p><strong>Important:</strong> Every indefinite integral includes "+ C", the constant of integration. Since the derivative of any constant is zero, when we reverse the process (integration), we must account for all possible constants that could have been there.</p>

                                <h3>Power, Exponential, and Logarithmic Functions</h3>
                                
                                <div class="math-block">
                                    $$\int x^n \, dx = \frac{x^{n+1}}{n+1} + C \quad (n \neq -1)$$
                                    $$\int \frac{1}{x} \, dx = \ln|x| + C$$
                                    $$\int e^x \, dx = e^x + C$$
                                    $$\int a^x \, dx = \frac{a^x}{\ln(a)} + C \quad (a > 0, a \neq 1)$$
                                </div>

                                <h3>Trigonometric Functions</h3>
                                
                                <div class="math-block">
                                    $$\int \sin x \, dx = -\cos x + C$$
                                    $$\int \cos x \, dx = \sin x + C$$
                                    $$\int \sec^2 x \, dx = \tan x + C$$
                                    $$\int \csc^2 x \, dx = -\cot x + C$$
                                    $$\int \sec x \tan x \, dx = \sec x + C$$
                                    $$\int \csc x \cot x \, dx = -\csc x + C$$
                                    $$\int \tan x \, dx = -\ln|\cos x| + C = \ln|\sec x| + C$$
                                    $$\int \cot x \, dx = \ln|\sin x| + C$$
                                    $$\int \sec x \, dx = \ln|\sec x + \tan x| + C$$
                                    $$\int \csc x \, dx = -\ln|\csc x + \cot x| + C$$
                                </div>

                                <h3>Inverse Trigonometric Functions</h3>
                                
                                <div class="math-block">
                                    $$\int \frac{1}{\sqrt{1-x^2}} \, dx = \arcsin x + C$$
                                    $$\int \frac{-1}{\sqrt{1-x^2}} \, dx = \arccos x + C$$
                                    $$\int \frac{1}{1+x^2} \, dx = \arctan x + C$$
                                    $$\int \frac{-1}{1+x^2} \, dx = \text{arccot } x + C$$
                                    $$\int \frac{1}{|x|\sqrt{x^2-1}} \, dx = \text{arcsec } |x| + C$$
                                    $$\int \frac{-1}{|x|\sqrt{x^2-1}} \, dx = \text{arccsc } |x| + C$$
                                </div>

                                <h3>Hyperbolic Functions</h3>
                                
                                <div class="math-block">
                                    $$\int \sinh x \, dx = \cosh x + C$$
                                    $$\int \cosh x \, dx = \sinh x + C$$
                                    $$\int \text{sech}^2 x \, dx = \tanh x + C$$
                                </div>

                                <div class="info-box">
                                    <p><strong>Memorization Tip:</strong> Notice how integrals mirror derivatives! If $\frac{d}{dx}(\sin x) = \cos x$, then $\int \cos x \, dx = \sin x + C$. The trig integrals with secant and cosecant can be tricky - memorize the common ones and derive others as needed using substitution techniques.</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Verify that $\int x^3 \, dx = \frac{x^4}{4} + C$ by differentiation:<br>
                                    $\frac{d}{dx}\left(\frac{x^4}{4} + C\right) = \frac{1}{4} \cdot 4x^3 + 0 = x^3$ ‚úì</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>Integration Techniques</h2>
                                
                                <p>Now that we know the integrals of common functions, let's learn techniques for integrating more complex expressions. These techniques allow us to break down complicated integrals into simpler ones we can solve.</p>

                                <h3>Basic Integration Rules</h3>
                                
                                <p><strong>Constant Multiple Rule:</strong> $\int cf(x) \, dx = c\int f(x) \, dx$</p>
                                <p><strong>Sum/Difference Rule:</strong> $\int [f(x) \pm g(x)] \, dx = \int f(x) \, dx \pm \int g(x) \, dx$</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> $\int (3x^2 - 5x + 2) \, dx = 3\int x^2 \, dx - 5\int x \, dx + \int 2 \, dx$<br>
                                    $= 3 \cdot \frac{x^3}{3} - 5 \cdot \frac{x^2}{2} + 2x + C = x^3 - \frac{5x^2}{2} + 2x + C$</p>
                                </div>

                                <h3>Integration by Substitution (u-substitution)</h3>
                                
                                <p>Substitution (u-substitution) is the reverse of the chain rule. If we have $\int f(g(x))g'(x) \, dx$, we can substitute $u = g(x)$, so $du = g'(x)dx$:</p>

                                <div class="math-block">
                                    $$\int f(g(x))g'(x) \, dx = \int f(u) \, du$$
                                </div>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof of Substitution Rule</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>The substitution rule for integration is essentially the chain rule in reverse. Let's prove it works.</p>
                                        <p>Suppose we want to prove that $\int f(g(x))g'(x) \, dx = F(g(x)) + C$, where $F$ is an antiderivative of $f$ (meaning $F'(u) = f(u)$).</p>
                                        <p>To verify this, we differentiate the right-hand side using the chain rule:</p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}[F(g(x))] = F'(g(x)) \cdot g'(x) = f(g(x)) \cdot g'(x)$$
                                        </div>
                                        <p>Since the derivative of $F(g(x))$ gives us back $f(g(x))g'(x)$, we've shown that $F(g(x))$ is indeed an antiderivative of $f(g(x))g'(x)$.</p>
                                        <p><strong>In terms of substitution:</strong> When we let $u = g(x)$, then $du = g'(x)dx$, and:</p>
                                        <div class="math-block">
                                            $$\int f(g(x))g'(x) \, dx = \int f(u) \, du = F(u) + C = F(g(x)) + C$$
                                        </div>
                                        <p><strong>QED.</strong> The substitution method works because it's simply the chain rule applied in reverse!</p>
                                        <p><strong>Intuition:</strong> When we see $f(g(x))g'(x)$, we recognize it as the derivative of $F(g(x))$ by the chain rule. So integrating it gives us back $F(g(x))$!</p>
                                    </div>
                                </details>

                                <div class="example-box">
                                    <p><strong>Example 1:</strong> $\int 2x \cos(x^2) \, dx$<br>
                                    Let $u = x^2$, then $du = 2x \, dx$<br>
                                    $\int 2x \cos(x^2) \, dx = \int \cos(u) \, du = \sin(u) + C = \sin(x^2) + C$<br>
                                    <strong>Verification:</strong> $\frac{d}{dx}[\sin(x^2)] = \cos(x^2) \cdot 2x$ ‚úì</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 2:</strong> $\int \frac{2x}{x^2 + 1} \, dx$<br>
                                    Let $u = x^2 + 1$, then $du = 2x \, dx$<br>
                                    $\int \frac{2x}{x^2 + 1} \, dx = \int \frac{1}{u} \, du = \ln|u| + C = \ln(x^2 + 1) + C$<br>
                                    Note: We can drop the absolute value since $x^2 + 1$ is always positive.</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 3:</strong> $\int e^{3x} \, dx$<br>
                                    Let $u = 3x$, then $du = 3dx$, so $dx = \frac{du}{3}$<br>
                                    $\int e^{3x} \, dx = \int e^u \cdot \frac{du}{3} = \frac{1}{3}\int e^u \, du = \frac{1}{3}e^u + C = \frac{1}{3}e^{3x} + C$</p>
                                </div>

                                <h3>Integration by Parts</h3>
                                
                                <p>Integration by parts is the reverse of the product rule. It's particularly useful when you have a product of functions where one becomes simpler when differentiated and the other is easy to integrate:</p>

                                <div class="math-block">
                                    $$\int u \, dv = uv - \int v \, du$$
                                </div>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof of Integration by Parts</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>Integration by parts comes directly from the product rule for derivatives. Let's derive it!</p>
                                        <p>Recall the product rule: if $h(x) = u(x)v(x)$, then:</p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}[u(x)v(x)] = u'(x)v(x) + u(x)v'(x)$$
                                        </div>
                                        <p>Rearranging this equation:</p>
                                        <div class="math-block">
                                            $$u(x)v'(x) = \frac{d}{dx}[u(x)v(x)] - u'(x)v(x)$$
                                        </div>
                                        <p>Now integrate both sides:</p>
                                        <div class="math-block">
                                            $$\int u(x)v'(x) \, dx = \int \frac{d}{dx}[u(x)v(x)] \, dx - \int u'(x)v(x) \, dx$$
                                        </div>
                                        <p>The first integral on the right is simply $u(x)v(x)$ (since integration and differentiation are inverse operations):</p>
                                        <div class="math-block">
                                            $$\int u(x)v'(x) \, dx = u(x)v(x) - \int u'(x)v(x) \, dx$$
                                        </div>
                                        <p>Using the notation $dv = v'(x)dx$ and $du = u'(x)dx$:</p>
                                        <div class="math-block">
                                            $$\int u \, dv = uv - \int v \, du$$
                                        </div>
                                        <p><strong>QED.</strong> This formula allows us to transform one integral into another, often simpler one!</p>
                                        <p><strong>Strategy:</strong> Choose $u$ to be the function that simplifies when differentiated, and $dv$ to be the function that's easy to integrate. A helpful mnemonic is LIATE (choose $u$ in this priority order: Logarithmic, Inverse trig, Algebraic, Trigonometric, Exponential).</p>
                                    </div>
                                </details>

                                <div class="example-box">
                                    <p><strong>Example 1:</strong> $\int x e^x \, dx$<br>
                                    Let $u = x$ (algebraic - gets simpler when differentiated), $dv = e^x dx$ (exponential - easy to integrate)<br>
                                    Then $du = dx$, $v = e^x$<br>
                                    $\int x e^x \, dx = xe^x - \int e^x \, dx = xe^x - e^x + C = e^x(x-1) + C$<br>
                                    <strong>Verification:</strong> $\frac{d}{dx}[e^x(x-1)] = e^x(x-1) + e^x = xe^x$ ‚úì</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 2:</strong> $\int \ln x \, dx$<br>
                                    This doesn't look like a product, but we can write it as $\int \ln x \cdot 1 \, dx$<br>
                                    Let $u = \ln x$ (logarithmic - priority), $dv = 1 \, dx$<br>
                                    Then $du = \frac{1}{x}dx$, $v = x$<br>
                                    $\int \ln x \, dx = x\ln x - \int x \cdot \frac{1}{x} \, dx = x\ln x - \int 1 \, dx = x\ln x - x + C = x(\ln x - 1) + C$</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 3:</strong> $\int x^2 \sin x \, dx$<br>
                                    Let $u = x^2$, $dv = \sin x \, dx$<br>
                                    Then $du = 2x \, dx$, $v = -\cos x$<br>
                                    $\int x^2 \sin x \, dx = -x^2\cos x - \int (-\cos x)(2x) \, dx = -x^2\cos x + 2\int x\cos x \, dx$<br>
                                    For the remaining integral, use integration by parts again: let $u = x$, $dv = \cos x \, dx$<br>
                                    Then $du = dx$, $v = \sin x$<br>
                                    $2\int x\cos x \, dx = 2[x\sin x - \int \sin x \, dx] = 2[x\sin x + \cos x]$<br>
                                    Final answer: $-x^2\cos x + 2x\sin x + 2\cos x + C$</p>
                                </div>

                                <h3>Applications of Integrals</h3>
                                
                                <p><strong>Area Between Curves:</strong> $\int_a^b [f(x) - g(x)] \, dx$ gives the area between $f(x)$ and $g(x)$</p>

                                <p><strong>Volume of Revolution:</strong> Rotating a curve around an axis creates a 3D solid. The disk method gives: $V = \pi \int_a^b [f(x)]^2 \, dx$</p>

                                <p><strong>Work and Energy:</strong> Work done by a force $F(x)$ over distance is $W = \int_a^b F(x) \, dx$</p>

                                <p><strong>Probability:</strong> For a probability density function $f(x)$, the probability that $a \leq x \leq b$ is $\int_a^b f(x) \, dx$</p>

                                <div class="info-box">
                                    <p><strong>In Our Simulations:</strong> We constantly integrate! The three-body problem integrates gravitational forces to find velocities, then integrates velocities to find positions. Fluid simulations integrate pressure gradients to update velocity fields. Integration is how we compute the future from the present!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

            </div>

            <!-- Section 3: Vector Calculus -->
            <div class="tutorial-content" id="vector-calc">
                
                <!-- Subsection 3.1 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">3.1</span>
                            <span>Introduction to Vector Calculus</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>What is Vector Calculus?</h2>
                                
                                <p>Vector calculus is the extension of single-variable calculus to functions of multiple variables and vector-valued functions. While ordinary calculus deals with functions like $f(x)$ that take a single number and return a single number, vector calculus handles much more: functions that take multiple coordinates $(x, y, z)$ and return scalars, or functions that return entire vectors.</p>

                                <p>Think about temperature in a room. At each point $(x, y, z)$ in space, there's a temperature value $T(x, y, z)$ - that's a <strong>scalar field</strong>. Now think about wind: at each point, the wind has both a speed and a direction - that's a <strong>vector field</strong> $\vec{v}(x, y, z)$. Vector calculus gives us the tools to analyze how these fields change, flow, rotate, and interact.</p>

                                <h3>Why Do We Need Vector Calculus?</h3>
                                
                                <p>The real world is multidimensional! Almost every physical phenomenon involves multiple variables:</p>
                                
                                <ul>
                                    <li><strong>Fluid Dynamics:</strong> Water flowing in a river has velocity that varies with position - you need three spatial coordinates to describe where you are, and three velocity components to describe how fast the water is moving in each direction.</li>
                                    <li><strong>Electromagnetism:</strong> Electric and magnetic fields permeate space, with both strength and direction at every point. Maxwell's equations, which govern all electricity and magnetism, are written in the language of vector calculus.</li>
                                    <li><strong>Heat Transfer:</strong> Temperature varies from point to point in an object, and heat flows from hot to cold regions. The rate and direction of heat flow depend on temperature gradients.</li>
                                    <li><strong>Weather Forecasting:</strong> Atmospheric pressure, temperature, humidity, and wind velocity all vary in three dimensions and time. Predicting weather requires vector calculus.</li>
                                    <li><strong>Gravitational Fields:</strong> Every mass creates a gravitational field around it. Understanding orbits, tides, and planetary motion requires vector calculus.</li>
                                </ul>

                                <div class="info-box">
                                    <p><strong>In Gravitation¬≥ Simulations:</strong> Vector calculus is the mathematical foundation of our simulations! The Navier-Stokes equations (fluid flow) are vector calculus equations. The three-body problem uses gradients of gravitational potential. The Lorenz attractor's equations describe how a vector changes over time in three-dimensional space. Every simulation you see is vector calculus in action!</p>
                                </div>

                                <h3>The Building Blocks</h3>
                                
                                <p>Vector calculus introduces several new concepts that extend what we learned in single-variable calculus:</p>

                                <p><strong>1. Partial Derivatives:</strong> When a function depends on multiple variables, we can find how it changes with respect to just one variable while holding the others constant. This generalizes the derivative to multivariable functions.</p>

                                <p><strong>2. Gradient (‚àá):</strong> The gradient tells us the direction of steepest increase for a scalar field. If you're hiking on a mountainside and want to climb as steeply as possible, follow the gradient! The gradient converts a scalar field into a vector field.</p>

                                <p><strong>3. Divergence (‚àá¬∑):</strong> Divergence measures how much a vector field is "spreading out" or "concentrating" at a point. Positive divergence means there's a source (like water flowing out from a spring), negative means there's a sink (like water draining).</p>

                                <p><strong>4. Curl (‚àá√ó):</strong> Curl measures how much a vector field is "rotating" or "circulating" at a point. In fluid dynamics, regions with high curl are vortices - whirlpools and tornadoes have high curl!</p>

                                <p><strong>5. Line Integrals:</strong> Instead of integrating along a straight x-axis, we integrate along curves in space. This is how we calculate work done by a force along a path.</p>

                                <p><strong>6. Surface Integrals:</strong> We integrate over curved surfaces in three-dimensional space. This is how we calculate flux - how much of a field passes through a surface.</p>

                                <p><strong>7. Volume Integrals:</strong> We integrate over three-dimensional regions. This generalizes area integrals to volumes.</p>

                                <h3>The Fundamental Theorems</h3>
                                
                                <p>Just as single-variable calculus has the Fundamental Theorem connecting derivatives and integrals, vector calculus has three fundamental theorems that connect differentiation and integration in higher dimensions:</p>

                                <p><strong>Green's Theorem:</strong> Relates a line integral around a closed curve to a double integral over the region it encloses. This is 2D.</p>

                                <p><strong>Stokes' Theorem:</strong> Relates the curl of a vector field over a surface to a line integral around the boundary of that surface. This is the 3D generalization of Green's Theorem.</p>

                                <p><strong>Divergence Theorem (Gauss's Theorem):</strong> Relates the divergence of a vector field in a volume to the flux through the boundary surface. This connects volume integrals to surface integrals.</p>

                                <p>These theorems are incredibly powerful - they're the mathematical foundation of conservation laws in physics!</p>

                                <div class="example-box">
                                    <p><strong>Real-World Example:</strong> Imagine you're studying airflow around an airplane wing. The velocity of air is a vector field $\vec{v}(x,y,z)$. To understand lift, you need:<br>
                                    ‚Ä¢ <strong>Gradient:</strong> How pressure changes near the wing<br>
                                    ‚Ä¢ <strong>Divergence:</strong> Whether air is being compressed or expanding<br>
                                    ‚Ä¢ <strong>Curl:</strong> How much the air is rotating (vorticity)<br>
                                    ‚Ä¢ <strong>Surface integrals:</strong> Total force on the wing<br>
                                    All of these are vector calculus concepts!</p>
                                </div>

                                <h3>Notation and Conventions</h3>
                                
                                <p>Vector calculus uses special notation to keep track of all the coordinates and operations:</p>

                                <ul>
                                    <li><strong>Position vector:</strong> $\vec{r} = (x, y, z)$ or $\vec{r} = x\hat{i} + y\hat{j} + z\hat{k}$</li>
                                    <li><strong>Nabla operator:</strong> $\nabla = \left(\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}\right)$</li>
                                    <li><strong>Gradient:</strong> $\nabla f$ or $\text{grad}(f)$</li>
                                    <li><strong>Divergence:</strong> $\nabla \cdot \vec{F}$ or $\text{div}(\vec{F})$</li>
                                    <li><strong>Curl:</strong> $\nabla \times \vec{F}$ or $\text{curl}(\vec{F})$</li>
                                    <li><strong>Laplacian:</strong> $\nabla^2 f = \nabla \cdot \nabla f$</li>
                                </ul>

                                <p>The $\nabla$ (nabla) operator is remarkable - it acts like a vector of partial derivatives, and we can use it with dot products, cross products, and multiplication to express all the main operations of vector calculus compactly!</p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 3.2 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">3.2</span>
                            <span>Partial Derivatives and the Gradient</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Partial Derivatives</h2>
                                
                                <p>When a function depends on multiple variables, we need a way to measure how it changes with respect to each variable individually. This is where <strong>partial derivatives</strong> come in - they're like ordinary derivatives, but we focus on one variable at a time while treating all others as constants.</p>

                                <p>For a function $f(x, y, z)$, we have three partial derivatives:</p>

                                <div class="math-block">
                                    $$\frac{\partial f}{\partial x}, \quad \frac{\partial f}{\partial y}, \quad \frac{\partial f}{\partial z}$$
                                </div>

                                <p>The symbol $\partial$ (partial) distinguishes partial derivatives from ordinary derivatives $d$. When computing $\frac{\partial f}{\partial x}$, we differentiate with respect to $x$ while treating $y$ and $z$ as if they were constants.</p>

                                <div class="example-box">
                                    <p><strong>Example 1:</strong> Let $f(x, y) = x^2y + 3xy^2$. Find both partial derivatives.<br><br>
                                    $\frac{\partial f}{\partial x} = 2xy + 3y^2$ (treat $y$ as constant)<br>
                                    $\frac{\partial f}{\partial y} = x^2 + 6xy$ (treat $x$ as constant)<br><br>
                                    Notice how we use the power rule, but the other variables are just coefficients!</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 2:</strong> Let $T(x, y, z, t) = e^{-t}\sin(x)\cos(y)z^2$ represent temperature in space and time.<br><br>
                                    $\frac{\partial T}{\partial x} = e^{-t}\cos(x)\cos(y)z^2$ (other variables are constants)<br>
                                    $\frac{\partial T}{\partial y} = -e^{-t}\sin(x)\sin(y)z^2$<br>
                                    $\frac{\partial T}{\partial z} = e^{-t}\sin(x)\cos(y) \cdot 2z$<br>
                                    $\frac{\partial T}{\partial t} = -e^{-t}\sin(x)\cos(y)z^2$ (temperature decays exponentially with time)</p>
                                </div>

                                <h3>Geometric Interpretation</h3>
                                
                                <p>For a function $f(x, y)$, imagine its graph as a surface in 3D space. The partial derivative $\frac{\partial f}{\partial x}$ tells you the slope of the surface if you walk in the $x$-direction (parallel to the $x$-axis). Similarly, $\frac{\partial f}{\partial y}$ gives the slope in the $y$-direction.</p>

                                <p>If you're hiking on a mountainside where elevation is $h(x, y)$:</p>
                                <ul>
                                    <li>$\frac{\partial h}{\partial x}$ tells you how steep it is if you walk east-west</li>
                                    <li>$\frac{\partial h}{\partial y}$ tells you how steep it is if you walk north-south</li>
                                    <li>But what if you want to walk northeast? That requires the gradient!</li>
                                </ul>

                                <h3>Higher-Order Partial Derivatives</h3>
                                
                                <p>Just as we can take second derivatives in single-variable calculus, we can take second (and higher) partial derivatives. For $f(x, y)$, there are four second partial derivatives:</p>

                                <div class="math-block">
                                    $$\frac{\partial^2 f}{\partial x^2}, \quad \frac{\partial^2 f}{\partial y^2}, \quad \frac{\partial^2 f}{\partial x \partial y}, \quad \frac{\partial^2 f}{\partial y \partial x}$$
                                </div>

                                <p>The mixed partials $\frac{\partial^2 f}{\partial x \partial y}$ and $\frac{\partial^2 f}{\partial y \partial x}$ measure how the rate of change in one direction depends on the other direction. Remarkably, if the function is "nice enough" (continuously differentiable), these mixed partials are equal - this is <strong>Clairaut's Theorem</strong>:</p>

                                <div class="math-block">
                                    $$\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}$$
                                </div>

                                <p>This means the order of differentiation doesn't matter!</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Let $f(x, y) = x^3y^2 + xy$. Find all second partial derivatives.<br><br>
                                    First partials: $\frac{\partial f}{\partial x} = 3x^2y^2 + y$, $\frac{\partial f}{\partial y} = 2x^3y + x$<br><br>
                                    Second partials:<br>
                                    $\frac{\partial^2 f}{\partial x^2} = 6xy^2$<br>
                                    $\frac{\partial^2 f}{\partial y^2} = 2x^3$<br>
                                    $\frac{\partial^2 f}{\partial x \partial y} = 6x^2y + 1$<br>
                                    $\frac{\partial^2 f}{\partial y \partial x} = 6x^2y + 1$<br><br>
                                    Note that the mixed partials are equal, as Clairaut's Theorem predicts!</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>The Gradient</h2>
                                
                                <p>While partial derivatives tell us about change in specific coordinate directions, the <strong>gradient</strong> combines all the partial derivatives into a single vector that points in the direction of steepest increase. For a scalar field $f(x, y, z)$, the gradient is:</p>

                                <div class="math-block">
                                    $$\nabla f = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}\right) = \frac{\partial f}{\partial x}\hat{i} + \frac{\partial f}{\partial y}\hat{j} + \frac{\partial f}{\partial z}\hat{k}$$
                                </div>

                                <p>The gradient is a <strong>vector field</strong> - at each point in space, it assigns a vector. This vector has two crucial properties:</p>

                                <p><strong>1. Direction:</strong> The gradient points in the direction of maximum increase of $f$. If you want to climb a hill as steeply as possible, go in the direction of $\nabla f$. If you want to descend as steeply as possible, go in the direction of $-\nabla f$.</p>

                                <p><strong>2. Magnitude:</strong> The magnitude $|\nabla f|$ tells you how steeply $f$ is changing in that direction. A large gradient means $f$ is changing rapidly; a small gradient means it's changing slowly.</p>

                                <div class="example-box">
                                    <p><strong>Example 1:</strong> Find the gradient of $f(x, y) = x^2 + y^2$ (a paraboloid - bowl shape).<br><br>
                                    $\nabla f = (2x, 2y)$<br><br>
                                    At point $(1, 1)$: $\nabla f = (2, 2)$ - points diagonally away from the origin<br>
                                    At point $(3, 0)$: $\nabla f = (6, 0)$ - points in the $x$-direction<br>
                                    At origin $(0, 0)$: $\nabla f = (0, 0)$ - zero vector (this is the minimum!)<br><br>
                                    The gradient vectors point radially outward from the origin, indicating the direction of steepest increase.</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 2:</strong> Temperature distribution: $T(x, y) = 100e^{-(x^2+y^2)}$ (hottest at origin, cools off with distance).<br><br>
                                    $\nabla T = \left(\frac{\partial T}{\partial x}, \frac{\partial T}{\partial y}\right) = (-200xe^{-(x^2+y^2)}, -200ye^{-(x^2+y^2)})$<br>
                                    $= -200e^{-(x^2+y^2)}(x, y)$<br><br>
                                    The gradient points radially inward (toward the origin) because temperature increases as you move toward the center. Heat flows opposite to the gradient - from hot to cold!</p>
                                </div>

                                <h3>The Directional Derivative</h3>
                                
                                <p>What if we want to know how $f$ changes in a direction that's not aligned with the coordinate axes? The <strong>directional derivative</strong> in the direction of a unit vector $\hat{u}$ is:</p>

                                <div class="math-block">
                                    $$D_{\hat{u}}f = \nabla f \cdot \hat{u}$$
                                </div>

                                <p>This is the dot product of the gradient with the direction vector! It tells us the rate of change of $f$ as we move in direction $\hat{u}$. The maximum value of $D_{\hat{u}}f$ occurs when $\hat{u}$ points in the same direction as $\nabla f$ - confirming that the gradient points in the direction of maximum increase.</p>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof: Directional Derivative Formula</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>Let $\hat{u} = (u_1, u_2, u_3)$ be a unit vector (so $|\hat{u}| = 1$). The directional derivative is defined as:</p>
                                        <div class="math-block">
                                            $$D_{\hat{u}}f = \lim_{h \to 0} \frac{f(\vec{r} + h\hat{u}) - f(\vec{r})}{h}$$
                                        </div>
                                        <p>where $\vec{r} = (x, y, z)$. Expanding $f(\vec{r} + h\hat{u}) = f(x + hu_1, y + hu_2, z + hu_3)$ using the multivariable chain rule:</p>
                                        <div class="math-block">
                                            $$D_{\hat{u}}f = \frac{\partial f}{\partial x}\frac{dx}{dh} + \frac{\partial f}{\partial y}\frac{dy}{dh} + \frac{\partial f}{\partial z}\frac{dz}{dh}$$
                                        </div>
                                        <p>Since $x = x_0 + hu_1$, we have $\frac{dx}{dh} = u_1$. Similarly, $\frac{dy}{dh} = u_2$ and $\frac{dz}{dh} = u_3$. Therefore:</p>
                                        <div class="math-block">
                                            $$D_{\hat{u}}f = \frac{\partial f}{\partial x}u_1 + \frac{\partial f}{\partial y}u_2 + \frac{\partial f}{\partial z}u_3 = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}\right) \cdot (u_1, u_2, u_3) = \nabla f \cdot \hat{u}$$
                                        </div>
                                        <p><strong>QED.</strong> This proves the directional derivative is simply the dot product of the gradient with the direction!</p>
                                        <p><strong>Maximum Rate:</strong> Since $D_{\hat{u}}f = |\nabla f||\hat{u}|\cos\theta = |\nabla f|\cos\theta$ (as $|\hat{u}| = 1$), this is maximized when $\cos\theta = 1$, i.e., when $\hat{u}$ points in the same direction as $\nabla f$. The maximum rate is $|\nabla f|$.</p>
                                    </div>
                                </details>

                                <div class="example-box">
                                    <p><strong>Example:</strong> For $f(x, y) = x^2 - y^2$ at point $(1, 1)$, find the rate of change in the direction toward $(2, 3)$.<br><br>
                                    Step 1: Find gradient: $\nabla f = (2x, -2y)$, so at $(1,1)$: $\nabla f = (2, -2)$<br>
                                    Step 2: Find direction vector: from $(1,1)$ to $(2,3)$ is $(1, 2)$<br>
                                    Step 3: Normalize: $\hat{u} = \frac{(1,2)}{\sqrt{1^2+2^2}} = \frac{(1,2)}{\sqrt{5}}$<br>
                                    Step 4: Compute: $D_{\hat{u}}f = (2,-2) \cdot \frac{(1,2)}{\sqrt{5}} = \frac{2-4}{\sqrt{5}} = \frac{-2}{\sqrt{5}} \approx -0.89$<br><br>
                                    The function is decreasing in this direction at rate $\approx 0.89$ units per unit distance.</p>
                                </div>

                                <h3>Level Curves and Gradient Perpendicularity</h3>
                                
                                <p>A <strong>level curve</strong> (or <strong>contour line</strong>) of $f(x, y)$ is a curve along which $f$ has a constant value. Think of elevation contours on a topographic map - each line represents a constant elevation.</p>

                                <p>Here's a beautiful theorem: <strong>The gradient is always perpendicular to level curves!</strong> Why? Because if you move along a level curve, $f$ doesn't change at all - its directional derivative in that direction is zero. Since $D_{\hat{u}}f = \nabla f \cdot \hat{u} = 0$, the vectors $\nabla f$ and $\hat{u}$ must be perpendicular.</p>

                                <p>This has a profound implication: if you want to stay at constant elevation while hiking, walk perpendicular to the gradient. If you want to climb/descend as little as possible, walk perpendicular to the gradient!</p>

                                <div class="info-box">
                                    <p><strong>In Fluid Dynamics:</strong> The gradient of pressure $\nabla P$ determines the force on fluid particles. Fluids accelerate from high pressure to low pressure, in the direction of $-\nabla P$. This is why wind flows from high-pressure to low-pressure regions. In our Navier-Stokes simulations, the pressure gradient term $-\nabla P$ drives much of the fluid motion!</p>
                                </div>

                                <h3>Properties of the Gradient</h3>
                                
                                <p>The gradient operator $\nabla$ is linear and follows these useful rules:</p>

                                <ol>
                                    <li>$\nabla(f + g) = \nabla f + \nabla g$ (linearity)</li>
                                    <li>$\nabla(cf) = c\nabla f$ for constant $c$</li>
                                    <li>$\nabla(fg) = f\nabla g + g\nabla f$ (product rule)</li>
                                    <li>$\nabla\left(\frac{f}{g}\right) = \frac{g\nabla f - f\nabla g}{g^2}$ (quotient rule)</li>
                                    <li>$\nabla(f(g)) = f'(g)\nabla g$ (chain rule)</li>
                                </ol>

                                <p>These rules make gradient calculations much easier - we can break complex functions into simpler pieces!</p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 3.3 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">3.3</span>
                            <span>Divergence and Curl</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Divergence: Measuring Flow</h2>
                                
                                <p>While the gradient operates on scalar fields and produces vector fields, <strong>divergence</strong> operates on vector fields and produces scalar fields. For a vector field $\vec{F} = (F_x, F_y, F_z)$, the divergence is:</p>

                                <div class="math-block">
                                    $$\text{div}(\vec{F}) = \nabla \cdot \vec{F} = \frac{\partial F_x}{\partial x} + \frac{\partial F_y}{\partial y} + \frac{\partial F_z}{\partial z}$$
                                </div>

                                <p>The notation $\nabla \cdot \vec{F}$ treats $\nabla$ like a vector $\left(\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}\right)$ and takes its dot product with $\vec{F}$.</p>

                                <h3>Physical Interpretation</h3>
                                
                                <p>Divergence measures whether a vector field is "spreading out" (diverging) or "coming together" (converging) at a point. Think of $\vec{F}$ as the velocity field of a fluid:</p>

                                <ul>
                                    <li><strong>Positive divergence ($\nabla \cdot \vec{F} > 0$):</strong> More fluid is flowing out than in - this is a <em>source</em>. Imagine water spraying from a sprinkler head.</li>
                                    <li><strong>Negative divergence ($\nabla \cdot \vec{F} < 0$):</strong> More fluid is flowing in than out - this is a <em>sink</em>. Imagine water draining down a drain.</li>
                                    <li><strong>Zero divergence ($\nabla \cdot \vec{F} = 0$):</strong> As much flows in as flows out - the field is <em>incompressible</em> or <em>solenoidal</em>. Like water moving through a pipe - it can't be compressed, so what flows in must flow out.</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Example 1:</strong> Radial flow field $\vec{F} = (x, y, z)$ - flows radially outward from origin.<br><br>
                                    $\nabla \cdot \vec{F} = \frac{\partial x}{\partial x} + \frac{\partial y}{\partial y} + \frac{\partial z}{\partial z} = 1 + 1 + 1 = 3$<br><br>
                                    Positive divergence everywhere! This field has a source at the origin - material is being created and flowing outward. The value 3 is constant, meaning the "source strength" is uniform throughout space.</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 2:</strong> Rotational flow field $\vec{F} = (-y, x, 0)$ - circular flow around z-axis.<br><br>
                                    $\nabla \cdot \vec{F} = \frac{\partial(-y)}{\partial x} + \frac{\partial x}{\partial y} + \frac{\partial 0}{\partial z} = 0 + 0 + 0 = 0$<br><br>
                                    Zero divergence! This is an incompressible flow - fluid circulates around the axis but isn't being created or destroyed anywhere. This is like a vortex in water.</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 3:</strong> Converging field $\vec{F} = (-x, -y, -z)$ - flows radially inward toward origin.<br><br>
                                    $\nabla \cdot \vec{F} = \frac{\partial(-x)}{\partial x} + \frac{\partial(-y)}{\partial y} + \frac{\partial(-z)}{\partial z} = -1 - 1 - 1 = -3$<br><br>
                                    Negative divergence! This field has a sink at the origin - material is flowing inward and being consumed.</p>
                                </div>

                                <h3>Applications in Physics</h3>
                                
                                <p><strong>Incompressible Fluids:</strong> For fluids like water (which can't be compressed), the velocity field must satisfy $\nabla \cdot \vec{v} = 0$. This is a fundamental constraint in fluid dynamics simulations!</p>

                                <p><strong>Gauss's Law (Electromagnetism):</strong> The divergence of the electric field equals the charge density divided by a constant: $\nabla \cdot \vec{E} = \frac{\rho}{\epsilon_0}$. Positive charges are sources of electric field (positive divergence), negative charges are sinks.</p>

                                <p><strong>Conservation Laws:</strong> The continuity equation $\frac{\partial \rho}{\partial t} + \nabla \cdot (\rho\vec{v}) = 0$ expresses conservation of mass. If density is increasing somewhere ($\frac{\partial \rho}{\partial t} > 0$), then mass must be flowing inward ($\nabla \cdot (\rho\vec{v}) < 0$).</p>

                                <div class="info-box">
                                    <p><strong>In Our Simulations:</strong> The Navier-Stokes equations for incompressible flow require $\nabla \cdot \vec{v} = 0$. Maintaining this constraint is one of the biggest computational challenges! Our fluid simulations use sophisticated algorithms to ensure the velocity field remains divergence-free at every time step.</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>Curl: Measuring Rotation</h2>
                                
                                <p>While divergence measures expansion/contraction, <strong>curl</strong> measures rotation. For a vector field $\vec{F} = (F_x, F_y, F_z)$, the curl is another vector field:</p>

                                <div class="math-block">
                                    $$\text{curl}(\vec{F}) = \nabla \times \vec{F} = \begin{vmatrix} \hat{i} & \hat{j} & \hat{k} \\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\ F_x & F_y & F_z \end{vmatrix}$$
                                </div>

                                <p>Expanding this determinant:</p>

                                <div class="math-block">
                                    $$\nabla \times \vec{F} = \left(\frac{\partial F_z}{\partial y} - \frac{\partial F_y}{\partial z}, \frac{\partial F_x}{\partial z} - \frac{\partial F_z}{\partial x}, \frac{\partial F_y}{\partial x} - \frac{\partial F_x}{\partial y}\right)$$
                                </div>

                                <p>The notation $\nabla \times \vec{F}$ treats $\nabla$ as a vector and takes its cross product with $\vec{F}$!</p>

                                <h3>Physical Interpretation</h3>

                                <p>Curl measures the tendency of a vector field to rotate around a point. Imagine placing a tiny paddle wheel in a fluid flow - the curl tells you how fast and in which direction it would spin:</p>

                                <ul>
                                    <li><strong>Non-zero curl:</strong> The field has rotational component - like a whirlpool or tornado. The direction of the curl vector points along the axis of rotation (right-hand rule).</li>
                                    <li><strong>Zero curl ($\nabla \times \vec{F} = \vec{0}$):</strong> The field is <em>irrotational</em> or <em>conservative</em>. No local rotation - the field could be the gradient of a potential function.</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Example 1:</strong> Rotational field $\vec{F} = (-y, x, 0)$ - counterclockwise circulation around z-axis.<br><br>
                                    $\nabla \times \vec{F} = \left(\frac{\partial 0}{\partial y} - \frac{\partial x}{\partial z}, \frac{\partial(-y)}{\partial z} - \frac{\partial 0}{\partial x}, \frac{\partial x}{\partial x} - \frac{\partial(-y)}{\partial y}\right)$<br>
                                    $= (0 - 0, 0 - 0, 1 - (-1)) = (0, 0, 2)$<br><br>
                                    The curl points along the positive z-axis with magnitude 2. This confirms the field rotates counterclockwise (when viewed from above) around the z-axis!</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 2:</strong> Radial field $\vec{F} = (x, y, z)$ - flows outward from origin without rotation.<br><br>
                                    $\nabla \times \vec{F} = \left(\frac{\partial z}{\partial y} - \frac{\partial y}{\partial z}, \frac{\partial x}{\partial z} - \frac{\partial z}{\partial x}, \frac{\partial y}{\partial x} - \frac{\partial x}{\partial y}\right)$<br>
                                    $= (0 - 0, 0 - 0, 0 - 0) = (0, 0, 0)$<br><br>
                                    Zero curl! This field has no rotation - it's purely radial expansion.</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 3:</strong> Gradient field $\vec{F} = \nabla f$ where $f(x,y,z) = x^2 + y^2 + z^2$.<br><br>
                                    $\vec{F} = (2x, 2y, 2z)$<br>
                                    $\nabla \times \vec{F} = (0, 0, 0)$<br><br>
                                    All gradient fields have zero curl! This is a fundamental theorem: $\nabla \times (\nabla f) = \vec{0}$ for any scalar field $f$.</p>
                                </div>

                                <h3>Applications in Physics</h3>

                                <p><strong>Fluid Vorticity:</strong> In fluid dynamics, vorticity is defined as $\vec{\omega} = \nabla \times \vec{v}$, the curl of the velocity field. Regions with high vorticity are vortices - whirlpools, tornadoes, hurricanes. Our vortex simulations visualize this!</p>

                                <p><strong>Electromagnetic Fields:</strong> Maxwell's equations include curl operations. Faraday's law: $\nabla \times \vec{E} = -\frac{\partial \vec{B}}{\partial t}$ (changing magnetic field creates circulating electric field). Amp√®re's law: $\nabla \times \vec{B} = \mu_0\vec{J}$ (electric current creates circulating magnetic field).</p>

                                <p><strong>Conservative Fields:</strong> A force field is conservative if $\nabla \times \vec{F} = \vec{0}$. This means work done moving along a closed path is zero - energy is conserved! Gravitational and electrostatic fields are conservative.</p>

                                <div class="info-box">
                                    <p><strong>In Our Simulations:</strong> The Von K√°rm√°n vortex street simulation shows regions of high curl where vortices form behind obstacles. The Kelvin-Helmholtz instability creates rolling vortices with significant curl. Curl is the mathematical signature of swirling, rotating flow patterns!</p>
                                </div>

                                <h3>Important Identities</h3>

                                <p>There are several important vector calculus identities involving curl and divergence:</p>

                                <p><strong>1. Curl of gradient is zero:</strong> $\nabla \times (\nabla f) = \vec{0}$</p>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof: Curl of Gradient is Zero</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>Let $f(x, y, z)$ be a twice-differentiable scalar field. The gradient is:</p>
                                        <div class="math-block">
                                            $$\nabla f = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}\right)$$
                                        </div>
                                        <p>Now compute the curl of this vector field:</p>
                                        <div class="math-block">
                                            $$\nabla \times (\nabla f) = \left(\frac{\partial}{\partial y}\frac{\partial f}{\partial z} - \frac{\partial}{\partial z}\frac{\partial f}{\partial y}, \frac{\partial}{\partial z}\frac{\partial f}{\partial x} - \frac{\partial}{\partial x}\frac{\partial f}{\partial z}, \frac{\partial}{\partial x}\frac{\partial f}{\partial y} - \frac{\partial}{\partial y}\frac{\partial f}{\partial x}\right)$$
                                        </div>
                                        <p>Using Clairaut's Theorem (mixed partial derivatives are equal for nice functions):</p>
                                        <div class="math-block">
                                            $$\frac{\partial^2 f}{\partial y \partial z} = \frac{\partial^2 f}{\partial z \partial y}, \quad \frac{\partial^2 f}{\partial z \partial x} = \frac{\partial^2 f}{\partial x \partial z}, \quad \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}$$
                                        </div>
                                        <p>So each component becomes:</p>
                                        <div class="math-block">
                                            $$\nabla \times (\nabla f) = (0, 0, 0) = \vec{0}$$
                                        </div>
                                        <p><strong>QED.</strong> This proves that gradient fields are always irrotational! This is why conservative forces have no curl.</p>
                                    </div>
                                </details>

                                <p><strong>2. Divergence of curl is zero:</strong> $\nabla \cdot (\nabla \times \vec{F}) = 0$</p>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof: Divergence of Curl is Zero</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>Let $\vec{F} = (F_x, F_y, F_z)$ be a twice-differentiable vector field. The curl is:</p>
                                        <div class="math-block">
                                            $$\nabla \times \vec{F} = \left(\frac{\partial F_z}{\partial y} - \frac{\partial F_y}{\partial z}, \frac{\partial F_x}{\partial z} - \frac{\partial F_z}{\partial x}, \frac{\partial F_y}{\partial x} - \frac{\partial F_x}{\partial y}\right)$$
                                        </div>
                                        <p>Now compute the divergence of this:</p>
                                        <div class="math-block">
                                            $$\nabla \cdot (\nabla \times \vec{F}) = \frac{\partial}{\partial x}\left(\frac{\partial F_z}{\partial y} - \frac{\partial F_y}{\partial z}\right) + \frac{\partial}{\partial y}\left(\frac{\partial F_x}{\partial z} - \frac{\partial F_z}{\partial x}\right) + \frac{\partial}{\partial z}\left(\frac{\partial F_y}{\partial x} - \frac{\partial F_x}{\partial y}\right)$$
                                        </div>
                                        <p>Expanding:</p>
                                        <div class="math-block">
                                            $$= \frac{\partial^2 F_z}{\partial x \partial y} - \frac{\partial^2 F_y}{\partial x \partial z} + \frac{\partial^2 F_x}{\partial y \partial z} - \frac{\partial^2 F_z}{\partial y \partial x} + \frac{\partial^2 F_y}{\partial z \partial x} - \frac{\partial^2 F_x}{\partial z \partial y}$$
                                        </div>
                                        <p>By Clairaut's Theorem, mixed partials are equal, so terms cancel in pairs:</p>
                                        <div class="math-block">
                                            $$= \left(\frac{\partial^2 F_z}{\partial x \partial y} - \frac{\partial^2 F_z}{\partial y \partial x}\right) + \left(\frac{\partial^2 F_x}{\partial y \partial z} - \frac{\partial^2 F_x}{\partial z \partial y}\right) + \left(\frac{\partial^2 F_y}{\partial z \partial x} - \frac{\partial^2 F_y}{\partial x \partial z}\right) = 0$$
                                        </div>
                                        <p><strong>QED.</strong> This proves that curl fields are always solenoidal (divergence-free)! This is why magnetic fields have no divergence - they can be written as the curl of a vector potential.</p>
                                    </div>
                                </details>

                                <p><strong>3. Product rules exist for curl and divergence</strong></p>
                                <p><strong>4. Laplacian:</strong> $\nabla^2 f = \nabla \cdot (\nabla f)$ combines divergence and gradient</p>

                                <p>These identities constrain which fields are physically realizable and help simplify complex equations!</p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 3.4 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">3.4</span>
                            <span>Line Integrals and Path Independence</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Line Integrals: Integrating Along Curves</h2>

                                <p>In single-variable calculus, we integrate along the x-axis. In vector calculus, we can integrate along any curve in space. This is called a <strong>line integral</strong> (though it works with any curve, not just straight lines!).</p>

                                <p>There are two types of line integrals:</p>

                                <h3>Line Integral of a Scalar Field</h3>

                                <p>If $f(x, y, z)$ is a scalar field and $C$ is a curve parameterized by $\vec{r}(t)$ for $a \leq t \leq b$, then:</p>

                                <div class="math-block">
                                    $$\int_C f \, ds = \int_a^b f(\vec{r}(t)) |\vec{r}'(t)| \, dt$$
                                </div>

                                <p>Here $ds$ represents an infinitesimal arc length element along the curve. This integral "adds up" the values of $f$ along the entire path, weighted by distance traveled.</p>

                                <h3>Line Integral of a Vector Field</h3>

                                <p>If $\vec{F}(x, y, z)$ is a vector field and $C$ is a curve parameterized by $\vec{r}(t)$, then:</p>

                                <div class="math-block">
                                    $$\int_C \vec{F} \cdot d\vec{r} = \int_a^b \vec{F}(\vec{r}(t)) \cdot \vec{r}'(t) \, dt$$
                                </div>

                                <p>This measures how much the vector field "goes along" the curve. In physics, this represents <strong>work</strong> done by a force field $\vec{F}$ moving an object along path $C$!</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Calculate the work done by force $\vec{F} = (y, -x, 0)$ moving a particle along the semicircle $\vec{r}(t) = (\cos t, \sin t, 0)$ from $t=0$ to $t=\pi$.<br><br>
                                    $\vec{r}'(t) = (-\sin t, \cos t, 0)$<br>
                                    $\vec{F}(\vec{r}(t)) = (\sin t, -\cos t, 0)$<br>
                                    $\vec{F} \cdot \vec{r}' = (\sin t)(-\sin t) + (-\cos t)(\cos t) + 0 = -\sin^2 t - \cos^2 t = -1$<br>
                                    $\int_C \vec{F} \cdot d\vec{r} = \int_0^\pi (-1) \, dt = -\pi$<br><br>
                                    The force does negative work (opposes motion) totaling $-\pi$ units.</p>
                                </div>

                                <h3>Conservative Fields and Path Independence</h3>

                                <p>A vector field $\vec{F}$ is <strong>conservative</strong> if it can be written as the gradient of a scalar potential function: $\vec{F} = \nabla f$ for some $f$. Conservative fields have remarkable properties:</p>

                                <ol>
                                    <li><strong>Path independence:</strong> $\int_C \vec{F} \cdot d\vec{r}$ depends only on the endpoints, not the path taken!</li>
                                    <li><strong>Zero circulation:</strong> $\oint_C \vec{F} \cdot d\vec{r} = 0$ for any closed path</li>
                                    <li><strong>Irrotational:</strong> $\nabla \times \vec{F} = \vec{0}$</li>
                                </ol>

                                <p>If $\vec{F} = \nabla f$, then:</p>

                                <div class="math-block">
                                    $$\int_C \vec{F} \cdot d\vec{r} = f(\text{end}) - f(\text{start})$$
                                </div>

                                <p>This is the <strong>Fundamental Theorem for Line Integrals</strong> - it's just like the Fundamental Theorem of Calculus, but for paths in space!</p>

                                <div class="info-box">
                                    <p><strong>Physical Meaning:</strong> Conservative forces (like gravity) satisfy energy conservation. The work done moving between two points is independent of path - it depends only on the change in potential energy. That's why it's called "conservative"!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 3.5 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">3.5</span>
                            <span>Surface Integrals and Flux</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Surface Integrals</h2>

                                <p>Just as line integrals extend integration to curves, <strong>surface integrals</strong> extend integration to 2D surfaces embedded in 3D space. This lets us calculate quantities like surface area, mass of a thin shell, or flux through a surface.</p>

                                <h3>Surface Integral of a Scalar Field</h3>

                                <p>For a scalar field $f(x, y, z)$ over a surface $S$ parameterized by $\vec{r}(u, v)$:</p>

                                <div class="math-block">
                                    $$\iint_S f \, dS = \iint_D f(\vec{r}(u,v)) \left|\frac{\partial \vec{r}}{\partial u} \times \frac{\partial \vec{r}}{\partial v}\right| \, du \, dv$$
                                </div>

                                <p>The cross product $\frac{\partial \vec{r}}{\partial u} \times \frac{\partial \vec{r}}{\partial v}$ gives a normal vector to the surface, and its magnitude is the local "area scaling factor."</p>

                                <h3>Flux: Surface Integral of a Vector Field</h3>

                                <p>The <strong>flux</strong> of a vector field $\vec{F}$ through a surface $S$ measures how much of the field "flows through" the surface:</p>

                                <div class="math-block">
                                    $$\text{Flux} = \iint_S \vec{F} \cdot \hat{n} \, dS = \iint_S \vec{F} \cdot d\vec{S}$$
                                </div>

                                <p>where $\hat{n}$ is the unit normal vector to the surface. The notation $d\vec{S} = \hat{n} \, dS$ combines direction (normal) and magnitude (area element).</p>

                                <div class="example-box">
                                    <p><strong>Physical Example:</strong> Imagine holding a window screen in the wind. The flux of the velocity field through the screen depends on:<br>
                                    ‚Ä¢ How strong the wind is (magnitude of $\vec{v}$)<br>
                                    ‚Ä¢ How large the screen is (area $dS$)<br>
                                    ‚Ä¢ The angle between wind and screen (dot product with normal)<br>
                                    If the wind blows parallel to the screen, flux is zero. If perpendicular, flux is maximum!</p>
                                </div>

                                <h3>Applications of Flux</h3>

                                <p><strong>Fluid Flow:</strong> Flux of velocity field through a surface gives the volume flow rate - how much fluid passes through per unit time.</p>

                                <p><strong>Heat Flow:</strong> Flux of heat current through a surface tells how much thermal energy crosses it.</p>

                                <p><strong>Electric Flux:</strong> Gauss's law relates electric flux through a closed surface to enclosed charge: $\oint_S \vec{E} \cdot d\vec{S} = \frac{Q_{\text{enc}}}{\epsilon_0}$</p>

                                <div class="info-box">
                                    <p><strong>In Our Simulations:</strong> When we visualize fluid flow around obstacles, we're essentially looking at flux patterns. High flux regions show where lots of fluid is moving through. Zero flux boundaries (like solid walls) are where the velocity field is parallel to the surface.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 3.6 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">3.6</span>
                            <span>The Fundamental Theorems</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Green's Theorem (2D)</h2>

                                <p><strong>Green's Theorem</strong> connects a line integral around a closed curve to a double integral over the region it encloses. For a vector field $\vec{F} = (P, Q)$ in 2D:</p>

                                <div class="math-block">
                                    $$\oint_C (P \, dx + Q \, dy) = \iint_D \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right) \, dA$$
                                </div>

                                <p>The left side is a line integral around boundary curve $C$. The right side is a double integral over region $D$. The expression $\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}$ is related to the 2D curl!</p>

                                <div class="example-box">
                                    <p><strong>Intuition:</strong> Imagine a parking lot with cars driving in loops. Green's theorem says: the total circulation around the boundary equals the sum of all the little circulations (vorticity) inside!</p>
                                </div>

                                <h3>Stokes' Theorem (3D)</h3>

                                <p><strong>Stokes' Theorem</strong> is the 3D generalization of Green's theorem. It relates the curl of a vector field over a surface to a line integral around the boundary:</p>

                                <div class="math-block">
                                    $$\oint_{\partial S} \vec{F} \cdot d\vec{r} = \iint_S (\nabla \times \vec{F}) \cdot \hat{n} \, dS$$
                                </div>

                                <p>The left side: circulation of $\vec{F}$ around boundary curve $\partial S$<br>
                                The right side: flux of curl through surface $S$</p>

                                <p><strong>Physical Meaning:</strong> The circulation around the boundary equals the total rotational tendency (curl) integrated over the interior. If you have a soap film stretched across a wire loop and hold it in flowing water, Stokes' theorem relates the force making the film spin to the vorticity of the water!</p>

                                <h3>Divergence Theorem (Gauss's Theorem)</h3>

                                <p>The <strong>Divergence Theorem</strong> relates the divergence of a vector field in a volume to the flux through its boundary surface:</p>

                                <div class="math-block">
                                    $$\iiint_V (\nabla \cdot \vec{F}) \, dV = \oint_{\partial V} \vec{F} \cdot \hat{n} \, dS$$
                                </div>

                                <p>The left side: volume integral of divergence throughout region $V$<br>
                                The right side: flux through boundary surface $\partial V$</p>

                                <p><strong>Physical Meaning:</strong> If divergence represents sources/sinks inside a volume, the total amount created inside must equal the net flow out through the boundary. This is the mathematical expression of conservation laws!</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Conservation of mass: If $\vec{F} = \rho\vec{v}$ is mass flux, then divergence theorem says: rate of mass increase inside = net mass flow in through boundary. This is why $\frac{\partial \rho}{\partial t} + \nabla \cdot (\rho\vec{v}) = 0$ (continuity equation)!</p>
                                </div>

                                <div class="info-box">
                                    <p><strong>Why These Theorems Matter:</strong> These three theorems are the foundation of all conservation laws in physics! They connect local properties (divergence, curl) to global properties (flux, circulation). Every major physics equation - from fluid dynamics to electromagnetism to quantum mechanics - relies on these fundamental relationships!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 3.7 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">3.7</span>
                            <span>Multivariable Calculus Applications</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Optimization in Multiple Variables</h2>

                                <p>Finding maxima and minima of multivariable functions is crucial in physics, engineering, and data science. For a function $f(x, y, z)$:</p>

                                <h3>Critical Points</h3>

                                <p>A point is <strong>critical</strong> if $\nabla f = \vec{0}$. This is where all partial derivatives are zero - analogous to $f'(x) = 0$ in single-variable calculus. Critical points can be:</p>

                                <ul>
                                    <li><strong>Local maximum:</strong> Peak of a hill</li>
                                    <li><strong>Local minimum:</strong> Bottom of a valley</li>
                                    <li><strong>Saddle point:</strong> Mountain pass - max in one direction, min in another</li>
                                </ul>

                                <h3>Second Derivative Test (2D)</h3>

                                <p>For $f(x, y)$ at critical point $(x_0, y_0)$, compute the <strong>Hessian determinant</strong>:</p>

                                <div class="math-block">
                                    $$D = f_{xx}f_{yy} - (f_{xy})^2$$
                                </div>

                                <p>Classification:</p>
                                <ul>
                                    <li>If $D > 0$ and $f_{xx} > 0$: local minimum</li>
                                    <li>If $D > 0$ and $f_{xx} < 0$: local maximum</li>
                                    <li>If $D < 0$: saddle point</li>
                                    <li>If $D = 0$: test is inconclusive</li>
                                </ul>

                                <div class="info-box">
                                    <p><strong>In Physics:</strong> Systems naturally evolve toward configurations that minimize potential energy. Finding $\nabla U = \vec{0}$ (where $U$ is potential energy) gives equilibrium positions. Stable equilibria are local minima!</p>
                                </div>

                                <h3>Constrained Optimization: Lagrange Multipliers</h3>

                                <p>Often we want to optimize $f(x, y, z)$ subject to a constraint $g(x, y, z) = c$. The method of <strong>Lagrange multipliers</strong> says: at an optimum point, $\nabla f$ and $\nabla g$ are parallel:</p>

                                <div class="math-block">
                                    $$\nabla f = \lambda \nabla g$$
                                </div>

                                <p>This gives us equations to solve for the optimal point. The constant $\lambda$ is called the Lagrange multiplier.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Find the point on plane $x + y + z = 1$ closest to origin.<br>
                                    Minimize: $f(x,y,z) = x^2 + y^2 + z^2$ (square of distance)<br>
                                    Constraint: $g(x,y,z) = x + y + z = 1$<br>
                                    $\nabla f = (2x, 2y, 2z)$, $\nabla g = (1, 1, 1)$<br>
                                    Setting $\nabla f = \lambda \nabla g$: $(2x, 2y, 2z) = \lambda(1, 1, 1)$<br>
                                    This gives $x = y = z = \lambda/2$. Using constraint: $3(\lambda/2) = 1$, so $\lambda = 2/3$<br>
                                    Solution: $(1/3, 1/3, 1/3)$ with distance $\frac{1}{\sqrt{3}}$ from origin.</p>
                                </div>

                                <h3>Vector Calculus in Coordinate Systems</h3>

                                <p>We've been working in Cartesian coordinates, but gradient, divergence, and curl can be expressed in other coordinate systems too!</p>

                                <p><strong>Cylindrical Coordinates $(r, \theta, z)$:</strong></p>
                                <div class="math-block">
                                    $$\nabla f = \left(\frac{\partial f}{\partial r}, \frac{1}{r}\frac{\partial f}{\partial \theta}, \frac{\partial f}{\partial z}\right)$$
                                </div>

                                <p><strong>Spherical Coordinates $(r, \theta, \phi)$:</strong></p>
                                <div class="math-block">
                                    $$\nabla f = \left(\frac{\partial f}{\partial r}, \frac{1}{r}\frac{\partial f}{\partial \theta}, \frac{1}{r\sin\theta}\frac{\partial f}{\partial \phi}\right)$$
                                </div>

                                <p>These formulas account for how coordinate grid lines curve and scale with position. Using the right coordinate system for a problem's symmetry can dramatically simplify calculations!</p>

                                <div class="info-box">
                                    <p><strong>In Our Simulations:</strong> The Taylor-Couette flow (fluid between rotating cylinders) is naturally described in cylindrical coordinates. Gravitational problems with spherical symmetry use spherical coordinates. Choosing coordinates wisely makes the math tractable!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

            </div>

            <!-- Section 4: Differential Equations -->
            <div class="tutorial-content" id="diffeq">
                
                <!-- Subsection 4.1 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">4.1</span>
                            <span>Introduction to Differential Equations</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>What is a Differential Equation?</h2>
                                
                                <p>A <strong>differential equation</strong> is an equation that relates a function to its derivatives. While an algebraic equation like $x^2 - 5x + 6 = 0$ asks "what value of $x$ makes this true?", a differential equation asks "what <em>function</em> makes this true?"</p>

                                <p>Think about a falling object. Newton's second law says $F = ma$, which relates force to acceleration. But acceleration is the second derivative of position: $a = \frac{d^2 x}{dt^2}$. So we have:</p>

                                <div class="math-block">
                                    $$m\frac{d^2 x}{dt^2} = -mg$$
                                </div>

                                <p>This is a differential equation! It relates the function $x(t)$ (position) to its second derivative. Solving it means finding the function $x(t)$ that satisfies this relationship.</p>

                                <h3>Why Are Differential Equations Important?</h3>
                                
                                <p>Differential equations are the mathematical language of change and motion. They appear everywhere in science and engineering:</p>

                                <ul>
                                    <li><strong>Physics:</strong> Newton's laws, Maxwell's equations, Schr√∂dinger's equation, heat equation, wave equation - all are differential equations</li>
                                    <li><strong>Biology:</strong> Population growth, spread of diseases, neural activity, chemical reactions in cells</li>
                                    <li><strong>Economics:</strong> Option pricing, economic growth models, supply and demand dynamics</li>
                                    <li><strong>Engineering:</strong> Circuit analysis, control systems, structural mechanics, fluid dynamics</li>
                                    <li><strong>Chemistry:</strong> Reaction kinetics, diffusion, concentration gradients</li>
                                </ul>

                                <div class="info-box">
                                    <p><strong>In Gravitation¬≥:</strong> Every simulation you see is governed by differential equations! The Lorenz attractor, three-body problem, Navier-Stokes equations for fluids - they're all systems of differential equations. Understanding how to work with these equations is key to understanding the simulations.</p>
                                </div>

                                <h3>Classification of Differential Equations</h3>

                                <p>Differential equations come in many varieties, and we classify them in several ways:</p>

                                <h4>Ordinary vs. Partial Differential Equations</h4>

                                <p><strong>Ordinary Differential Equations (ODEs):</strong> Involve derivatives with respect to a single independent variable, typically time $t$.</p>
                                <div class="math-block">
                                    $$\frac{dy}{dt} = ky \quad \text{(exponential growth)}$$
                                </div>

                                <p><strong>Partial Differential Equations (PDEs):</strong> Involve partial derivatives with respect to multiple independent variables, like position and time.</p>
                                <div class="math-block">
                                    $$\frac{\partial u}{\partial t} = \alpha \frac{\partial^2 u}{\partial x^2} \quad \text{(heat equation)}$$
                                </div>

                                <h4>Order of a Differential Equation</h4>

                                <p>The <strong>order</strong> is the highest derivative that appears:</p>
                                <ul>
                                    <li><strong>First-order:</strong> $\frac{dy}{dx} = x + y$ (only first derivative)</li>
                                    <li><strong>Second-order:</strong> $\frac{d^2y}{dx^2} + y = 0$ (second derivative appears)</li>
                                    <li><strong>n-th order:</strong> Involves $n$-th derivative</li>
                                </ul>

                                <h4>Linear vs. Nonlinear</h4>

                                <p>A differential equation is <strong>linear</strong> if the unknown function and its derivatives appear linearly (to the first power, not multiplied together). Otherwise, it's <strong>nonlinear</strong>.</p>

                                <p><strong>Linear:</strong> $\frac{d^2y}{dx^2} + y = \sin x$ - $y$ and its derivatives appear to the first power only</p>

                                <p><strong>Nonlinear:</strong> $\frac{dy}{dx} = y^2$ or $\frac{d^2y}{dx^2} = \sin y$ - the function or its derivatives appear nonlinearly</p>

                                <p>Linear equations are much easier to solve! We have general methods for them. Nonlinear equations often require numerical methods or special techniques.</p>

                                <h4>Homogeneous vs. Inhomogeneous</h4>

                                <p>For linear differential equations:</p>

                                <p><strong>Homogeneous:</strong> All terms involve the unknown function or its derivatives. The right side is zero.</p>
                                <div class="math-block">
                                    $$\frac{d^2y}{dx^2} + 3\frac{dy}{dx} + 2y = 0$$
                                </div>

                                <p><strong>Inhomogeneous (non-homogeneous):</strong> There's a "forcing term" - a function that doesn't involve $y$.</p>
                                <div class="math-block">
                                    $$\frac{d^2y}{dx^2} + 3\frac{dy}{dx} + 2y = e^x$$
                                </div>

                                <h3>Solutions to Differential Equations</h3>

                                <p>A <strong>solution</strong> to a differential equation is a function that satisfies the equation when substituted in. There are different types:</p>

                                <p><strong>General Solution:</strong> Contains arbitrary constants. For an $n$-th order ODE, the general solution has $n$ arbitrary constants.</p>

                                <p><strong>Particular Solution:</strong> A specific solution obtained by choosing values for the constants, typically using <strong>initial conditions</strong> or <strong>boundary conditions</strong>.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Consider $\frac{dy}{dx} = 2x$<br><br>
                                    <strong>General solution:</strong> $y = x^2 + C$ (C is an arbitrary constant)<br>
                                    <strong>Particular solution:</strong> If $y(0) = 3$, then $3 = 0 + C$, so $C = 3$, giving $y = x^2 + 3$<br><br>
                                    We can verify: $\frac{dy}{dx} = 2x$ ‚úì and $y(0) = 0 + 3 = 3$ ‚úì</p>
                                </div>

                                <h3>Initial Value Problems vs. Boundary Value Problems</h3>

                                <p><strong>Initial Value Problem (IVP):</strong> All conditions specified at a single point (usually the starting time).</p>
                                <div class="math-block">
                                    $$\frac{d^2y}{dx^2} + y = 0, \quad y(0) = 1, \quad y'(0) = 0$$
                                </div>
                                <p>This is like knowing the initial position and velocity of a particle.</p>

                                <p><strong>Boundary Value Problem (BVP):</strong> Conditions specified at multiple points (boundaries of the domain).</p>
                                <div class="math-block">
                                    $$\frac{d^2y}{dx^2} + y = 0, \quad y(0) = 0, \quad y(\pi) = 0$$
                                </div>
                                <p>This is like knowing the position at both ends of a vibrating string.</p>

                                <div class="info-box">
                                    <p><strong>Existence and Uniqueness:</strong> Not all differential equations have solutions! And some have infinitely many. Existence and uniqueness theorems tell us when we're guaranteed a unique solution. For IVPs, if the equation is "nice enough" (continuous, Lipschitz continuous), there exists a unique solution near the initial point.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 4.2 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">4.2</span>
                            <span>First-Order ODEs: Separable Equations</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Separable Differential Equations</h2>
                                
                                <p>A <strong>separable</strong> differential equation can be written in the form:</p>

                                <div class="math-block">
                                    $$\frac{dy}{dx} = g(x)h(y)$$
                                </div>

                                <p>The key idea: we can separate the variables, getting all $y$-terms on one side and all $x$-terms on the other:</p>

                                <div class="math-block">
                                    $$\frac{1}{h(y)} \, dy = g(x) \, dx$$
                                </div>

                                <p>Then we integrate both sides:</p>

                                <div class="math-block">
                                    $$\int \frac{1}{h(y)} \, dy = \int g(x) \, dx + C$$
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 1:</strong> Solve $\frac{dy}{dx} = xy$<br><br>
                                    <strong>Step 1:</strong> Separate variables: $\frac{1}{y} \, dy = x \, dx$<br>
                                    <strong>Step 2:</strong> Integrate both sides: $\int \frac{1}{y} \, dy = \int x \, dx$<br>
                                    <strong>Step 3:</strong> $\ln|y| = \frac{x^2}{2} + C$<br>
                                    <strong>Step 4:</strong> Solve for $y$: $|y| = e^{x^2/2 + C} = e^C \cdot e^{x^2/2}$<br>
                                    <strong>General solution:</strong> $y = Ae^{x^2/2}$ where $A = \pm e^C$ (can be any nonzero constant)<br><br>
                                    We can verify: $\frac{dy}{dx} = A \cdot x \cdot e^{x^2/2} = x \cdot (Ae^{x^2/2}) = xy$ ‚úì</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 2:</strong> Solve $\frac{dy}{dx} = \frac{x}{y}$ with $y(0) = 2$<br><br>
                                    <strong>Step 1:</strong> Separate: $y \, dy = x \, dx$<br>
                                    <strong>Step 2:</strong> Integrate: $\int y \, dy = \int x \, dx$<br>
                                    <strong>Step 3:</strong> $\frac{y^2}{2} = \frac{x^2}{2} + C$<br>
                                    <strong>Step 4:</strong> $y^2 = x^2 + 2C$ or $y^2 = x^2 + K$ (where $K = 2C$)<br>
                                    <strong>Initial condition:</strong> $y(0) = 2$ gives $4 = 0 + K$, so $K = 4$<br>
                                    <strong>Particular solution:</strong> $y^2 = x^2 + 4$, or $y = \sqrt{x^2 + 4}$ (taking positive root since $y(0) = 2 > 0$)</p>
                                </div>

                                <h3>Important Applications</h3>

                                <p><strong>Exponential Growth/Decay:</strong> The equation $\frac{dy}{dt} = ky$ describes many natural processes:</p>
                                <ul>
                                    <li>Population growth ($k > 0$)</li>
                                    <li>Radioactive decay ($k < 0$)</li>
                                    <li>Compound interest</li>
                                    <li>Cooling/heating (Newton's law of cooling)</li>
                                </ul>

                                <p>Separating and integrating: $\frac{dy}{y} = k \, dt$ gives $\ln|y| = kt + C$, so $y = Ae^{kt}$</p>

                                <div class="example-box">
                                    <p><strong>Application:</strong> Carbon-14 dating uses the decay equation. If $y(t)$ is the amount of C-14 at time $t$:<br>
                                    $\frac{dy}{dt} = -\lambda y$ where $\lambda = \frac{\ln 2}{5730}$ (half-life is 5730 years)<br>
                                    Solution: $y(t) = y_0 e^{-\lambda t}$<br>
                                    By measuring how much C-14 remains, we can calculate how long ago an organism died!</p>
                                </div>

                                <p><strong>Logistic Growth:</strong> More realistic population model accounting for limited resources:</p>

                                <div class="math-block">
                                    $$\frac{dP}{dt} = rP\left(1 - \frac{P}{K}\right)$$
                                </div>

                                <p>where $r$ is growth rate and $K$ is carrying capacity. This is separable! Solving gives:</p>

                                <div class="math-block">
                                    $$P(t) = \frac{K}{1 + Ae^{-rt}}$$
                                </div>

                                <p>The S-shaped logistic curve: growth starts exponential but slows as $P$ approaches $K$.</p>

                                <div class="info-box">
                                    <p><strong>In Our Simulations:</strong> While our chaotic systems involve more complex equations, understanding exponential growth is key to seeing how small changes can grow rapidly - the butterfly effect! The separation of variables technique you learn here extends to more complex systems using numerical methods.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 4.3 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">4.3</span>
                            <span>First-Order ODEs: Linear Equations</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>First-Order Linear Differential Equations</h2>
                                
                                <p>A <strong>first-order linear ODE</strong> has the standard form:</p>

                                <div class="math-block">
                                    $$\frac{dy}{dx} + P(x)y = Q(x)$$
                                </div>

                                <p>The key feature: $y$ and $\frac{dy}{dx}$ appear only to the first power, not multiplied together or in nonlinear functions.</p>

                                <h3>The Integrating Factor Method</h3>

                                <p>The solution technique uses an <strong>integrating factor</strong> $\mu(x) = e^{\int P(x) \, dx}$. Here's why this works:</p>

                                <p>Multiply both sides by $\mu(x)$:</p>
                                <div class="math-block">
                                    $$\mu(x)\frac{dy}{dx} + \mu(x)P(x)y = \mu(x)Q(x)$$
                                </div>

                                <p>The magic: the left side is exactly the derivative of $\mu(x)y$ by the product rule!</p>
                                <div class="math-block">
                                    $$\frac{d}{dx}[\mu(x)y] = \mu(x)Q(x)$$
                                </div>

                                <p>Now integrate both sides:</p>
                                <div class="math-block">
                                    $$\mu(x)y = \int \mu(x)Q(x) \, dx + C$$
                                </div>

                                <p>Finally, solve for $y$:</p>
                                <div class="math-block">
                                    $$y = \frac{1}{\mu(x)}\left[\int \mu(x)Q(x) \, dx + C\right]$$
                                </div>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof: Why the Integrating Factor Works</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>We want to show that $\mu(x) = e^{\int P(x) \, dx}$ makes the left side of $\mu\frac{dy}{dx} + \mu P(x)y$ equal to $\frac{d}{dx}[\mu y]$.</p>
                                        
                                        <p>By the product rule:</p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}[\mu(x)y] = \mu'(x)y + \mu(x)\frac{dy}{dx}$$
                                        </div>
                                        
                                        <p>Now, since $\mu(x) = e^{\int P(x) \, dx}$, we have:</p>
                                        <div class="math-block">
                                            $$\mu'(x) = \frac{d}{dx}[e^{\int P(x) \, dx}] = P(x) \cdot e^{\int P(x) \, dx} = P(x)\mu(x)$$
                                        </div>
                                        
                                        <p>Substituting back:</p>
                                        <div class="math-block">
                                            $$\frac{d}{dx}[\mu(x)y] = P(x)\mu(x)y + \mu(x)\frac{dy}{dx} = \mu(x)\left[P(x)y + \frac{dy}{dx}\right]$$
                                        </div>
                                        
                                        <p>This is exactly what we get when we multiply the original equation by $\mu(x)$! <strong>QED.</strong></p>
                                    </div>
                                </details>

                                <h3>Step-by-Step Solution Process</h3>

                                <p><strong>1.</strong> Write equation in standard form: $\frac{dy}{dx} + P(x)y = Q(x)$</p>
                                <p><strong>2.</strong> Find integrating factor: $\mu(x) = e^{\int P(x) \, dx}$</p>
                                <p><strong>3.</strong> Multiply equation by $\mu(x)$</p>
                                <p><strong>4.</strong> Recognize left side as $\frac{d}{dx}[\mu(x)y]$</p>
                                <p><strong>5.</strong> Integrate: $\mu(x)y = \int \mu(x)Q(x) \, dx + C$</p>
                                <p><strong>6.</strong> Solve for $y$</p>

                                <div class="example-box">
                                    <p><strong>Example 1:</strong> Solve $\frac{dy}{dx} + 2y = 4$<br><br>
                                    <strong>Step 1:</strong> Already in standard form with $P(x) = 2$, $Q(x) = 4$<br>
                                    <strong>Step 2:</strong> $\mu(x) = e^{\int 2 \, dx} = e^{2x}$<br>
                                    <strong>Step 3:</strong> Multiply: $e^{2x}\frac{dy}{dx} + 2e^{2x}y = 4e^{2x}$<br>
                                    <strong>Step 4:</strong> Recognize: $\frac{d}{dx}[e^{2x}y] = 4e^{2x}$<br>
                                    <strong>Step 5:</strong> Integrate: $e^{2x}y = \int 4e^{2x} \, dx = 2e^{2x} + C$<br>
                                    <strong>Step 6:</strong> $y = 2 + Ce^{-2x}$<br><br>
                                    As $x \to \infty$, $y \to 2$ (the particular solution without the transient term).</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Example 2:</strong> Solve $x\frac{dy}{dx} + y = x^2$ with $y(1) = 0$<br><br>
                                    <strong>Step 1:</strong> Divide by $x$: $\frac{dy}{dx} + \frac{1}{x}y = x$ (standard form)<br>
                                    <strong>Step 2:</strong> $\mu(x) = e^{\int \frac{1}{x} \, dx} = e^{\ln|x|} = |x| = x$ (assuming $x > 0$)<br>
                                    <strong>Step 3:</strong> Multiply: $x\frac{dy}{dx} + y = x^2$<br>
                                    <strong>Step 4:</strong> $\frac{d}{dx}[xy] = x^2$<br>
                                    <strong>Step 5:</strong> Integrate: $xy = \int x^2 \, dx = \frac{x^3}{3} + C$<br>
                                    <strong>Step 6:</strong> $y = \frac{x^2}{3} + \frac{C}{x}$<br>
                                    <strong>Initial condition:</strong> $y(1) = 0$ gives $0 = \frac{1}{3} + C$, so $C = -\frac{1}{3}$<br>
                                    <strong>Solution:</strong> $y = \frac{x^2}{3} - \frac{1}{3x} = \frac{x^3 - 1}{3x}$</p>
                                </div>

                                <h3>Applications</h3>

                                <p><strong>RC Circuits:</strong> For a resistor-capacitor circuit with voltage source $V(t)$:</p>
                                <div class="math-block">
                                    $$R\frac{dQ}{dt} + \frac{Q}{C} = V(t)$$
                                </div>
                                <p>This is first-order linear! It describes how charge $Q(t)$ on the capacitor changes.</p>

                                <p><strong>Mixing Problems:</strong> A tank contains solution being mixed. If solution flows in at rate $r_{\text{in}}$ with concentration $c_{\text{in}}$, and flows out at rate $r_{\text{out}}$:</p>
                                <div class="math-block">
                                    $$\frac{dA}{dt} = r_{\text{in}}c_{\text{in}} - r_{\text{out}}\frac{A}{V}$$
                                </div>
                                <p>where $A(t)$ is amount of substance and $V$ is tank volume. Linear ODE!</p>

                                <p><strong>Newton's Law of Cooling:</strong> An object's temperature $T(t)$ in environment at temperature $T_{\text{env}}$:</p>
                                <div class="math-block">
                                    $$\frac{dT}{dt} = -k(T - T_{\text{env}})$$
                                </div>
                                <p>Rearranging: $\frac{dT}{dt} + kT = kT_{\text{env}}$ - first-order linear!</p>

                                <div class="info-box">
                                    <p><strong>Key Insight:</strong> The general solution always has form: $y = y_{\text{particular}} + y_{\text{homogeneous}}$. The homogeneous solution (solving $\frac{dy}{dx} + P(x)y = 0$) gives transient behavior that decays away. The particular solution gives the steady-state or forced response.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 4.4 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">4.4</span>
                            <span>First-Order ODEs: Exact Equations</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Exact Differential Equations</h2>
                                
                                <p>An <strong>exact</strong> differential equation has the form:</p>

                                <div class="math-block">
                                    $$M(x, y) \, dx + N(x, y) \, dy = 0$$
                                </div>

                                <p>where there exists a function $F(x, y)$ such that:</p>

                                <div class="math-block">
                                    $$\frac{\partial F}{\partial x} = M \quad \text{and} \quad \frac{\partial F}{\partial y} = N$$
                                </div>

                                <p>If such a function exists, the differential equation is exact, and its general solution is simply:</p>

                                <div class="math-block">
                                    $$F(x, y) = C$$
                                </div>

                                <h3>Test for Exactness</h3>

                                <p>How do we know if an equation is exact? Use this test: The equation $M \, dx + N \, dy = 0$ is exact if and only if:</p>

                                <div class="math-block">
                                    $$\frac{\partial M}{\partial y} = \frac{\partial N}{\partial x}$$
                                </div>

                                <p>This follows from Clairaut's theorem on mixed partial derivatives!</p>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof: Why This Test Works</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>If the equation is exact, then $M = \frac{\partial F}{\partial x}$ and $N = \frac{\partial F}{\partial y}$ for some function $F$.</p>
                                        
                                        <p>Taking partial derivatives:</p>
                                        <div class="math-block">
                                            $$\frac{\partial M}{\partial y} = \frac{\partial}{\partial y}\left(\frac{\partial F}{\partial x}\right) = \frac{\partial^2 F}{\partial y \partial x}$$
                                        </div>
                                        <div class="math-block">
                                            $$\frac{\partial N}{\partial x} = \frac{\partial}{\partial x}\left(\frac{\partial F}{\partial y}\right) = \frac{\partial^2 F}{\partial x \partial y}$$
                                        </div>
                                        
                                        <p>By Clairaut's theorem (assuming $F$ is nice enough), mixed partials are equal:</p>
                                        <div class="math-block">
                                            $$\frac{\partial^2 F}{\partial y \partial x} = \frac{\partial^2 F}{\partial x \partial y}$$
                                        </div>
                                        
                                        <p>Therefore:</p>
                                        <div class="math-block">
                                            $$\frac{\partial M}{\partial y} = \frac{\partial N}{\partial x}$$
                                        </div>
                                        <p><strong>QED.</strong> This test is both necessary and sufficient for exactness!</p>
                                    </div>
                                </details>

                                <h3>Finding the Solution</h3>

                                <p>If the equation passes the exactness test, we find $F(x, y)$ by:</p>

                                <p><strong>Method 1:</strong> Integrate $M$ with respect to $x$, treating $y$ as constant:
                                $$F(x, y) = \int M(x, y) \, dx + g(y)$$
                                where $g(y)$ is an unknown function of $y$ only.</p>

                                <p>Then differentiate with respect to $y$ and set equal to $N$ to find $g(y)$.</p>

                                <p><strong>Method 2:</strong> Integrate $N$ with respect to $y$, treating $x$ as constant:
                                $$F(x, y) = \int N(x, y) \, dy + h(x)$$
                                where $h(x)$ is an unknown function of $x$ only.</p>

                                <p>Then differentiate with respect to $x$ and set equal to $M$ to find $h(x)$.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Solve $(2xy + 1) \, dx + (x^2 - 1) \, dy = 0$<br><br>
                                    <strong>Test for exactness:</strong><br>
                                    $M = 2xy + 1$, so $\frac{\partial M}{\partial y} = 2x$<br>
                                    $N = x^2 - 1$, so $\frac{\partial N}{\partial x} = 2x$<br>
                                    Since $\frac{\partial M}{\partial y} = \frac{\partial N}{\partial x}$, the equation is exact! ‚úì<br><br>
                                    <strong>Find F:</strong><br>
                                    $F = \int M \, dx = \int (2xy + 1) \, dx = x^2y + x + g(y)$<br>
                                    Now $\frac{\partial F}{\partial y} = x^2 + g'(y)$ must equal $N = x^2 - 1$<br>
                                    So $x^2 + g'(y) = x^2 - 1$, giving $g'(y) = -1$, thus $g(y) = -y$<br>
                                    Therefore $F(x, y) = x^2y + x - y$<br><br>
                                    <strong>Solution:</strong> $x^2y + x - y = C$</p>
                                </div>

                                <h3>Integrating Factors for Non-Exact Equations</h3>

                                <p>If an equation isn't exact, we might be able to multiply by an <strong>integrating factor</strong> $\mu(x, y)$ to make it exact. Common cases:</p>

                                <p><strong>If $\frac{1}{N}\left(\frac{\partial M}{\partial y} - \frac{\partial N}{\partial x}\right)$ depends only on $x$:</strong></p>
                                <p>Then $\mu(x) = e^{\int \frac{1}{N}\left(\frac{\partial M}{\partial y} - \frac{\partial N}{\partial x}\right) dx}$ works</p>

                                <p><strong>If $\frac{1}{M}\left(\frac{\partial N}{\partial x} - \frac{\partial M}{\partial y}\right)$ depends only on $y$:</strong></p>
                                <p>Then $\mu(y) = e^{\int \frac{1}{M}\left(\frac{\partial N}{\partial x} - \frac{\partial M}{\partial y}\right) dy}$ works</p>

                                <div class="info-box">
                                    <p><strong>Connection to Physics:</strong> Exact equations arise naturally in conservative systems. If forces are conservative (derivable from a potential), Newton's laws lead to exact differential equations. The potential energy function is exactly the function $F$ we're finding!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 4.5 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">4.5</span>
                            <span>Second-Order Linear ODEs</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Second-Order Linear Differential Equations</h2>
                                
                                <p>A <strong>second-order linear ODE</strong> has the standard form:</p>

                                <div class="math-block">
                                    $$a(x)\frac{d^2y}{dx^2} + b(x)\frac{dy}{dx} + c(x)y = f(x)$$
                                </div>

                                <p>If $f(x) = 0$, it's <strong>homogeneous</strong>. Otherwise, it's <strong>inhomogeneous</strong>.</p>

                                <p>Second-order ODEs are crucial in physics - they describe oscillations, waves, and many mechanical systems!</p>

                                <h3>Constant Coefficient Homogeneous Equations</h3>

                                <p>For equations with constant coefficients:</p>

                                <div class="math-block">
                                    $$ay'' + by' + cy = 0$$
                                </div>

                                <p>We use the <strong>characteristic equation</strong> method. Try a solution of the form $y = e^{rx}$:</p>

                                <div class="math-block">
                                    $$ar^2e^{rx} + bre^{rx} + ce^{rx} = 0$$
                                </div>

                                <p>Since $e^{rx} \neq 0$, we get the <strong>characteristic equation</strong>:</p>

                                <div class="math-block">
                                    $$ar^2 + br + c = 0$$
                                </div>

                                <p>Using the quadratic formula: $r = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$</p>

                                <h3>Three Cases Based on Discriminant</h3>

                                <p><strong>Case 1: Two distinct real roots</strong> ($b^2 - 4ac > 0$): $r_1, r_2$</p>
                                <p>General solution: $y = C_1e^{r_1x} + C_2e^{r_2x}$</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Solve $y'' - 5y' + 6y = 0$<br>
                                    Characteristic equation: $r^2 - 5r + 6 = 0$<br>
                                    $(r - 2)(r - 3) = 0$, so $r_1 = 2$, $r_2 = 3$<br>
                                    Solution: $y = C_1e^{2x} + C_2e^{3x}$</p>
                                </div>

                                <p><strong>Case 2: One repeated real root</strong> ($b^2 - 4ac = 0$): $r$ (double root)</p>
                                <p>General solution: $y = C_1e^{rx} + C_2xe^{rx} = (C_1 + C_2x)e^{rx}$</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Solve $y'' - 4y' + 4y = 0$<br>
                                    Characteristic equation: $r^2 - 4r + 4 = (r - 2)^2 = 0$<br>
                                    Double root: $r = 2$<br>
                                    Solution: $y = (C_1 + C_2x)e^{2x}$</p>
                                </div>

                                <p><strong>Case 3: Complex conjugate roots</strong> ($b^2 - 4ac < 0$): $r = \alpha \pm i\beta$</p>
                                <p>General solution: $y = e^{\alpha x}(C_1\cos(\beta x) + C_2\sin(\beta x))$</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Solve $y'' + 4y = 0$<br>
                                    Characteristic equation: $r^2 + 4 = 0$<br>
                                    $r^2 = -4$, so $r = \pm 2i$ (purely imaginary: $\alpha = 0$, $\beta = 2$)<br>
                                    Solution: $y = C_1\cos(2x) + C_2\sin(2x)$<br>
                                    This represents oscillatory behavior!</p>
                                </div>

                                <h3>Physical Applications: Harmonic Oscillator</h3>

                                <p>The simple harmonic oscillator equation describes springs, pendulums, and many oscillating systems:</p>

                                <div class="math-block">
                                    $$m\frac{d^2x}{dt^2} + kx = 0$$
                                </div>

                                <p>Dividing by $m$: $\frac{d^2x}{dt^2} + \omega^2x = 0$ where $\omega = \sqrt{k/m}$ is the natural frequency.</p>

                                <p>Characteristic equation: $r^2 + \omega^2 = 0$ gives $r = \pm i\omega$</p>

                                <p>Solution: $x(t) = A\cos(\omega t) + B\sin(\omega t) = C\cos(\omega t + \phi)$</p>

                                <p>This is simple harmonic motion! The system oscillates with period $T = \frac{2\pi}{\omega}$.</p>

                                <h3>Damped Harmonic Oscillator</h3>

                                <p>Adding friction or resistance gives the <strong>damped harmonic oscillator</strong>:</p>

                                <div class="math-block">
                                    $$m\frac{d^2x}{dt^2} + \gamma\frac{dx}{dt} + kx = 0$$
                                </div>

                                <p>where $\gamma$ is the damping coefficient. This leads to three behaviors based on the discriminant:</p>

                                <ul>
                                    <li><strong>Underdamped</strong> ($\gamma^2 < 4mk$): Oscillates with decreasing amplitude</li>
                                    <li><strong>Critically damped</strong> ($\gamma^2 = 4mk$): Returns to equilibrium as fast as possible without oscillating</li>
                                    <li><strong>Overdamped</strong> ($\gamma^2 > 4mk$): Returns to equilibrium slowly without oscillating</li>
                                </ul>

                                <div class="info-box">
                                    <p><strong>In Our Simulations:</strong> The double pendulum involves second-order ODEs! Each angle requires a second-order equation. The Malkus waterwheel is also governed by second-order equations describing angular acceleration. Understanding these solutions helps predict whether systems oscillate or settle down.</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>Inhomogeneous Equations: Method of Undetermined Coefficients</h2>

                                <p>For inhomogeneous equations $ay'' + by' + cy = f(x)$ with constant coefficients, the solution has two parts:</p>

                                <div class="math-block">
                                    $$y = y_h + y_p$$
                                </div>

                                <p>where $y_h$ is the homogeneous solution (what we just learned) and $y_p$ is a <strong>particular solution</strong>.</p>

                                <h3>Finding the Particular Solution</h3>

                                <p>The <strong>method of undetermined coefficients</strong> works when $f(x)$ is a polynomial, exponential, sine/cosine, or combinations. We guess the form of $y_p$ based on $f(x)$:</p>

                                <ul>
                                    <li>If $f(x) = P_n(x)$ (polynomial of degree $n$): try $y_p = A_nx^n + A_{n-1}x^{n-1} + \cdots + A_0$</li>
                                    <li>If $f(x) = Ke^{\alpha x}$: try $y_p = Ae^{\alpha x}$</li>
                                    <li>If $f(x) = K\cos(\beta x)$ or $K\sin(\beta x)$: try $y_p = A\cos(\beta x) + B\sin(\beta x)$</li>
                                </ul>

                                <p><strong>Modification rule:</strong> If your guess is already part of the homogeneous solution, multiply by $x$ (or $x^2$ if needed).</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Solve $y'' + 4y = 8x^2$<br><br>
                                    <strong>Homogeneous solution:</strong> $r^2 + 4 = 0$ gives $r = \pm 2i$<br>
                                    $y_h = C_1\cos(2x) + C_2\sin(2x)$<br><br>
                                    <strong>Particular solution:</strong> Since $f(x) = 8x^2$, try $y_p = Ax^2 + Bx + C$<br>
                                    $y_p' = 2Ax + B$, $y_p'' = 2A$<br>
                                    Substitute: $2A + 4(Ax^2 + Bx + C) = 8x^2$<br>
                                    $4Ax^2 + 4Bx + (2A + 4C) = 8x^2$<br>
                                    Matching coefficients: $4A = 8 \Rightarrow A = 2$, $4B = 0 \Rightarrow B = 0$, $2A + 4C = 0 \Rightarrow C = -1$<br>
                                    $y_p = 2x^2 - 1$<br><br>
                                    <strong>General solution:</strong> $y = C_1\cos(2x) + C_2\sin(2x) + 2x^2 - 1$</p>
                                </div>

                                <h3>Variation of Parameters</h3>

                                <p>When undetermined coefficients doesn't work (e.g., $f(x) = \tan x$, $\ln x$, etc.), use <strong>variation of parameters</strong>. If the homogeneous solutions are $y_1$ and $y_2$, then:</p>

                                <div class="math-block">
                                    $$y_p = u_1(x)y_1 + u_2(x)y_2$$
                                </div>

                                <p>where $u_1$ and $u_2$ satisfy certain differential equations derived from the original equation. This method always works for linear equations!</p>

                                <div class="info-box">
                                    <p><strong>In Physics:</strong> Inhomogeneous equations describe driven or forced systems. For example, a mass-spring system driven by an external force $F(t)$ satisfies $my'' + \gamma y' + ky = F(t)$. The particular solution represents the forced response, while the homogeneous solution is the natural/transient response.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 4.6 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">4.6</span>
                            <span>Systems of ODEs and Phase Space</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Systems of Differential Equations</h2>
                                
                                <p>Many physical systems require multiple coupled differential equations. For example, tracking a particle in 3D space requires equations for $(x, y, z)$ positions and $(v_x, v_y, v_z)$ velocities - six equations total!</p>

                                <p>A <strong>system of first-order ODEs</strong> looks like:</p>

                                <div class="math-block">
                                    $$\frac{dx}{dt} = f(t, x, y)$$
                                    $$\frac{dy}{dt} = g(t, x, y)$$
                                </div>

                                <p>We can write this compactly using vector notation:</p>

                                <div class="math-block">
                                    $$\frac{d\vec{u}}{dt} = \vec{F}(t, \vec{u})$$
                                </div>

                                <p>where $\vec{u} = (x, y)$ and $\vec{F} = (f, g)$.</p>

                                <div class="info-box">
                                    <p><strong>Key Insight:</strong> Any higher-order ODE can be converted to a system of first-order ODEs! For example, $y'' + p(x)y' + q(x)y = r(x)$ becomes two first-order equations by introducing $v = y'$:</p>
                                    <div class="math-block">
                                        $$\frac{dy}{dx} = v$$
                                        $$\frac{dv}{dx} = r(x) - q(x)y - p(x)v$$
                                    </div>
                                </div>

                                <h3>Phase Space and Trajectories</h3>

                                <p><strong>Phase space</strong> is a geometric way to visualize solutions to differential equations. For a system with variables $(x, y)$, phase space is the $(x, y)$ plane. Each point represents a possible state of the system, and solutions trace out curves called <strong>trajectories</strong> or <strong>orbits</strong>.</p>

                                <p>Think of a pendulum: its state is completely determined by angle $\theta$ and angular velocity $\omega$. Phase space is the $(\theta, \omega)$ plane. As the pendulum swings, the point $(\theta(t), \omega(t))$ traces a path through this space!</p>

                                <div class="example-box">
                                    <p><strong>Example - Predator-Prey (Lotka-Volterra):</strong> Population dynamics of predators and prey:<br><br>
                                    $\frac{dx}{dt} = \alpha x - \beta xy$ (prey population)<br>
                                    $\frac{dy}{dt} = -\gamma y + \delta xy$ (predator population)<br><br>
                                    where $x$ is prey, $y$ is predator, and the constants are growth/interaction rates. Phase space is the $(x, y)$ plane showing how populations evolve!</ p>
                                </div>

                                <h3>Equilibrium Points and Stability</h3>

                                <p>An <strong>equilibrium point</strong> (or <strong>fixed point</strong>) is where $\frac{d\vec{u}}{dt} = \vec{0}$. The system doesn't change if it starts there.</p>

                                <p>Equilibria can be:</p>
                                <ul>
                                    <li><strong>Stable (attractor):</strong> Nearby trajectories converge to it. Like a marble at the bottom of a bowl.</li>
                                    <li><strong>Unstable (repeller):</strong> Nearby trajectories diverge away. Like a marble balanced on top of a hill.</li>
                                    <li><strong>Saddle:</strong> Stable in some directions, unstable in others. Like a mountain pass.</li>
                                </ul>

                                <h3>Linearization and Stability Analysis</h3>

                                <p>To analyze stability near equilibrium $\vec{u}_0$, we <strong>linearize</strong> the system. The <strong>Jacobian matrix</strong> of partial derivatives:</p>

                                <div class="math-block">
                                    $$J = \begin{pmatrix} \frac{\partial f}{\partial x} & \frac{\partial f}{\partial y} \\ \frac{\partial g}{\partial x} & \frac{\partial g}{\partial y} \end{pmatrix}$$
                                </div>

                                <p>evaluated at $\vec{u}_0$ determines stability. The eigenvalues of $J$ tell the story:</p>
                                <ul>
                                    <li>All eigenvalues have negative real parts ‚Üí <strong>stable</strong></li>
                                    <li>At least one eigenvalue has positive real part ‚Üí <strong>unstable</strong></li>
                                    <li>Pure imaginary eigenvalues ‚Üí <strong>center</strong> (neutral stability, orbits)</li>
                                </ul>

                                <div class="info-box">
                                    <p><strong>In Gravitation¬≥:</strong> The Lorenz attractor, R√∂ssler attractor, and all chaotic attractors are systems of ODEs visualized in phase space! The strange attractor shapes you see ARE phase space trajectories. Understanding stability helps us see why trajectories are attracted to certain regions.</p>
                                </div>
                            </div>

                            <div class="docs-section">
                                <h2>The Lorenz System Example</h2>

                                <p>The famous <strong>Lorenz equations</strong> that govern our Lorenz attractor simulation:</p>

                                <div class="math-block">
                                    $$\frac{dx}{dt} = \sigma(y - x)$$
                                    $$\frac{dy}{dt} = x(\rho - z) - y$$
                                    $$\frac{dz}{dt} = xy - \beta z$$
                                </div>

                                <p>This system has three variables $(x, y, z)$, so phase space is 3D! The attractor you see in the simulation is a trajectory in this 3D phase space.</p>

                                <p><strong>Equilibrium points:</strong> Setting all derivatives to zero:</p>
                                <ul>
                                    <li>Origin: $(0, 0, 0)$</li>
                                    <li>Two symmetric points: $(\pm\sqrt{\beta(\rho-1)}, \pm\sqrt{\beta(\rho-1)}, \rho-1)$ when $\rho > 1$</li>
                                </ul>

                                <p>For classic parameters ($\sigma = 10$, $\rho = 28$, $\beta = 8/3$), all equilibria are unstable - trajectories spiral away, creating the famous butterfly-shaped attractor!</p>

                                <div class="info-box">
                                    <p><strong>Chaos:</strong> The Lorenz system exhibits sensitive dependence on initial conditions - the butterfly effect! Two nearly identical starting points lead to completely different trajectories after enough time. This is deterministic chaos: the equations are deterministic (no randomness), but the behavior appears random due to extreme sensitivity.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 4.7 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">4.7</span>
                            <span>Partial Differential Equations (PDEs)</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Introduction to PDEs</h2>
                                
                                <p><strong>Partial Differential Equations (PDEs)</strong> involve functions of multiple variables and their partial derivatives. While ODEs describe change along a single dimension (usually time), PDEs describe change across space AND time.</p>

                                <p>The general form of a linear second-order PDE in two variables:</p>

                                <div class="math-block">
                                    $$A\frac{\partial^2 u}{\partial x^2} + B\frac{\partial^2 u}{\partial x \partial y} + C\frac{\partial^2 u}{\partial y^2} + D\frac{\partial u}{\partial x} + E\frac{\partial u}{\partial y} + Fu = G$$
                                </div>

                                <h3>Classification of PDEs</h3>

                                <p>Based on the discriminant $B^2 - 4AC$, we classify PDEs as:</p>
                                <ul>
                                    <li><strong>Elliptic</strong> ($B^2 - 4AC < 0$): Describes equilibrium states - Laplace's equation, Poisson's equation</li>
                                    <li><strong>Parabolic</strong> ($B^2 - 4AC = 0$): Describes diffusion processes - heat equation</li>
                                    <li><strong>Hyperbolic</strong> ($B^2 - 4AC > 0$): Describes wave propagation - wave equation</li>
                                </ul>

                                <h3>The Heat Equation (Parabolic)</h3>

                                <p>The <strong>heat equation</strong> describes how temperature diffuses through a material:</p>

                                <div class="math-block">
                                    $$\frac{\partial u}{\partial t} = \alpha \frac{\partial^2 u}{\partial x^2}$$
                                </div>

                                <p>where $u(x, t)$ is temperature, $\alpha$ is thermal diffusivity. This says: temperature changes in time proportional to the curvature in space.</p>

                                <p><strong>Physical meaning:</strong> Heat flows from hot to cold. The second derivative $\frac{\partial^2 u}{\partial x^2}$ measures curvature - if temperature has a "peak" (positive curvature below, negative above), heat flows away from the peak, smoothing it out.</p>

                                <div class="example-box">
                                    <p><strong>1D Heat Equation Solution (Separation of Variables):</strong><br>
                                    For a rod of length $L$ with ends held at zero temperature, try $u(x,t) = X(x)T(t)$:<br><br>
                                    Substituting: $X(x)T'(t) = \alpha X''(x)T(t)$<br>
                                    Dividing: $\frac{T'(t)}{\alpha T(t)} = \frac{X''(x)}{X(x)} = -\lambda$ (separation constant)<br><br>
                                    This gives two ODEs:<br>
                                    $T'(t) = -\alpha\lambda T(t) \Rightarrow T(t) = e^{-\alpha\lambda t}$<br>
                                    $X''(x) = -\lambda X(x) \Rightarrow X(x) = A\sin(\sqrt{\lambda}x) + B\cos(\sqrt{\lambda}x)$<br><br>
                                    Boundary conditions $u(0,t) = u(L,t) = 0$ lead to solutions:<br>
                                    $u_n(x,t) = \sin\left(\frac{n\pi x}{L}\right)e^{-\alpha(n\pi/L)^2 t}$ for $n = 1, 2, 3, \ldots$<br><br>
                                    General solution is a superposition: $u(x,t) = \sum_{n=1}^\infty B_n\sin\left(\frac{n\pi x}{L}\right)e^{-\alpha(n\pi/L)^2 t}$</p>
                                </div>

                                <h3>The Wave Equation (Hyperbolic)</h3>

                                <p>The <strong>wave equation</strong> describes vibrations and wave propagation:</p>

                                <div class="math-block">
                                    $$\frac{\partial^2 u}{\partial t^2} = c^2 \frac{\partial^2 u}{\partial x^2}$$
                                </div>

                                <p>where $c$ is wave speed. This describes vibrating strings, sound waves, light waves, and any propagating disturbance.</p>

                                <p><strong>General solution (d'Alembert):</strong> $u(x,t) = f(x - ct) + g(x + ct)$</p>

                                <p>This represents two waves: $f(x - ct)$ traveling right, $g(x + ct)$ traveling left, both at speed $c$!</p>

                                <div class="example-box">
                                    <p><strong>Physical Example:</strong> A guitar string plucked at the center creates two waves traveling in opposite directions. When they reach the ends (fixed boundaries), they reflect and interfere, creating standing waves - the musical notes you hear!</p>
                                </div>

                                <h3>Laplace's Equation (Elliptic)</h3>

                                <p><strong>Laplace's equation</strong> describes steady-state (time-independent) distributions:</p>

                                <div class="math-block">
                                    $$\nabla^2 u = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} + \frac{\partial^2 u}{\partial z^2} = 0$$
                                </div>

                                <p>This equation has no time derivative - it describes equilibrium. Applications include:</p>
                                <ul>
                                    <li>Steady-state temperature distribution</li>
                                    <li>Electrostatic potential (no charges present)</li>
                                    <li>Gravitational potential  (no masses present)</li>
                                    <li>Fluid flow potential (irrotational, incompressible flow)</li>
                                </ul>

                                <p><strong>Poisson's Equation:</strong> Generalization with sources/sinks: $\nabla^2 u = f$</p>

                                <div class="info-box">
                                    <p><strong>In Our Simulations:</strong> The Navier-Stokes equations for incompressible flow involve a Poisson equation for pressure! Solving this efficiently is critical for realistic fluid simulations. The pressure field must satisfy $\nabla^2 p = -\rho\nabla \cdot (\vec{v} \cdot \nabla\vec{v})$ to maintain incompressibility.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 4.8 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">4.8</span>
                            <span>Numerical Methods for ODEs</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Why Numerical Methods?</h2>
                                
                                <p>Most differential equations can't be solved analytically - we can't write down an exact formula for the solution! The Lorenz equations, three-body problem, Navier-Stokes equations - none have closed-form solutions. This is where <strong>numerical methods</strong> come in: algorithms that approximate solutions to arbitrary precision.</p>

                                <h3>Euler's Method (First-Order)</h3>

                                <p>The simplest numerical method for solving $\frac{dy}{dx} = f(x, y)$ with initial condition $y(x_0) = y_0$:</p>

                                <div class="math-block">
                                    $$y_{n+1} = y_n + h \cdot f(x_n, y_n)$$
                                </div>

                                <p>where $h$ is the step size, $x_{n+1} = x_n + h$. This approximates the derivative by a finite difference!</p>

                                <p><strong>How it works:</strong> The tangent line at $(x_n, y_n)$ has slope $f(x_n, y_n)$. We follow this tangent line for distance $h$ to estimate the next point.</p>

                                <div class="example-box">
                                    <p><strong>Example:</strong> Solve $\frac{dy}{dx} = y$ with $y(0) = 1$ using Euler's method with $h = 0.1$:<br><br>
                                    $y_1 = y_0 + 0.1 \cdot y_0 = 1 + 0.1(1) = 1.1$<br>
                                    $y_2 = y_1 + 0.1 \cdot y_1 = 1.1 + 0.1(1.1) = 1.21$<br>
                                    $y_3 = 1.21 + 0.1(1.21) = 1.331$<br><br>
                                    Exact solution is $y = e^x$, so at $x = 0.3$: $y \approx 1.3499$. Our approximation $y_3 = 1.331$ has error!</p>
                                </div>

                                <h3>Runge-Kutta Methods (Higher Order)</h3>

                                <p>Euler's method has large errors. <strong>Runge-Kutta methods</strong> achieve better accuracy by evaluating the slope at multiple points within each step.</p>

                                <p><strong>RK4 (Fourth-Order Runge-Kutta):</strong> The most popular method, achieves fourth-order accuracy:</p>

                                <div class="math-block">
                                    $$k_1 = hf(x_n, y_n)$$
                                    $$k_2 = hf(x_n + h/2, y_n + k_1/2)$$
                                    $$k_3 = hf(x_n + h/2, y_n + k_2/2)$$
                                    $$k_4 = hf(x_n + h, y_n + k_3)$$
                                    $$y_{n+1} = y_n + \frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$
                                </div>

                                <p>RK4 evaluates the slope at the beginning, middle (twice), and end of the interval, then takes a weighted average!</p>

                                <div class="info-box">
                                    <p><strong>In Gravitation¬≥:</strong> ALL our simulations use Runge-Kutta methods! The Lorenz attractor, three-body problem, every fluid simulation - they all evolve forward in time using RK4 or similar methods. Without numerical integration, we couldn't visualize these complex systems!</p>
                                </div>

                                <h3>Adaptive Step Size</h3>

                                <p>Fixed step sizes waste computation in smooth regions and lose accuracy in rapidly changing regions. <strong>Adaptive methods</strong> adjust $h$ automatically:</p>

                                <ul>
                                    <li>Where solution changes rapidly: use smaller $h$ for accuracy</li>
                                    <li>Where solution is smooth: use larger $h$ for speed</li>
                                </ul>

                                <p>Methods like <strong>RK45</strong> (Runge-Kutta-Fehlberg) compare 4th and 5th order estimates to control error and adapt step size.</p>

                                <h3>Stability and Stiffness</h3>

                                <p>Some equations are <strong>stiff</strong> - they have solutions with very different time scales. Fast-decaying transients require tiny steps, making explicit methods inefficient.</p>

                                <p><strong>Implicit methods</strong> like backward Euler can handle stiff equations better, though they require solving equations at each step.</p>

                                <div class="info-box">
                                    <p><strong>Trade-offs:</strong> Explicit methods (like RK4) are simple and fast for non-stiff equations. Implicit methods are more stable for stiff equations but computationally expensive. Choosing the right method for your problem is crucial!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 4.9 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">4.9</span>
                            <span>Nonlinear Dynamics and Chaos</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Nonlinear Differential Equations</h2>
                                
                                <p>Most real-world systems are <strong>nonlinear</strong> - the differential equations governing them involve products, powers, or other nonlinear combinations of variables and derivatives. Unlike linear equations, nonlinear equations can exhibit incredibly rich behavior!</p>

                                <h3>Characteristics of Nonlinear Systems</h3>

                                <ul>
                                    <li><strong>No superposition:</strong> If $y_1$ and $y_2$ are solutions, $y_1 + y_2$ is NOT necessarily a solution</li>
                                    <li><strong>Multiple equilibria:</strong> Can have many equilibrium points with different stability</li>
                                    <li><strong>Limit cycles:</strong> Isolated periodic orbits that attract nearby trajectories</li>
                                    <li><strong>Bifurcations:</strong> Qualitative behavior changes as parameters vary</li>
                                    <li><strong>Chaos:</strong> Sensitive dependence on initial conditions</li>
                                </ul>

                                <h3>Bifurcations</h3>

                                <p>A <strong>bifurcation</strong> occurs when a small change in a parameter causes a qualitative change in the system's behavior. Common types:</p>

                                <p><strong>Saddle-node bifurcation:</strong> Two equilibria collide and annihilate</p>
                                <p><strong>Hopf bifurcation:</strong> An equilibrium loses stability and a limit cycle is born</p>
                                <p><strong>Period-doubling bifurcation:</strong> A periodic orbit's period doubles - route to chaos!</p>

                                <div class="example-box">
                                    <p><strong>Example - Logistic Map:</strong> The discrete map $x_{n+1} = rx_n(1 - x_n)$<br>
                                    ‚Ä¢ $r < 1$: Fixed point at $x = 0$ (extinction)<br>
                                    ‚Ä¢ $1 < r < 3$: Fixed point at $x = \frac{r-1}{r}$ (stable population)<br>
                                    ‚Ä¢ $r > 3$: Period-doubling cascade begins<br>
                                    ‚Ä¢ $r \approx 3.57$: Chaos emerges!</p>
                                </div>

                                <h3>Chaos and Strange Attractors</h3>

                                <p><strong>Deterministic chaos</strong> means:</p>
                                <ol>
                                    <li><strong>Deterministic:</strong> Equations have no randomness - same initial conditions give same result</li>
                                    <li><strong>Sensitive dependence:</strong> Tiny changes in initial conditions lead to exponentially growing differences</li>
                                    <li><strong>Bounded:</strong> Solutions stay in a finite region of phase space</li>
                                    <li><strong>Aperiodic:</strong> Never exactly repeats</li>
                                </ol>

                                <p><strong>Strange attractors</strong> are fractal structures in phase space that chaotic systems are attracted to. The Lorenz attractor, R√∂ssler attractor - these are st range attractors!</p>

                                <div class="info-box">
                                    <p><strong>The Butterfly Effect:</strong> In the Lorenz system, changing the initial $x$ value by $10^{-10}$ leads to completely different trajectories after about 20 time units. This extreme sensitivity makes long-term prediction impossible, even though the equations are perfectly deterministic! This is why weather forecasting beyond ~10 days is fundamentally limited.</p>
                                </div>

                                <h3>Lyapunov Exponents</h3>

                                <p><strong>Lyapunov exponents</strong> quantify how fast nearby trajectories diverge. For a system with state $\vec{x}(t)$:</p>

                                <div class="math-block">
                                    $$\lambda = \lim_{t \to \infty} \frac{1}{t}\ln\left(\frac{|\delta\vec{x}(t)|}{|\delta\vec{x}(0)|}\right)$$
                                </div>

                                <p>where $\delta\vec{x}$ is a small perturbation. Classification:</p>
                                <ul>
                                    <li>$\lambda < 0$: Trajectories converge (stable, regular behavior)</li>
                                    <li>$\lambda = 0$: Neutral (periodic orbits)</li>
                                    <li>$\lambda > 0$: Trajectories diverge (chaos!)</li>
                                </ul>

                                <p>For the Lorenz attractor with standard parameters, the largest Lyapunov exponent is $\lambda \approx 0.9$ - confirming chaos!</p>

                                <div class="info-box">
                                    <p><strong>In Our Simulations:</strong> Every chaotic attractor you see has at least one positive Ly apunov exponent. This is the mathematical signature of chaos - the butterfly effect quantified!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 4.10 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">4.10</span>
                            <span>Fourier Series and Transform</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Fourier Series for Periodic Functions</h2>
                                
                                <p>Any <strong>periodic function</strong> $f(x)$ with period $2L$ can be represented as an infinite sum of sines and cosines:</p>

                                <div class="math-block">
                                    $$f(x) = \frac{a_0}{2} + \sum_{n=1}^\infty \left[a_n\cos\left(\frac{n\pi x}{L}\right) + b_n\sin\left(\frac{n\pi x}{L}\right)\right]$$
                                </div>

                                <p><strong>Fourier coefficients:</strong></p>

                                <div class="math-block">
                                    $$a_n = \frac{1}{L}\int_{-L}^L f(x)\cos\left(\frac{n\pi x}{L}\right)dx$$
                                    $$b_n = \frac{1}{L}\int_{-L}^L f(x)\sin\left(\frac{n\pi x}{L}\right)dx$$
                                </div>

                                <p>This amazing result says: any periodic function is a "mixture" of pure frequencies!</p>

                                <div class="example-box">
                                    <p><strong>Square Wave Example:</strong> The square wave (up 1, down -1) has Fourier series:<br>
                                    $$f(x) = \frac{4}{\pi}\left(\sin x + \frac{1}{3}\sin 3x + \frac{1}{5}\sin 5x + \cdots\right) = \frac{4}{\pi}\sum_{n=0}^\infty \frac{\sin[(2n+1)x]}{2n+1}$$<br>
                                    Only odd harmonics appear! Each term adds sharper corners.</p>
                                </div>

                                <h3>Fourier Transform</h3>

                                <p>For non-periodic functions, the <strong>Fourier transform</strong> decomposes signals into continuous frequency components:</p>

                                <div class="math-block">
                                    $$\hat{f}(\omega) = \int_{-\infty}^\infty f(x)e^{-i\omega x}dx$$
                                </div>

                                <p>Inverse transform:</p>

                                <div class="math-block">
                                    $$f(x) = \frac{1}{2\pi}\int_{-\infty}^\infty \hat{f}(\omega)e^{i\omega x}d\omega$$
                                </div>

                                <h3>Applications in Differential Equations</h3>

                                <p><strong>Solving PDEs:</strong> Fourier methods convert PDEs into ODEs! For the heat equation, transforming in space gives an ODE in time for each frequency component.</p>

                                <p><strong>Signal Processing:</strong> Understanding frequency content helps analyze oscillations, vibrations, and waves</p>

                                <p><strong>Spectral Methods:</strong> Numerical PDE solvers using Fourier basis functions achieve high accuracy</p>

                                <div class="info-box">
                                    <p><strong>In Physics:</strong> Fourier analysis is everywhere! Sound is decomposed into frequency components (that's how musical notes work). Light waves have frequency components (colors). Quantum mechanics uses Fourier transforms to connect position and momentum. Understanding Fourier analysis is key to understanding waves and oscillations!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 4.11 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">4.11</span>
                            <span>Navier-Stokes Equations</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>The Navier-Stokes Equations</h2>
                                
                                <p>The <strong>Navier-Stokes equations</strong> govern fluid motion - they're the PDEs describing how velocity and pressure in a fluid evolve. These are among the most important equations in physics and engineering!</p>

                                <p><strong>Incompressible flow</strong> (liquids like water):</p>

                                <div class="math-block">
                                    $$\frac{\partial \vec{v}}{\partial t} + (\vec{v} \cdot \nabla)\vec{v} = -\frac{1}{\rho}\nabla p + \nu\nabla^2\vec{v} + \vec{f}$$
                                    $$\nabla \cdot \vec{v} = 0$$
                                </div>

                                <p>Let's decode each term:</p>

                                <ul>
                                    <li><strong>$\frac{\partial \vec{v}}{\partial t}$:</strong> Local acceleration - how velocity changes in time at a fixed point</li>
                                    <li><strong>$(\vec{v} \cdot \nabla)\vec{v}$:</strong> Convective acceleration - fluid carrying itself along creates nonlinear term!</li>
                                    <li><strong>$-\frac{1}{\rho}\nabla p$:</strong> Pressure gradient force -  fluid flows from high to low pressure</li>
                                    <li><strong>$\nu\nabla^2\vec{v}$:</strong> Viscous diffusion - friction smooths out velocity variations</li>
                                    <li><strong>$\vec{f}$:</strong> External forces (gravity, electromagnetic, etc.)</li>
                                    <li><strong>$\nabla \cdot \vec{v} = 0$:</strong> Incompressibility constraint - fluid can't be compressed</li>
                                </ul>

                                <h3>Reynolds Number</h3>

                                <p>The <strong>Reynolds number</strong> determines flow regime:</p>

                                <div class="math-block">
                                    $$Re = \frac{\rho VL}{\mu} = \frac{VL}{\nu}$$
                                </div>

                                <p>where $V$ is characteristic velocity, $L$ is characteristic length, $\nu$ is kinematic viscosity.</p>

                                <ul>
                                    <li><strong>Low Re ($ <2000$):</strong> Laminar flow - smooth, orderly streamlines</li>
                                    <li><strong>High Re (>4000):</strong> Turbulent flow - chaotic, swirling eddies</li>
                                    <li><strong>Intermediate:</strong> Transition region</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Physical Examples:</strong><br>
                                    ‚Ä¢ Honey flowing (very low Re) - highly viscous, laminar<br>
                                    ‚Ä¢ River rapids (high Re) - turbulent, chaotic<br>
                                    ‚Ä¢ Blood flow in capillaries (low Re) - laminar<br>
                                    ‚Ä¢ Air around airplane wing (high Re) - turbulent boundary layer</p>
                                </div>

                                <h3>Vorticity and Circulation</h3>

                                <p><strong>Vorticity</strong> $\vec{\omega} = \nabla \times \vec{v}$ measures local rotation. Taking the curl of Navier-Stokes gives the <strong>vorticity equation</strong>:</p>

                                <div class="math-block">
                                    $$\frac{\partial \vec{\omega}}{\partial t} + (\vec{v} \cdot \nabla)\vec{\omega} = (\vec{\omega} \cdot \nabla)\vec{v} + \nu\nabla^2\vec{\omega}$$
                                </div>

                                <p>This describes how vorticity is advected, stretched, and diffused!</p>

                                <div class="info-box">
                                    <p><strong>In Gravitation¬≥ Fluid Simulations:</strong> von K√°rm√°n vortex street, Kelvin-Helmholtz instability, Rayleigh-B√©nard convection - all show vorticity dynamics! The swirling patterns you see are regions of high vorticity. The Navier-Stokes equations govern every fluid simulation, making them come alive with realistic flow patterns.</p>
                                </div>

                                <h3>The Millennium Prize Problem</h3>

                                <p>Despite their importance, we don't fully understand the Navier-Stokes equations! The <strong>Clay Millennium Prize</strong> offers $1 million for proving:</p>

                                <ol>
                                    <li>Do smooth solutions exist for all time for any smooth initial conditions?</li>
                                    <li>Or can finite-time singularities develop (solution "blows up")?</li>
                                </ol>

                                <p>This is one of the most important unsolved problems in mathematics! We can simulate the equations numerically, but rigorous mathematical theory is incomplete.</p>

                                <div class="info-box">
                                    <p><strong>Why It Matters:</strong> Understanding whether solutions exist globally affects how we simulate fluids, design aircraft, predict weather, model ocean currents, and much more. The Navier-Stokes equations are central to understanding nature, yet their mathematics remains mysterious!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

            </div>

            <!-- Section 5: Advanced Concepts -->
            <div class="tutorial-content" id="advanced">
                
                <!-- Subsection 5.1 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">5.1</span>
                            <span>Hamiltonian Mechanics & Phase Space</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Hamilton's Equations</h2>
                                
                                <p><strong>Hamiltonian mechanics</strong> is a reformulation of classical mechanics that's especially powerful for analyzing complex systems. While Newton's laws use forces and accelerations, Hamiltonian mechanics uses energy and momenta.</p>

                                <p>The <strong>Hamiltonian</strong> $H(q, p, t)$ is typically the total energy of the system, expressed in terms of generalized coordinates $q$ and generalized momenta $p = \frac{\partial L}{\partial \dot{q}}$ (where $L$ is the Lagrangian).</p>

                                <h3>Hamilton's Canonical Equations</h3>

                                <p>For a system with $n$ degrees of freedom, Hamilton's equations are:</p>

                                <div class="math-block">
                                    $$\frac{dq_i}{dt} = \frac{\partial H}{\partial p_i}$$
                                    $$\frac{dp_i}{dt} = -\frac{\partial H}{\partial q_i}$$
                                </div>

                                <p>for $i = 1, 2, \ldots, n$. These are $2n$ first-order differential equations, in contrast to Newton's $n$ second-order equations!</p>

                                <div class="example-box">
                                    <p><strong>Example - Simple Harmonic Oscillator:</strong> For a mass-spring system with mass $m$ and spring constant $k$:<br><br>
                                    Hamiltonian: $H(q, p) = \frac{p^2}{2m} + \frac{1}{2}kq^2$<br><br>
                                    Hamilton's equations:<br>
                                    $\frac{dq}{dt} = \frac{\partial H}{\partial p} = \frac{p}{m}$ (momentum ‚Üí velocity)<br>
                                    $\frac{dp}{dt} = -\frac{\partial H}{\partial q} = -kq$ (spring force)<br><br>
                                    These reproduce $m\ddot{q} = -kq$, Newton's second law!</p>
                                </div>

                                <h3>Hamiltonian vs. Lagrangian Formulation</h3>

                                <p><strong>Lagrangian mechanics</strong> uses the Lagrangian $L = T - V$ (kinetic minus potential energy) and derives equations from the principle of least action. <strong>Hamiltonian mechanics</strong> uses $H = T + V$ (total energy) and treats positions and momenta on equal footing.</p>

                                <p><strong>Key differences:</strong></p>
                                <ul>
                                    <li><strong>Variables:</strong> Lagrangian uses $(q, \dot{q})$; Hamiltonian uses $(q, p)$</li>
                                    <li><strong>Equations:</strong> Lagrangian gives $n$ second-order ODEs; Hamiltonian gives $2n$ first-order ODEs</li>
                                    <li><strong>Energy:</strong> In Hamiltonian formulation, energy conservation is explicit when $\frac{\partial H}{\partial t} = 0$</li>
                                    <li><strong>Phase space:</strong> Hamiltonian naturally lives in phase space - perfect for studying dynamics!</li>
                                </ul>

                                <p>The transformation from Lagrangian to Hamiltonian involves a <strong>Legendre transformation</strong>:</p>

                                <div class="math-block">
                                    $$H(q, p, t) = p\dot{q} - L(q, \dot{q}, t)$$
                                </div>

                                <p>where $p = \frac{\partial L}{\partial \dot{q}}$ and we solve for $\dot{q}$ in terms of $p$.</p>

                                <h3>Symplectic Geometry Basics</h3>

                                <p>Hamilton's equations preserve a special geometric structure called <strong>symplectic structure</strong>. The phase space $(q, p)$ has a natural 2-form that's preserved by Hamiltonian flow.</p>

                                <p><strong>Liouville's theorem:</strong> Hamiltonian flow preserves phase space volume. If you take a "blob" of initial conditions in phase space and evolve them, the blob may stretch and deform, but its volume never changes!</p>

                                <div class="math-block">
                                    $$\frac{d}{dt}\int\int dq \, dp = 0$$
                                </div>

                                <p>This is profound: it means information is never lost in Hamiltonian systems - they're reversible!</p>

                                <p><strong>Canonical transformations</strong> are coordinate changes that preserve the Hamiltonian structure. They're like "good" coordinate changes that keep the equations in canonical form.</p>

                                <div class="info-box">
                                    <p><strong>Why Symplectic Geometry Matters:</strong> Understanding the symplectic structure helps explain why certain numerical integration methods (symplectic integrators) are better for long-time simulations. They preserve the geometric structure and don't artificially add or remove energy over time.</p>
                                </div>

                                <h3>Poincar√© Sections and Return Maps</h3>

                                <p>A <strong>Poincar√© section</strong> (or Poincar√© map) is a powerful tool for visualizing high-dimensional dynamics. We choose a lower-dimensional surface in phase space and record where trajectories intersect it.</p>

                                <p>For a 3D system $(x, y, z)$, we might choose the plane $z = 0$ and plot $(x, y)$ values whenever the trajectory crosses. This reduces continuous trajectories to discrete points!</p>

                                <p><strong>What the patterns reveal:</strong></p>
                                <ul>
                                    <li><strong>Fixed point:</strong> Periodic orbit (crosses at same point each period)</li>
                                    <li><strong>Closed curve:</strong> Quasi-periodic orbit (winding around a torus)</li>
                                    <li><strong>Scattered points:</strong> Chaotic motion (sensitive dependence)</li>
                                    <li><strong>Island chains:</strong> Regions of stability within chaos</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Application to Three-Body Problem:</strong> In the restricted three-body problem (e.g., Sun-Jupiter-asteroid), a Poincar√© section reveals:<br>
                                    ‚Ä¢ Stable resonant orbits (fixed points or small islands)<br>
                                    ‚Ä¢ Chaotic zones (scattered points)<br>
                                    ‚Ä¢ Transition regions between order and chaos<br><br>
                                    This helps us understand asteroid belt structure and gaps!</p>
                                </div>

                                <p>The <strong>return map</strong> or <strong>first return map</strong> takes a point on the Poincar√© section to the next intersection point. It's a discrete dynamical system: $\vec{x}_{n+1} = F(\vec{x}_n)$</p>

                                <p>Analyzing the return map is often easier than the full continuous system!</p>

                                <div class="info-box">
                                    <p><strong>Connection to Three-Body Problem:</strong> The Gravitation¬≥ three-body simulation can be analyzed using Hamiltonian mechanics! The Hamiltonian is:</p>
                                    <div class="math-block">
                                        $$H = \sum_{i=1}^3 \frac{|\vec{p}_i|^2}{2m_i} - \sum_{i \lt j} \frac{Gm_im_j}{|\vec{q}_i - \vec{q}_j|}$$
                                    </div>
                                    <p>This is the kinetic energy plus gravitational potential energy. Energy conservation ($H$ = constant) constrains the motion. Poincar√© sections reveal stable and chaotic orbital regions!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 5.2 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">5.2</span>
                            <span>Bifurcation Theory</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>What is a Bifurcation?</h2>
                                
                                <p>A <strong>bifurcation</strong> is a qualitative change in a system's behavior as a parameter is varied. Think of it as a "tipping point" where the system's dynamics fundamentally transform. The word comes from the Latin "bifurcus" meaning "two-forked" - like a road splitting into two paths.</p>

                                <p>Bifurcations explain how complex behaviors emerge from simple equations as parameters change. They're the mathematical explanation for sudden transitions: why does a dripping faucet suddenly become chaotic? Why does laminar flow become turbulent? Bifurcation theory provides the answers!</p>

                                <h3>Types of Bifurcations</h3>

                                <p>There are several fundamental types of bifurcations, each with distinct characteristics:</p>

                                <h4>1. Saddle-Node (Fold) Bifurcation</h4>

                                <p>Two equilibrium points - one stable, one unstable - collide and annihilate each other as a parameter crosses a critical value.</p>

                                <p><strong>Normal form:</strong> $\frac{dx}{dt} = r + x^2$</p>

                                <ul>
                                    <li>$r < 0$: Two equilibria at $x = \pm\sqrt{-r}$ (one stable, one unstable)</li>
                                    <li>$r = 0$: Single equilibrium at $x = 0$ (semi-stable)</li>
                                    <li>$r > 0$: No equilibria! Solutions diverge to infinity</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Physical Example:</strong> Consider a ball rolling in a potential well whose shape changes. As you flatten the well, the stable equilibrium at the bottom and unstable equilibrium at the rim approach each other, meet, and disappear. Suddenly there's nowhere stable for the ball to rest!</p>
                                </div>

                                <h4>2. Transcritical Bifurcation</h4>

                                <p>Two equilibria exchange stability as they pass through each other. Both exist on both sides of the bifurcation, but stability switches.</p>

                                <p><strong>Normal form:</strong> $\frac{dx}{dt} = rx - x^2$</p>

                                <ul>
                                    <li>$r < 0$: $x = 0$ stable, $x = r$ unstable</li>
                                    <li>$r = 0$: Bifurcation occurs</li>
                                    <li>$r > 0$: $x = 0$ unstable, $x = r$ stable</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Physical Example:</strong> A laser threshold! Below threshold ($r < 0$), the "off" state is stable. Above threshold ($r > 0$), the "on" state is stable. At the bifurcation point, the laser turns on.</p>
                                </div>

                                <h4>3. Pitchfork Bifurcation</h4>

                                <p>A single equilibrium spawns two new equilibria (or vice versa), creating a symmetry-breaking bifurcation.</p>

                                <p><strong>Supercritical pitchfork:</strong> $\frac{dx}{dt} = rx - x^3$</p>

                                <ul>
                                    <li>$r < 0$: One stable equilibrium at $x = 0$</li>
                                    <li>$r = 0$: Bifurcation point</li>
                                    <li>$r > 0$: $x = 0$ becomes unstable, two new stable equilibria at $x = \pm\sqrt{r}$ appear</li>
                                </ul>

                                <p><strong>Subcritical pitchfork:</strong> $\frac{dx}{dt} = rx + x^3$ (unstable branches)</p>

                                <div class="example-box">
                                    <p><strong>Physical Example:</strong> A vertical rod compressed from above. Below a critical load ($r < 0$), it stays straight. Above the critical load ($r > 0$), it buckles left or right - spontaneous symmetry breaking! The direction is random, determined by tiny perturbations.</p>
                                </div>

                                <h4>4. Hopf Bifurcation</h4>

                                <p>A stable equilibrium loses stability and gives birth to a stable limit cycle (periodic orbit). The system transitions from a steady state to oscillation!</p>

                                <p><strong>Normal form</strong> (in polar coordinates):</p>
                                <div class="math-block">
                                    $$\frac{dr}{dt} = r(\mu - r^2)$$
                                    $$\frac{d\theta}{dt} = \omega$$
                                </div>

                                <ul>
                                    <li>$\mu < 0$: Stable fixed point at origin, no oscillations</li>
                                    <li>$\mu = 0$: Hopf bifurcation</li>
                                    <li>$\mu > 0$: Fixed point unstable, stable limit cycle at $r = \sqrt{\mu}$ - periodic oscillations!</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Physical Example:</strong> A violin string! Below a critical bowing pressure, the string is stationary. Above it, the string oscillates periodically, producing a musical tone. The Hopf bifurcation marks the onset of oscillation.</p>
                                </div>

                                <h3>Bifurcation Diagrams</h3>

                                <p>A <strong>bifurcation diagram</strong> plots equilibria or long-term behavior versus a control parameter. It's a "map" showing how the system's attractors change.</p>

                                <p><strong>How to read them:</strong></p>
                                <ul>
                                    <li><strong>Solid lines:</strong> Stable equilibria or orbits</li>
                                    <li><strong>Dashed lines:</strong> Unstable equilibria</li>
                                    <li><strong>Branches meet:</strong> Bifurcation point</li>
                                    <li><strong>Scattered regions:</strong> Chaos!</li>
                                </ul>

                                <div class="info-box">
                                    <p><strong>Classic Example - Logistic Map:</strong> The bifurcation diagram for $x_{n+1} = rx_n(1-x_n)$ is one of the most famous images in mathematics! As $r$ increases:<br>
                                    ‚Ä¢ $r < 3$: Single fixed point<br>
                                    ‚Ä¢ $r \approx 3$: Period-doubling begins<br>
                                    ‚Ä¢ $r \approx 3.57$: Onset of chaos<br>
                                    ‚Ä¢ Windows of periodic behavior within chaos<br>
                                    This is the <strong>period-doubling route to chaos</strong>!</p>
                                </div>

                                <h3>Period-Doubling Route to Chaos</h3>

                                <p>One universal pathway to chaos is through a cascade of period-doubling bifurcations:</p>

                                <ol>
                                    <li>System starts with period-1 orbit (fixed point)</li>
                                    <li>First bifurcation ‚Üí period-2 orbit</li>
                                    <li>Second bifurcation ‚Üí period-4 orbit</li>
                                    <li>Period-8, 16, 32, ... (exponentially faster)</li>
                                    <li>Infinite cascade accumulates at onset of chaos</li>
                                </ol>

                                <p><strong>Feigenbaum's constants:</strong> Mitchell Feigenbaum discovered universal constants describing the bifurcation cascade:</p>

                                <div class="math-block">
                                    $$\delta = \lim_{n \to \infty} \frac{r_n - r_{n-1}}{r_{n+1} - r_n} \approx 4.669...$$
                                </div>

                                <p>This constant appears in ALL unimodal maps undergoing period-doubling! It's a universal feature of the route to chaos.</p>

                                <div class="info-box">
                                    <p><strong>Connection to Lorenz/R√∂ssler Systems:</strong> The famous chaotic attractors in Gravitation¬≥ undergo bifurcations as parameters change!<br><br>
                                    <strong>Lorenz attractor:</strong> As $\rho$ (the Rayleigh number) increases:<br>
                                    ‚Ä¢ $\rho < 1$: All trajectories converge to origin (stable fixed point)<br>
                                    ‚Ä¢ $\rho = 1$: Pitchfork bifurcation! Origin becomes unstable, two new fixed points appear<br>
                                    ‚Ä¢ $\rho \approx 13.9$: Hopf bifurcation at the two fixed points<br>
                                    ‚Ä¢ $\rho \approx 24.7$: Chaotic attractor emerges!<br>
                                    ‚Ä¢ $\rho = 28$ (standard): Full chaotic behavior<br><br>
                                    <strong>R√∂ssler attractor:</strong> As parameters vary, it transitions through:<br>
                                    ‚Ä¢ Fixed point ‚Üí periodic orbit (Hopf bifurcation)<br>
                                    ‚Ä¢ Period-doubling cascade<br>
                                    ‚Ä¢ Chaos with characteristic spiral structure<br><br>
                                    Understanding these bifurcations explains why certain parameter values produce chaos while others don't!</p>
                                </div>

                                <h3>Control Parameters and Bifurcations</h3>

                                <p>The <strong>control parameter</strong> is the variable we tune to produce bifurcations. In physical systems:</p>

                                <ul>
                                    <li><strong>Fluids:</strong> Reynolds number (velocity, viscosity ratio)</li>
                                    <li><strong>Lasers:</strong> Pump power</li>
                                    <li><strong>Climate:</strong> Solar radiation, CO‚ÇÇ concentration</li>
                                    <li><strong>Chemical reactions:</strong> Reactant concentrations, temperature</li>
                                    <li><strong>Ecosystems:</strong> Birth/death rates, carrying capacity</li>
                                </ul>

                                <p>Finding bifurcation points helps predict when systems will undergo dramatic changes - critical for control and prediction!</p>

                                <div class="example-box">
                                    <p><strong>Real-World Application:</strong> Understanding bifurcations in climate models helps identify "tipping points" where small changes could trigger large-scale shifts (e.g., ice sheet collapse, ocean circulation changes). These are literally bifurcations in Earth's climate system!</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 5.3 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">5.3</span>
                            <span>Fractal Dimensions & Strange Attractors</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>What is a Fractal?</h2>
                                
                                <p>A <strong>fractal</strong> is a mathematical set that exhibits self-similarity across scales - it looks similar whether you zoom in or out. While traditional geometry deals with smooth objects (lines, circles, planes), fractal geometry describes the rough, irregular, fragmented shapes found throughout nature.</p>

                                <p>Coined by Benoit Mandelbrot in 1975, the term comes from the Latin "fractus" meaning "broken" or "fractured." Fractals challenge our intuition about dimension - they can have non-integer dimensions!</p>

                                <h3>Self-Similarity</h3>

                                <p><strong>Self-similarity</strong> means a pattern repeats at different scales. There are different types:</p>

                                <ul>
                                    <li><strong>Exact self-similarity:</strong> Perfect copies at all scales (mathematical fractals like the Koch snowflake)</li>
                                    <li><strong>Statistical self-similarity:</strong> Same statistical properties at different scales (coastlines, clouds)</li>
                                    <li><strong>Asymptotic self-similarity:</strong> Becomes self-similar in the limit (strange attractors)</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Classic Examples:</strong><br>
                                    ‚Ä¢ <strong>Koch snowflake:</strong> Each side is replaced by 4 smaller segments, repeated infinitely<br>
                                    ‚Ä¢ <strong>Sierpi≈Ñski triangle:</strong> Remove middle triangle, repeat on remaining triangles<br>
                                    ‚Ä¢ <strong>Mandelbrot set:</strong> Infinitely complex boundary with self-similar structures<br>
                                    ‚Ä¢ <strong>Nature:</strong> Coastlines, mountains, trees, blood vessels, lightning, clouds</p>
                                </div>

                                <h3>Fractal Dimensions</h3>

                                <p>Traditional dimensions are integers: a point is 0D, a line is 1D, a plane is 2D, space is 3D. But fractals can have non-integer dimensions!</p>

                                <p>The <strong>fractal dimension</strong> measures how completely a fractal fills space. A highly crinkled curve fills more space than a straight line but less than a plane - it has dimension between 1 and 2.</p>

                                <h4>Box-Counting Dimension (Capacity Dimension)</h4>

                                <p>The <strong>box-counting dimension</strong> is the most practical method for measuring fractal dimension:</p>

                                <p><strong>Algorithm:</strong></p>
                                <ol>
                                    <li>Cover the fractal with a grid of boxes of size $\epsilon$</li>
                                    <li>Count how many boxes $N(\epsilon)$ are needed to cover the fractal</li>
                                    <li>Repeat with smaller box sizes</li>
                                    <li>The dimension is:</li>
                                </ol>

                                <div class="math-block">
                                    $$D_{\text{box}} = \lim_{\epsilon \to 0} \frac{\ln N(\epsilon)}{\ln(1/\epsilon)}$$
                                </div>

                                <p>In practice, plot $\ln N(\epsilon)$ vs. $\ln(1/\epsilon)$ - the slope is the dimension!</p>

                                <div class="example-box">
                                    <p><strong>Example Calculation:</strong><br><br>
                                    For the Koch snowflake:<br>
                                    ‚Ä¢ Start with 1 segment<br>
                                    ‚Ä¢ After 1 iteration: 4 segments, each 1/3 the length<br>
                                    ‚Ä¢ After 2 iterations: 16 segments, each 1/9 the length<br>
                                    ‚Ä¢ Pattern: $N = 4^n$ at scale $\epsilon = 3^{-n}$<br><br>
                                    Dimension: $D = \frac{\ln 4}{\ln 3} \approx 1.262$<br><br>
                                    It's between a line (1D) and plane (2D), but closer to a line!</p>
                                </div>

                                <h4>Hausdorff Dimension</h4>

                                <p>The <strong>Hausdorff dimension</strong> is the most mathematically rigorous definition, based on measure theory. It generalizes the concept of dimension to arbitrary sets.</p>

                                <p>For a set $S$, cover it with sets of diameter $\leq \epsilon$. The $d$-dimensional Hausdorff measure is:</p>

                                <div class="math-block">
                                    $$H^d(S) = \lim_{\epsilon \to 0} \inf \sum_i (\text{diam}(U_i))^d$$
                                </div>

                                <p>The <strong>Hausdorff dimension</strong> is the unique value $d$ where $H^d$ transitions from infinity to zero:</p>

                                <div class="math-block">
                                    $$D_H = \inf\{d : H^d(S) = 0\} = \sup\{d : H^d(S) = \infty\}$$
                                </div>

                                <p>For "nice" fractals, box-counting and Hausdorff dimensions are equal. For strange attractors, they may differ slightly.</p>

                                <h4>Correlation Dimension</h4>

                                <p>The <strong>correlation dimension</strong> is particularly useful for analyzing data from chaotic systems and strange attractors:</p>

                                <p>Given $N$ points on an attractor, count pairs within distance $r$:</p>

                                <div class="math-block">
                                    $$C(r) = \lim_{N \to \infty} \frac{1}{N^2} \sum_{i \neq j} \Theta(r - |\vec{x}_i - \vec{x}_j|)$$
                                </div>

                                <p>where $\Theta$ is the Heaviside step function. For small $r$:</p>

                                <div class="math-block">
                                    $$C(r) \sim r^{D_2}$$
                                </div>

                                <p>The exponent $D_2$ is the <strong>correlation dimension</strong>. Plot $\ln C(r)$ vs. $\ln r$ - the slope gives $D_2$!</p>

                                <p>The correlation dimension is often easier to compute from time series data than box-counting dimension.</p>

                                <div class="info-box">
                                    <p><strong>Kaplan-Yorke Dimension:</strong> For strange attractors, the <strong>Kaplan-Yorke (Lyapunov) dimension</strong> provides another estimate based on Lyapunov exponents:<br>
                                    $$D_{KY} = j + \frac{\sum_{i=1}^j \lambda_i}{|\lambda_{j+1}|}$$<br>
                                    where $\lambda_i$ are Lyapunov exponents in decreasing order, and $j$ is the largest integer such that $\sum_{i=1}^j \lambda_i \geq 0$.</p>
                                </div>

                                <h3>Strange Attractors</h3>

                                <p>A <strong>strange attractor</strong> is an attractor (a set toward which trajectories converge) that also exhibits:</p>

                                <ul>
                                    <li><strong>Fractal structure:</strong> Non-integer dimension</li>
                                    <li><strong>Sensitive dependence:</strong> Positive Lyapunov exponent (chaos)</li>
                                    <li><strong>Self-similarity:</strong> Complex structure at all scales</li>
                                </ul>

                                <p>"Strange" refers to the geometric properties; "attractor" means trajectories are drawn toward it.</p>

                                <h4>Why Lorenz Attractor Looks Fractal</h4>

                                <p>The <strong>Lorenz attractor</strong> you see in Gravitation¬≥ simulations is a strange attractor with fascinating fractal properties:</p>

                                <p><strong>Visual Structure:</strong> The famous butterfly shape arises from the interplay of stretching and folding in phase space. Trajectories spiral around two unstable fixed points, occasionally switching between them.</p>

                                <p><strong>Fractal Cross-Section:</strong> If you take a slice through the Lorenz attractor (a Poincar√© section), you see a fractal Cantor-like set! What looks like a single curve from far away is actually an infinite set of closely spaced curves when you zoom in.</p>

                                <p><strong>Dimension:</strong> The Lorenz attractor has dimension $D \approx 2.06$ (between a surface and a volume). It's not quite 2D, but not quite 3D either!</p>

                                <p><strong>How it forms:</strong></p>
                                <ol>
                                    <li><strong>Stretching:</strong> Trajectories diverge exponentially (positive Lyapunov exponent)</li>
                                    <li><strong>Folding:</strong> To stay bounded, trajectories must fold back</li>
                                    <li><strong>Repeated folding:</strong> Creates layers upon layers, like folding dough</li>
                                </ol>

                                <p>This stretch-and-fold mechanism creates the fractal structure! Each fold doubles the number of layers, creating a Cantor-set-like cross-section. This is why zooming in reveals ever-finer structure - it's infinitely detailed.</p>

                                <p><strong>Physical Meaning:</strong> The fractal dimension reflects the complexity of the dynamics. A dimension of ~2.06 means the attractor is more complex than a simple surface but not as space-filling as a 3D volume. It represents a delicate balance between order and chaos.</p>

                                <div class="info-box">
                                    <p><strong>In Gravitation¬≥ Simulations:</strong> When you see the Lorenz or R√∂ssler attractors, you're seeing fractal objects! The apparent "thickness" of the trajectory isn't due to plotting multiple lines - it's the fractal structure itself. Nearby initial conditions diverge but remain on the attractor, creating the intricate layered patterns. Understanding fractal dimensions helps us quantify this complexity mathematically!</p>
                                </div>

                                <h3>Applications of Fractal Analysis</h3>

                                <p>Fractal dimensions aren't just mathematical curiosities - they have practical applications:</p>

                                <ul>
                                    <li><strong>Characterizing chaos:</strong> Strange attractors in real data can be identified by measuring fractal dimension</li>
                                    <li><strong>Analyzing turbulence:</strong> Turbulent flows have fractal structure in their velocity fields</li>
                                    <li><strong>Medical imaging:</strong> Blood vessel networks, lung tissue, and brain structure show fractal patterns</li>
                                    <li><strong>Material science:</strong> Fracture surfaces, porous materials have fractal geometry</li>
                                    <li><strong>Ecology:</strong> Landscape patterns, species distributions often follow fractal statistics</li>
                                    <li><strong>Finance:</strong> Price fluctuations exhibit fractal-like scaling</li>
                                </ul>

                                <p>Benoit Mandelbrot famously asked, "How long is the coast of Britain?" The answer depends on your ruler size - smaller rulers reveal more detail, increasing the measured length. This self-similarity is a hallmark of fractals!</p>

                                <div class="example-box">
                                    <p><strong>Measuring Chaotic Attractors:</strong> To find the dimension of a strange attractor from simulation data:<br><br>
                                    1. Generate long trajectory on the attractor<br>
                                    2. Embed in appropriate dimension (use time delay embedding if needed)<br>
                                    3. Compute correlation dimension using correlation sum<br>
                                    4. Plot $\ln C(r)$ vs. $\ln r$ and find scaling region<br>
                                    5. Slope in scaling region = correlation dimension<br><br>
                                    For Lorenz: $D_2 \approx 2.06$<br>
                                    For R√∂ssler: $D_2 \approx 1.99$<br>
                                    For H√©non map: $D_2 \approx 1.26$</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 5.4 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">5.4</span>
                            <span>Turbulence & Energy Cascades</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>What is Turbulence?</h2>
                                
                                <p><strong>Turbulence</strong> is perhaps the most important unsolved problem in classical physics. It's the chaotic, irregular motion of fluids characterized by swirling eddies and vortices at many different scales. While laminar flow is smooth and predictable, turbulent flow is chaotic and seemingly random - yet it follows deterministic equations!</p>

                                <p>Leonardo da Vinci sketched turbulent water flows in the 15th century, noting their complex spiral patterns. Centuries later, Richard Feynman called turbulence "the most important unsolved problem of classical physics." Despite the Navier-Stokes equations governing all fluid motion, turbulence remains poorly understood theoretically, even as we observe it everywhere in nature.</p>

                                <h3>Characteristics of Turbulent Flow</h3>

                                <ul>
                                    <li><strong>Chaotic:</strong> Extreme sensitivity to initial conditions - slight changes lead to completely different flows</li>
                                    <li><strong>Multi-scale:</strong> Structures exist at all length scales, from large eddies to tiny vortices</li>
                                    <li><strong>Dissipative:</strong> Energy continuously dissipated as heat due to viscosity</li>
                                    <li><strong>Diffusive:</strong> Rapid mixing - turbulence is nature's blender</li>
                                    <li><strong>Three-dimensional:</strong> Vortex stretching requires 3D - no turbulence in 2D fluids!</li>
                                    <li><strong>High Reynolds number:</strong> Inertial forces dominate viscous forces</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Turbulence in Nature:</strong><br>
                                    ‚Ä¢ Smoke rising from a candle (starts laminar, becomes turbulent)<br>
                                    ‚Ä¢ Ocean currents and atmospheric winds<br>
                                    ‚Ä¢ Flow around airplane wings at cruising speed<br>
                                    ‚Ä¢ Blood flow in the heart during strenuous exercise<br>
                                    ‚Ä¢ Stellar atmospheres and solar wind<br>
                                    ‚Ä¢ Interstellar gas clouds<br>
                                    ‚Ä¢ Coffee stirred with a spoon</p>
                                </div>

                                <h3>Reynolds Decomposition</h3>

                                <p><strong>Reynolds decomposition</strong> is a fundamental tool for analyzing turbulent flows. It separates the velocity field into a mean (time-averaged) component and a fluctuating (turbulent) component:</p>

                                <div class="math-block">
                                    $$\vec{v}(\vec{x}, t) = \overline{\vec{v}}(\vec{x}) + \vec{v}'(\vec{x}, t)$$
                                </div>

                                <p>where:</p>
                                <ul>
                                    <li>$\vec{v}$ is the instantaneous velocity</li>
                                    <li>$\overline{\vec{v}}$ is the time-averaged (mean) velocity: $\overline{\vec{v}} = \frac{1}{T}\int_0^T \vec{v}(t) \, dt$</li>
                                    <li>$\vec{v}'$ is the fluctuating velocity with $\overline{\vec{v}'} = 0$</li>
                                </ul>

                                <p>The same decomposition applies to pressure and other quantities:</p>
                                <div class="math-block">
                                    $$p = \overline{p} + p'$$
                                </div>

                                <p><strong>Why decompose?</strong> While the instantaneous turbulent flow is chaotic and unpredictable, the mean flow often has useful structure we can analyze and predict. Separating mean from fluctuations lets us study each aspect separately.</p>

                                <h4>Reynolds-Averaged Navier-Stokes (RANS) Equations</h4>

                                <p>Substituting Reynolds decomposition into the Navier-Stokes equations and time-averaging gives the <strong>RANS equations</strong>:</p>

                                <div class="math-block">
                                    $$\frac{\partial \overline{\vec{v}}}{\partial t} + (\overline{\vec{v}} \cdot \nabla)\overline{\vec{v}} = -\frac{1}{\rho}\nabla \overline{p} + \nu\nabla^2\overline{\vec{v}} - \nabla \cdot \overline{\vec{v}'\vec{v}'}$$
                                </div>

                                <p>The new term $\overline{\vec{v}'\vec{v}'}$ is the <strong>Reynolds stress tensor</strong> - it represents how turbulent fluctuations affect the mean flow. This is momentum transfer by turbulent eddies!</p>

                                <p><strong>The closure problem:</strong> The RANS equations introduce new unknowns (Reynolds stresses) without providing equations for them. We now have more unknowns than equations! This is the famous <strong>turbulence closure problem</strong> - we need additional models (turbulence models) to close the system.</p>

                                <div class="info-box">
                                    <p><strong>Turbulence Models:</strong> Engineering simulations use turbulence models like:<br>
                                    ‚Ä¢ <strong>k-Œµ model:</strong> Two equations for turbulent kinetic energy and dissipation rate<br>
                                    ‚Ä¢ <strong>k-œâ model:</strong> Equations for kinetic energy and specific dissipation rate<br>
                                    ‚Ä¢ <strong>Spalart-Allmaras:</strong> Single equation for eddy viscosity<br>
                                    ‚Ä¢ <strong>Large Eddy Simulation (LES):</strong> Resolve large eddies, model small ones<br>
                                    These models make assumptions about how turbulent stresses relate to mean flow, allowing practical computations.</p>
                                </div>

                                <h3>The Energy Cascade</h3>

                                <p>One of turbulence's most beautiful concepts is the <strong>energy cascade</strong> - energy flows from large scales to small scales in a sequential process:</p>

                                <ol>
                                    <li><strong>Energy input at large scales:</strong> External forcing creates large eddies (e.g., wind shear, stirring)</li>
                                    <li><strong>Eddy breakup:</strong> Large eddies are unstable and break into smaller eddies</li>
                                    <li><strong>Cascade continues:</strong> Medium eddies break into smaller eddies, and so on</li>
                                    <li><strong>Dissipation at small scales:</strong> Eventually eddies become so small that viscosity dominates, converting kinetic energy into heat</li>
                                </ol>

                                <p>Lewis Fry Richardson captured this poetically in 1922:</p>
                                <div style="padding: 1rem; margin: 1rem 0; border-left: 3px solid var(--accent-blue); font-style: italic; background: rgba(0, 212, 255, 0.05);">
                                    <p>"Big whorls have little whorls<br>
                                    That feed on their velocity,<br>
                                    And little whorls have lesser whorls<br>
                                    And so on to viscosity."</p>
                                </div>

                                <h4>The Inertial Range</h4>

                                <p>Between the large energy-containing scales and small dissipative scales lies the <strong>inertial range</strong> - a regime where:</p>
                                <ul>
                                    <li>Energy is neither created nor destroyed, only transferred</li>
                                    <li>Viscosity is negligible (high Reynolds number)</li>
                                    <li>Universal scaling laws apply</li>
                                    <li>Behavior is self-similar across scales</li>
                                </ul>

                                <p>The inertial range exists when Reynolds number is sufficiently high. For typical atmospheric flows, it spans scales from ~1 km down to ~1 mm!</p>

                                <h3>Kolmogorov's Theory and the -5/3 Law</h3>

                                <p>In 1941, Andrey Kolmogorov developed a groundbreaking statistical theory of turbulence. He made revolutionary assumptions:</p>

                                <p><strong>Kolmogorov's Hypotheses:</strong></p>
                                <ol>
                                    <li><strong>Local isotropy:</strong> At sufficiently small scales, turbulence is statistically isotropic (no preferred direction) and homogeneous</li>
                                    <li><strong>Universal equilibrium:</strong> Statistics depend only on energy dissipation rate $\epsilon$ and viscosity $\nu$</li>
                                    <li><strong>Energy cascade:</strong> Energy flux $\epsilon$ through the inertial range is constant</li>
                                </ol>

                                <p><strong>Dimensional Analysis:</strong> In the inertial range, the only relevant parameters are:</p>
                                <ul>
                                    <li>$\epsilon$ - energy dissipation rate (units: m¬≤/s¬≥)</li>
                                    <li>$\ell$ - length scale (units: m)</li>
                                </ul>

                                <p>The characteristic velocity at scale $\ell$ must satisfy:</p>
                                <div class="math-block">
                                    $$v(\ell) \sim (\epsilon \ell)^{1/3}$$
                                </div>

                                <p>This is <strong>Kolmogorov's scaling law</strong>!</p>

                                <h4>The Famous -5/3 Law</h4>

                                <p>The <strong>energy spectrum</strong> $E(k)$ describes how kinetic energy is distributed across wavenumbers $k = 2\pi/\ell$ (spatial frequencies). Kolmogorov's theory predicts:</p>

                                <div class="math-block">
                                    $$E(k) = C\epsilon^{2/3}k^{-5/3}$$
                                </div>

                                <p>where $C \approx 1.5$ is the Kolmogorov constant. This is the <strong>Kolmogorov -5/3 law</strong> - one of the most famous results in fluid dynamics!</p>

                                <p><strong>What it means:</strong></p>
                                <ul>
                                    <li>Energy decreases with wavenumber as a power law</li>
                                    <li>Small scales (high $k$) have less energy than large scales (low $k$)</li>
                                    <li>The -5/3 exponent is universal - independent of flow details!</li>
                                    <li>On log-log plot: straight line with slope -5/3</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Experimental Verification:</strong> The -5/3 law has been verified in countless experiments:<br>
                                    ‚Ä¢ Wind measurements from tall towers<br>
                                    ‚Ä¢ Ocean current measurements<br>
                                    ‚Ä¢ Wind tunnel experiments<br>
                                    ‚Ä¢ Atmospheric boundary layer studies<br>
                                    ‚Ä¢ Direct numerical simulations (DNS)<br><br>
                                    The agreement is remarkable - nature really does follow Kolmogorov's prediction across vastly different flows!</p>
                                </div>

                                <h4>Kolmogorov Scales</h4>

                                <p>Kolmogorov identified three fundamental scales characterizing turbulence:</p>

                                <p><strong>Kolmogorov length scale:</strong></p>
                                <div class="math-block">
                                    $$\eta = \left(\frac{\nu^3}{\epsilon}\right)^{1/4}$$
                                </div>
                                <p>This is the smallest scale of turbulent motion - where viscosity dominates and energy is dissipated.</p>

                                <p><strong>Kolmogorov time scale:</strong></p>
                                <div class="math-block">
                                    $$\tau_\eta = \left(\frac{\nu}{\epsilon}\right)^{1/2}$$
                                </div>
                                <p>The time scale of the smallest eddies.</p>

                                <p><strong>Kolmogorov velocity scale:</strong></p>
                                <div class="math-block">
                                    $$v_\eta = (\nu\epsilon)^{1/4}$$
                                </div>
                                <p>The velocity scale of dissipative motions.</p>

                                <p>These scales define the resolution needed for Direct Numerical Simulation (DNS) - to fully resolve turbulence, you need grid spacing $\Delta x < \eta$. For high Reynolds numbers, this becomes computationally prohibitive!</p>

                                <h3>Intermittency and Deviations</h3>

                                <p>While Kolmogorov's -5/3 law works remarkably well, turbulence is more complex than his 1941 theory assumed:</p>

                                <p><strong>Intermittency:</strong> Turbulent fluctuations aren't uniformly distributed in space. Intense vortical structures occur intermittently, surrounded by relatively quiescent fluid. This leads to:</p>

                                <ul>
                                    <li>Non-Gaussian probability distributions (fat tails)</li>
                                    <li>Deviations from Kolmogorov scaling at high orders</li>
                                    <li>Multifractal structure in dissipation field</li>
                                </ul>

                                <p>Kolmogorov himself refined his theory in 1962 (K62) to account for intermittency in the dissipation field.</p>

                                <div class="info-box">
                                    <p><strong>In Gravitation¬≥ Fluid Simulations:</strong> The turbulent patterns you see in simulations like:<br>
                                    ‚Ä¢ <strong>Von K√°rm√°n vortex street:</strong> Energy cascades from the cylinder to smaller vortices downstream<br>
                                    ‚Ä¢ <strong>Kelvin-Helmholtz instability:</strong> Large billows break down into smaller turbulent structures<br>
                                    ‚Ä¢ <strong>Rayleigh-B√©nard convection:</strong> Thermal plumes become turbulent at high Rayleigh numbers<br>
                                    ‚Ä¢ <strong>Richtmyer-Meshkov instability:</strong> Initial perturbations cascade to turbulent mixing<br><br>
                                    All exhibit energy cascades from large to small scales! The swirling patterns are the energy cascade in action. While we can't see Kolmogorov microscales in these visualizations (they're too small), the cascade process governs what we observe at larger scales.</p>
                                </div>

                                <h3>Applications and Importance</h3>

                                <p>Understanding turbulence and energy cascades is critical for:</p>

                                <ul>
                                    <li><strong>Aerodynamics:</strong> Drag reduction, turbulent boundary layers on aircraft</li>
                                    <li><strong>Weather prediction:</strong> Atmospheric turbulence affects forecasts</li>
                                    <li><strong>Climate modeling:</strong> Turbulent mixing in oceans and atmosphere</li>
                                    <li><strong>Combustion:</strong> Turbulent mixing enhances burning rates</li>
                                    <li><strong>Astrophysics:</strong> Star formation, accretion disks, cosmic structure</li>
                                    <li><strong>Engineering:</strong> Heat exchangers, chemical reactors, pipelines</li>
                                    <li><strong>Energy:</strong> Wind turbines extract energy from turbulent boundary layers</li>
                                </ul>

                                <p>Despite centuries of study, turbulence remains an active research frontier. As physicist Horace Lamb allegedly said on his deathbed: "I am an old man now, and when I die and go to heaven there are two matters on which I hope for enlightenment. One is quantum electrodynamics, and the other is the turbulent motion of fluids. And about the former I am rather optimistic."</p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 5.5 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">5.5</span>
                            <span>KAM Theory</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>KAM Theory: Order Within Chaos</h2>
                                
                                <p><strong>KAM theory</strong> (named after Kolmogorov, Arnold, and Moser) is one of the most profound results in dynamical systems. It explains why some systems remain orderly while others descend into chaos, and it reveals the intricate boundary between predictable and chaotic motion.</p>

                                <p>The theory addresses a fundamental question: When we slightly perturb an integrable system (one with enough conserved quantities that we can solve exactly), does the regular motion persist, or does chaos emerge?</p>

                                <h3>Integrable Systems</h3>

                                <p>An <strong>integrable system</strong> is a Hamiltonian system with $n$ degrees of freedom that has $n$ independent conserved quantities (constants of motion) in involution. This is a very special situation - most systems aren't integrable!</p>

                                <p><strong>Why integrable systems are special:</strong></p>
                                <ul>
                                    <li>Motion is confined to $n$-dimensional surfaces in $2n$-dimensional phase space</li>
                                    <li>These surfaces are $n$-dimensional tori (donut shapes generalized to $n$ dimensions)</li>
                                    <li>Motion on each torus is <strong>quasi-periodic</strong> - like superposing multiple periodic motions with incommensurate frequencies</li>
                                    <li>Solutions can be found by quadrature (just integrals)</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Example - Two Uncoupled Harmonic Oscillators:</strong><br>
                                    Hamiltonian: $H = \frac{p_1^2}{2m} + \frac{1}{2}k_1q_1^2 + \frac{p_2^2}{2m} + \frac{1}{2}k_2q_2^2$<br><br>
                                    This system has two conserved quantities: $E_1 = \frac{p_1^2}{2m} + \frac{1}{2}k_1q_1^2$ and $E_2 = \frac{p_2^2}{2m} + \frac{1}{2}k_2q_2^2$<br><br>
                                    Phase space is 4D: $(q_1, p_1, q_2, p_2)$. The motion is confined to 2D tori defined by constant $E_1$ and $E_2$. On each torus, the motion is quasi-periodic with frequencies $\omega_1 = \sqrt{k_1/m}$ and $\omega_2 = \sqrt{k_2/m}$.</p>
                                </div>

                                <h3>Quasi-Periodic Motion on Tori</h3>

                                <p>In integrable systems, trajectories wind around tori in phase space. The motion is characterized by <strong>winding numbers</strong> - the ratios of frequencies.</p>

                                <p><strong>Two types of tori:</strong></p>

                                <p><strong>1. Rational tori</strong> ($\omega_1/\omega_2$ = rational number): The trajectory eventually closes on itself - periodic motion. These are relatively rare (measure zero).</p>

                                <p><strong>2. Irrational tori</strong> ($\omega_1/\omega_2$ = irrational number): The trajectory never repeats exactly but densely fills the torus surface. This is <strong>quasi-periodic</strong> motion - it's ordered but not periodic.</p>

                                <div class="info-box">
                                    <p><strong>Musical Analogy:</strong> Think of a piano. If you strike two keys whose frequencies are rationally related (e.g., an octave, where $f_1/f_2 = 2/1$), you get a pleasant harmonic sound that repeats. If frequencies are irrationally related, the combined waveform never exactly repeats - quasi-periodic!</p>
                                </div>

                                <h3>Perturbations and the KAM Theorem</h3>

                                <p>Now the key question: What happens if we <em>slightly perturb</em> an integrable system? This is realistic - real systems are never perfectly integrable!</p>

                                <p>Consider a perturbed Hamiltonian:</p>
                                <div class="math-block">
                                    $$H = H_0 + \epsilon H_1$$
                                </div>

                                <p>where $H_0$ is integrable, $\epsilon$ is a small parameter, and $H_1$ is the perturbation.</p>

                                <p><strong>Naive expectation:</strong> Small perturbations might destroy <em>all</em> tori, leading to widespread chaos.</p>

                                <p><strong>KAM Theorem says:</strong> Not so fast! Under certain conditions, <em>most</em> tori survive!</p>

                                <h4>The KAM Theorem (Simplified Statement)</h4>

                                <p>For a nearly integrable Hamiltonian system with $n$ degrees of freedom:</p>

                                <ol>
                                    <li><strong>Survival of tori:</strong> If the perturbation $\epsilon$ is sufficiently small, most invariant tori persist (though slightly deformed)</li>
                                    <li><strong>Non-degeneracy condition:</strong> The unperturbed frequencies must depend non-degenerately on actions (Hessian condition)</li>
                                    <li><strong>Diophantine condition:</strong> The survived tori have "sufficiently irrational" frequencies - frequencies must be "far from" rational ratios</li>
                                </ol>

                                <p>More technically, frequencies $\vec{\omega}$ must satisfy a <strong>Diophantine condition</strong>:</p>

                                <div class="math-block">
                                    $$|\vec{k} \cdot \vec{\omega}| \geq \frac{\gamma}{|\vec{k}|^\tau}$$
                                </div>

                                <p>for all integer vectors $\vec{k} \neq \vec{0}$, where $\gamma > 0$ and $\tau \geq n-1$ are constants. This excludes frequencies too close to resonances.</p>

                                <p><strong>What doesn't survive:</strong> Rational tori (resonant tori) are destroyed by arbitrarily small perturbations. They break up into chaotic layers.</p>

                                <div class="info-box">
                                    <p><strong>Key Insight:</strong> KAM theory tells us that integrable systems are "structurally stable" to small perturbations! While resonant tori are destroyed, irrational tori with "sufficiently irrational" frequencies persist. Since irrational numbers are "more common" than rational numbers (in measure-theoretic sense), most tori survive.</p>
                                </div>

                                <h3>Resonances and Chaos Onset</h3>

                                <p>While KAM tori act as barriers preventing chaotic mixing, the gaps between them (where resonant tori were destroyed) contain chaotic zones.</p>

                                <h4>Resonance Conditions</h4>

                                <p>A <strong>resonance</strong> occurs when frequencies are commensurate:</p>

                                <div class="math-block">
                                    $$m\omega_1 + n\omega_2 + \cdots = 0$$
                                </div>

                                <p>for integers $m, n, \ldots$. At resonances:</p>
                                <ul>
                                    <li>Small perturbations have large cumulative effects</li>
                                    <li>Tori break up into <strong>resonance islands</strong></li>
                                    <li>Chaotic motion appears in the <strong>separatrix layers</strong> between islands</li>
                                </ul>

                                <h4>The Transition to Chaos</h4>

                                <p>As perturbation strength $\epsilon$ increases, more tori are destroyed:</p>

                                <ol>
                                    <li><strong>Small $\epsilon$:</strong> Most tori survive (KAM regime). Chaotic regions are small, isolated near low-order resonances</li>
                                    <li><strong>Moderate $\epsilon$:</strong> More tori destroyed. Chaotic regions grow and connect. Remnant KAM tori act as partial barriers.</li>
                                    <li><strong>Large $\epsilon$:</strong> Most tori destroyed. Widespread chaos. Only small islands of stability remain.</li>
                                    <li><strong>Very large $\epsilon$:</strong> Essentially fully chaotic. Global chaos with perhaps tiny stability islands.</li>
                                </ol>

                                <p>This describes the <strong>KAM transition</strong> - the gradual onset of chaos as integrability is broken!</p>

                                <div class="example-box">
                                    <p><strong>Standard Map (Chirikov Map):</strong> The prototypical example for studying KAM theory:<br>
                                    $$p_{n+1} = p_n + K\sin(\theta_n)$$
                                    $$\theta_{n+1} = \theta_n + p_{n+1}$$<br><br>
                                    For $K = 0$ (integrable): All motion is on invariant circles<br>
                                    For small $K$: Most KAM tori survive, small chaotic regions<br>
                                    For $K \approx 0.97$: Last major KAM torus destroyed<br>
                                    For $K > 1$: Widespread chaos, though stability islands persist<br><br>
                                    This simple map illustrates the entire KAM transition!</p>
                                </div>

                                <h3>Poincar√© Sections and KAM Structure</h3>

                                <p>The <strong>Poincar√© section</strong> (surface of section) beautifully reveals KAM structure. For a 2D area-preserving map or a 3D Hamiltonian system:</p>

                                <ul>
                                    <li><strong>Smooth curves:</strong> Invariant tori (KAM tori) - regular motion</li>
                                    <li><strong>Island chains:</strong> Resonances - periodic orbits with stable/unstable manifolds</li>
                                    <li><strong>Scattered points:</strong> Chaotic zones - stochastic-looking motion</li>
                                    <li><strong>Self-similar structure:</strong> Zoom into any island chain reveals smaller islands - fractal structure!</li>
                                </ul>

                                <p>The phrase "Poincar√© section" was introduced by Poincar√© himself while studying the three-body problem - he was the first to see this beautiful structure!</p>

                                <div class="info-box">
                                    <p><strong>Connection to Three-Body Problem:</strong> The Gravitation¬≥ three-body simulation is a perfect example of KAM physics!<br><br>
                                    <strong>Integrable limit:</strong> If one mass is negligible (restricted three-body problem), and orbits are unperturbed, we have approximate integrability with conserved Jacobi constant.<br><br>
                                    <strong>Perturbations:</strong> Other bodies provide perturbations. Asteroid orbits in the solar system experience perturbations from Jupiter and other planets.<br><br>
                                    <strong>KAM tori:</strong> Stable asteroid orbits lie on KAM tori. These are the stable zones in the asteroid belt.<br><br>
                                    <strong>Resonances:</strong> Kirkwood gaps in the asteroid belt occur at orbital resonances with Jupiter (e.g., 3:1, 5:2, 7:3 resonances). These resonant orbits are chaotic - asteroids there get ejected!<br><br>
                                    <strong>Chaos onset:</strong> Some asteroids have chaotic orbits due to overlap of resonances. Their long-term fate is unpredictable - they might collide with a planet, get ejected, or settle into a different orbit.<br><br>
                                    KAM theory explains the structure of the solar system - why some orbits are stable for billions of years while others are chaotic!</p>
                                </div>

                                <h3>Arnold Diffusion</h3>

                                <p>For systems with 3 or more degrees of freedom ($n \geq 3$), there's a subtle phenomenon called <strong>Arnold diffusion</strong>.</p>

                                <p>In 2D systems, KAM tori are curves that divide phase space into disconnected regions - they're absolute barriers. But in higher dimensions, KAM tori are $n$-dimensional surfaces in $2n$-dimensional phase space. They don't divide the space completely - there are "holes" between tori.</p>

                                <p><strong>Arnold diffusion</strong> is the slow migration of trajectories through these gaps. A trajectory in a chaotic layer near one destroyed resonance can slowly "diffuse" through chains of resonances to reach distant parts of phase space. This diffusion is exponentially slow but theoretically possible.</p>

                                <p>This has profound implications: even if most tori survive (KAM regime), the system isn't completely regular. Slow chaotic diffusion can transport trajectories across action space over very long times!</p>

                                <div class="example-box">
                                    <p><strong>Astrophysical Implications:</strong> Arnold diffusion might allow asteroids to slowly migrate between different regions of the solar system over billion-year timescales. While KAM tori provide stability barriers, diffusion through resonance chains could enable long-term orbital instability.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 5.6 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">5.6</span>
                            <span>Symmetries & Conservation Laws</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Noether's Theorem: The Deep Connection</h2>
                                
                                <p><strong>Noether's theorem</strong>, proven by Emmy Noether in 1915, is one of the most beautiful and profound results in physics. It reveals a deep connection between symmetries and conservation laws:</p>

                                <div class="info-box">
                                    <p><strong>Noether's Theorem:</strong> Every continuous symmetry of a physical system corresponds to a conservation law, and vice versa.</p>
                                </div>

                                <p>This isn't just a mathematical curiosity - it's the fundamental reason why energy, momentum, and angular momentum are conserved! Conservation laws aren't arbitrary rules of nature; they're inevitable consequences of spacetime symmetries.</p>

                                <h3>What is a Symmetry?</h3>

                                <p>A <strong>symmetry</strong> of a system is a transformation that leaves the physics unchanged. More precisely, a transformation is a symmetry if the action (or Lagrangian) is invariant under that transformation.</p>

                                <p><strong>Types of symmetries:</strong></p>
                                <ul>
                                    <li><strong>Time translation:</strong> Physics doesn't depend on when you perform an experiment</li>
                                    <li><strong>Space translation:</strong> Physics doesn't depend on where you are</li>
                                    <li><strong>Spatial rotation:</strong> Physics doesn't depend on orientation</li>
                                    <li><strong>Gauge symmetries:</strong> Internal symmetries (e.g., phase changes in quantum mechanics)</li>
                                </ul>

                                <h3>The Symmetry ‚Üí Conservation Law Correspondence</h3>

                                <p>Noether's theorem establishes specific correspondences:</p>

                                <h4>1. Time Translation Symmetry ‚Üí Energy Conservation</h4>

                                <p>If the Lagrangian doesn't explicitly depend on time ($\frac{\partial L}{\partial t} = 0$), then the system is symmetric under time translations. <strong>Consequence:</strong> Energy is conserved!</p>

                                <div class="math-block">
                                    $$E = \sum_i \dot{q}_i\frac{\partial L}{\partial \dot{q}_i} - L = H \quad \text{(Hamiltonian, total energy)}$$
                                </div>

                                <p><strong>Physical meaning:</strong> The laws of physics are the same today as they were yesterday or will be tomorrow. This temporal homogeneity <em>requires</em> energy conservation!</p>

                                <h4>2. Space Translation Symmetry ‚Üí Linear Momentum Conservation</h4>

                                <p>If the Lagrangian doesn't depend on absolute position (only on relative positions and velocities), the system is translationally invariant. <strong>Consequence:</strong> Linear momentum is conserved!</p>

                                <div class="math-block">
                                    $$\vec{P} = \sum_i \frac{\partial L}{\partial \dot{\vec{q}}_i} = \sum_i m_i\vec{v}_i = \text{constant}$$
                                </div>

                                <p><strong>Physical meaning:</strong> Physics is the same everywhere in empty space. You can't tell if you're in a laboratory on Earth or floating in deep space (ignoring gravity) - the fundamental laws don't change. This spatial homogeneity guarantees momentum conservation!</p>

                                <h4>3. Rotational Symmetry ‚Üí Angular Momentum Conservation</h4>

                                <p>If the Lagrangian is invariant under rotations (isotropic), then angular momentum is conserved!</p>

                                <div class="math-block">
                                    $$\vec{L} = \sum_i \vec{r}_i \times \vec{p}_i = \sum_i \vec{r}_i \times (m_i\vec{v}_i) = \text{constant}$$
                                </div>

                                <p><strong>Physical meaning:</strong> Physics has no preferred direction. North is no different from east or any other direction. This isotropy of space demands angular momentum conservation!</p>

                                <div class="example-box">
                                    <p><strong>Example - Planetary Orbits:</strong> The gravitational potential $V(r) = -\frac{GMm}{r}$ depends only on distance $r$, not on direction - it's spherically symmetric (rotationally invariant).<br><br>
                                    <strong>Consequence:</strong> Angular momentum $\vec{L} = \vec{r} \times m\vec{v}$ is conserved! This is why planets orbit in planes - the orbital plane is perpendicular to the conserved angular momentum vector. Kepler's second law (equal areas in equal times) is a direct consequence of angular momentum conservation.</p>
                                </div>

                                <h3>Noether's Theorem: The Mathematical Statement</h3>

                                <p>More formally, for a system with Lagrangian $L(q, \dot{q}, t)$:</p>

                                <p><strong>If:</strong> The action $S = \int L \, dt$ is invariant under a continuous transformation $q_i \to q_i + \epsilon \xi_i(q)$</p>

                                <p><strong>Then:</strong> The quantity</p>

                                <div class="math-block">
                                    $$Q = \sum_i \frac{\partial L}{\partial \dot{q}_i}\xi_i(q)$$
                                </div>

                                <p>is conserved: $\frac{dQ}{dt} = 0$</p>

                                <p>The function $\xi_i(q)$ is called the <strong>generator</strong> of the symmetry transformation. Different generators give different conserved quantities!</p>

                                <details style="margin: 1rem 0; padding: 1rem; background: rgba(0, 212, 255, 0.05); border-left: 3px solid var(--accent-blue); border-radius: var(--radius-sm); cursor: pointer;">
                                    <summary style="font-weight: 600; color: var(--accent-blue); cursor: pointer; user-select: none;">Proof Sketch of Noether's Theorem</summary>
                                    <div style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border-color);">
                                        <p>Consider an infinitesimal transformation: $q_i \to q_i' = q_i + \epsilon\xi_i(q)$ and $t \to t' = t + \epsilon\tau$</p>
                                        
                                        <p>The action must be invariant: $S' = S$, meaning:</p>
                                        <div class="math-block">
                                            $$\delta S = \int \delta L \, dt = 0$$
                                        </div>
                                        
                                        <p>The variation of the Lagrangian is:</p>
                                        <div class="math-block">
                                            $$\delta L = \sum_i \left(\frac{\partial L}{\partial q_i}\delta q_i + \frac{\partial L}{\partial \dot{q}_i}\delta\dot{q}_i\right) + \frac{\partial L}{\partial t}\delta t$$
                                        </div>
                                        
                                        <p>Using Euler-Lagrange equations $\frac{\partial L}{\partial q_i} = \frac{d}{dt}\frac{\partial L}{\partial \dot{q}_i}$ and integrating by parts:</p>
                                        <div class="math-block">
                                            $$0 = \delta S = \int \frac{d}{dt}\left(\sum_i \frac{\partial L}{\partial \dot{q}_i}\xi_i + L\tau\right) dt$$
                                        </div>
                                        
                                        <p>For this to be zero for all paths, the integrand must be a total time derivative. Therefore:</p>
                                        <div class="math-block">
                                            $$Q = \sum_i \frac{\partial L}{\partial \dot{q}_i}\xi_i - HL\tau$$
                                        </div>
                                        
                                        <p>is conserved: $\frac{dQ}{dt} = 0$. For time-independent transformations ($\tau = 0$), we get $Q = \sum_i p_i\xi_i$ where $p_i = \frac{\partial L}{\partial \dot{q}_i}$ is the generalized momentum.</p>
                                        
                                        <p><strong>QED</strong> (simplified). This elegant proof shows how symmetries automatically generate conserved quantities!</p>
                                    </div>
                                </details>

                                <h3>Examples of Symmetry-Conservation Pairs</h3>

                                <div class="example-box">
                                    <p><strong>Free Particle:</strong> $L = \frac{1}{2}m\dot{\vec{r}}^2$ (no potential, no forces)<br><br>
                                    <strong>Symmetries:</strong><br>
                                    ‚Ä¢ Time translation: $t \to t + \epsilon$ ‚Üí Energy conserved: $E = \frac{1}{2}m\vec{v}^2$<br>
                                    ‚Ä¢ Space translation: $\vec{r} \to \vec{r} + \vec{\epsilon}$ ‚Üí Momentum conserved: $\vec{p} = m\vec{v}$<br>
                                    ‚Ä¢ Rotation: $\vec{r} \to R\vec{r}$ ‚Üí Angular momentum conserved: $\vec{L} = \vec{r} \times m\vec{v}$<br><br>
                                    A free particle has all three symmetries, so all three quantities are conserved!</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Central Force:</strong> $V(\vec{r}) = V(r)$ where $r = |\vec{r}|$<br><br>
                                    Potential depends only on distance, not direction - spherically symmetric!<br>
                                    <strong>Symmetry:</strong> Rotational invariance<br>
                                    <strong>Conservation law:</strong> Angular momentum $\vec{L} = \vec{r} \times m\vec{v}$<br><br>
                                    This is why planetary orbits stay in a fixed plane!</p>
                                </div>

                                <h3>Conserved Quantities in Phase Space</h3>

                                <p>In Hamiltonian mechanics, conserved quantities have a geometric interpretation in phase space:</p>

                                <p><strong>Constants of motion</strong> are functions $f(q, p)$ that don't change along trajectories: $\frac{df}{dt} = 0$. These define <strong>invariant surfaces</strong> (level sets) in phase space where $f = \text{constant}$.</p>

                                <p>For an $n$-degree-of-freedom system:</p>
                                <ul>
                                    <li><strong>1 constant:</strong> Motion restricted to $(2n-1)$-dimensional surface</li>
                                    <li><strong>2 constants:</strong> Motion restricted to $(2n-2)$-dimensional surface</li>
                                    <li><strong>n constants (integrable):</strong> Motion restricted to $n$-dimensional torus!</li>
                                </ul>

                                <p><strong>Poisson bracket formulation:</strong> A function $f(q,p)$ is conserved if its Poisson bracket with the Hamiltonian vanishes:</p>

                                <div class="math-block">
                                    $$\{f, H\} = \sum_i \left(\frac{\partial f}{\partial q_i}\frac{\partial H}{\partial p_i} - \frac{\partial f}{\partial p_i}\frac{\partial H}{\partial q_i}\right) = 0$$
                                </div>

                                <p>Two constants $f$ and $g$ are <strong>in involution</strong> if $\{f, g\} = 0$. An integrable system needs $n$ independent constants all in involution with each other!</p>

                                <div class="info-box">
                                    <p><strong>In Three-Body Problem:</strong> The Gravitation¬≥ three-body simulation has several conserved quantities:<br>
                                    ‚Ä¢ <strong>Total energy</strong> $E = T + V$ (Hamiltonian)<br>
                                    ‚Ä¢ <strong>Total linear momentum</strong> $\vec{P} = \sum m_i\vec{v}_i$ (space translation symmetry)<br>
                                    ‚Ä¢ <strong>Total angular momentum</strong> $\vec{L} = \sum \vec{r}_i \times m_i\vec{v}_i$ (rotational symmetry)<br><br>
                                    These 7 conservation laws (1 energy + 3 momentum + 3 angular momentum) reduce the 18-dimensional phase space to an 11-dimensional invariant manifold! Yet the system is still chaotic because it's not integrable - we'd need 9 independent constants for integrability.</p>
                                </div>

                                <h3>Symmetry Breaking</h3>

                                <p><strong>Symmetry breaking</strong> occurs when the ground state or solution of a system has less symmetry than the equations governing it. This is crucial in phase transitions and fundamental physics!</p>

                                <h4>Spontaneous Symmetry Breaking</h4>

                                <p><strong>Spontaneous symmetry breaking</strong> happens when a symmetric system settles into an asymmetric state. The equations have a symmetry, but the solution doesn't.</p>

                                <div class="example-box">
                                    <p><strong>Classic Example - The Mexican Hat Potential:</strong><br>
                                    Potential: $V(\phi) = -\frac{1}{2}\mu^2\phi^2 + \frac{1}{4}\lambda\phi^4$ (where $\mu^2, \lambda > 0$)<br><br>
                                    This has rotational symmetry in $\phi$ space. But the minimum isn't at $\phi = 0$ - it's at $\phi = \pm\sqrt{\mu^2/\lambda}$!<br><br>
                                    The system must "choose" a direction (+) or (-), spontaneously breaking the symmetry. Which direction is random, determined by fluctuations. Once chosen, that symmetry is broken.</p>
                                </div>

                                <div class="example-box">
                                    <p><strong>Physical Example - Magnetism:</strong> A ferromagnet above the Curie temperature has no preferred direction - rotationally symmetric. As it cools below Curie temperature, magnetic domains align in some direction, spontaneously breaking rotational symmetry. The equations (microscopic interactions) are still symmetric, but the macroscopic state isn't!</p>
                                </div>

                                <h4>Explicit Symmetry Breaking</h4>

                                <p><strong>Explicit symmetry breaking</strong> occurs when we add a term to the equations that breaks the symmetry explicitly (e.g., an external field).</p>

                                <p>For example, adding a magnetic field $\vec{B}$ to a rotationally symmetric system breaks rotational symmetry - the field picks out a preferred direction.</p>

                                <h3>Goldstone's Theorem</h3>

                                <p>When a continuous symmetry is spontaneously broken, <strong>Goldstone's theorem</strong> says there must exist massless excitations called <strong>Goldstone bosons</strong>. These are modes that cost zero energy to excite.</p>

                                <p>In the Mexican hat potential, once the system settles into a valley, moving along the valley (changing the phase) costs no energy - that's the Goldstone mode!</p>

                                <div class="info-box">
                                    <p><strong>In Particle Physics:</strong> The Higgs mechanism involves spontaneous symmetry breaking of electroweak symmetry. Goldstone bosons would appear, but they're "eaten" by the gauge bosons (W and Z), giving them mass. The remaining Higgs boson is the massive excitation along the radial direction of the Mexican hat!</p>
                                </div>

                                <h3>Broken and Approximate Symmetries</h3>

                                <p>Real systems often have <strong>approximate symmetries</strong> - symmetries that are almost but not quite perfect:</p>

                                <ul>
                                    <li><strong>Isospin symmetry:</strong> Protons and neutrons are nearly identical (broken by electromagnetic effects)</li>
                                    <li><strong>Parity:</strong> Most forces respect mirror symmetry, but weak nuclear force doesn't!</li>
                                    <li><strong>Chiral symmetry:</strong> Approximately conserved in QCD, broken by quark masses</li>
                                </ul>

                                <p>Broken symmetries lead to <strong>approximate conservation laws</strong> that are useful but not exact.</p>

                                <h3>Conservation in Our Simulations</h3>

                                <div class="info-box">
                                    <p><strong>Three-Body Problem:</strong> The simulation conserves:<br>
                                    ‚Ä¢ <strong>Energy:</strong> Sum of kinetic and gravitational potential energy stays constant (time translation symmetry)<br>
                                    ‚Ä¢ <strong>Linear momentum:</strong> Total momentum of all three bodies is constant (space translation symmetry)<br>
                                    ‚Ä¢ <strong>Angular momentum:</strong> Total angular momentum about the center of mass is constant (rotational symmetry)<br><br>
                                    These conservation laws constrain the chaotic motion - trajectories must stay on surfaces defined by constant $E$, $\vec{P}$, and $\vec{L}$. This is why the three-body problem, despite being chaotic, isn't completely random - it has structure imposed by symmetries!</p>
                                </div>

                                <div class="info-box">
                                    <p><strong>Lorenz Attractor:</strong> The Lorenz equations $\frac{dx}{dt} = \sigma(y-x)$, $\frac{dy}{dt} = x(\rho-z)-y$, $\frac{dz}{dt} = xy-\beta z$ have a symmetry:<br>
                                    $(x, y, z) \to (-x, -y, z)$ (rotation by 180¬∞ around z-axis)<br><br>
                                    This symmetry explains the attractor's two-lobed butterfly structure - the two wings are related by this symmetry! While energy isn't conserved (the system is dissipative), this discrete symmetry shapes the attractor's geometry.</p>
                                </div>

                                <div class="info-box">
                                    <p><strong>Fluid Simulations:</strong> The Navier-Stokes equations conserve:<br>
                                    ‚Ä¢ <strong>Mass:</strong> $\frac{\partial \rho}{\partial t} + \nabla \cdot (\rho\vec{v}) = 0$ (continuity equation)<br>
                                    ‚Ä¢ <strong>Momentum:</strong> When integrated over a closed domain with no-slip boundaries<br>
                                    ‚Ä¢ <strong>Energy:</strong> In the inviscid limit (Euler equations)<br><br>
                                    For incompressible flow, $\nabla \cdot \vec{v} = 0$ itself is a constraint from mass conservation! The velocity field must be divergence-free, dramatically constraining the allowed flows.</p>
                                </div>

                                <h3>Applications Beyond Classical Mechanics</h3>

                                <p>Noether's theorem extends far beyond classical mechanics:</p>

                                <ul>
                                    <li><strong>Quantum field theory:</strong> Gauge symmetries lead to charge conservation (electric, color, weak)</li>
                                    <li><strong>General relativity:</strong> Local energy-momentum conservation from diffeomorphism invariance</li>
                                    <li><strong>Cosmology:</strong> Cosmic inflation and spontaneous symmetry breaking in early universe</li>
                                    <li><strong>Condensed matter:</strong> Superconductivity, superfluidity involve broken symmetries</li>
                                    <li><strong>Particle physics:</strong> Standard Model built on gauge symmetries and their breaking</li>
                                </ul>

                                <h3>Why This Matters</h3>

                                <p>Noether's theorem reveals that conservation laws aren't coincidences or experimental observations that might fail under extreme conditions. They're <em>consequences of the structure of spacetime itself</em>.</p>

                                <p>When we see energy conserved in the three-body simulation, it's not because we programmed it to be conserved - it's because the gravitational laws respect time translation symmetry. When angular momentum is conserved, it's because space is isotropic. The deepest laws of physics emerge from symmetry!</p>

                                <p>As physicist Eugene Wigner said: "It is now natural for us to try to derive the laws of nature and to test their validity by means of the laws of invariance, rather than to derive the laws of invariance from what we believe to be the laws of nature."</p>

                                <div class="example-box">
                                    <p><strong>Summary - The Power of Symmetry:</strong><br><br>
                                    <strong>Time translation ‚Üí Energy conservation</strong><br>
                                    The universe today has the same laws as yesterday<br><br>
                                    <strong>Space translation ‚Üí Momentum conservation</strong><br>
                                    The universe is the same here as anywhere else<br><br>
                                    <strong>Rotation ‚Üí Angular momentum conservation</strong><br>
                                    The universe has no preferred direction<br><br>
                                    These aren't separate facts - they're all manifestations of spacetime's fundamental symmetries! Noether's theorem unifies geometry and physics in a profound way.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 5.7 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">5.7</span>
                            <span>Advanced Numerical Methods</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Why Advanced Numerical Methods Matter</h2>
                                
                                <p>While basic methods like Euler and RK4 work for many problems, sophisticated simulations require specialized techniques. These advanced methods address critical challenges:</p>

                                <ul>
                                    <li><strong>Long-time accuracy:</strong> How to maintain precision over millions of time steps</li>
                                    <li><strong>Structure preservation:</strong> Conserving energy, momentum, symplectic structure</li>
                                    <li><strong>Multi-scale phenomena:</strong> Handling vastly different length and time scales</li>
                                    <li><strong>Computational efficiency:</strong> Getting accurate results without excessive computation</li>
                                </ul>

                                <div class="info-box">
                                    <p><strong>In Gravitation¬≥:</strong> All our simulations rely on these advanced methods! The three-body problem uses symplectic integrators to maintain energy conservation over long times. Fluid simulations use spectral methods for accuracy and adaptive mesh refinement to handle fine-scale structures. Understanding these methods helps you appreciate what's happening "under the hood."</p>
                                </div>

                                <h3>Symplectic Integrators: Preserving Structure</h3>

                                <p><strong>Symplectic integrators</strong> are numerical methods specifically designed for Hamiltonian systems. Unlike standard methods, they preserve the symplectic structure of phase space - the geometric structure that Hamiltonian flows naturally respect.</p>

                                <h4>What Makes a Method Symplectic?</h4>

                                <p>A numerical method is <strong>symplectic</strong> if it preserves the symplectic 2-form, which in canonical coordinates $(q, p)$ is:</p>

                                <div class="math-block">
                                    $$\omega = dq \wedge dp$$
                                </div>

                                <p>More practically, a one-step method $\Phi: (q_n, p_n) \to (q_{n+1}, p_{n+1})$ is symplectic if its Jacobian satisfies:</p>

                                <div class="math-block">
                                    $$J^T \Omega J = \Omega$$
                                </div>

                                <p>where $\Omega = \begin{pmatrix} 0 & I \\ -I & 0 \end{pmatrix}$ is the symplectic matrix.</p>

                                <p><strong>Why this matters:</strong> Symplectic methods preserve the volume of phase space (Liouville's theorem) and provide excellent long-term energy conservation, even without being energy-conserving at each step!</p>

                                <h4>The Simplest Symplectic Integrator: Symplectic Euler</h4>

                                <p>For a separable Hamiltonian $H(q, p) = T(p) + V(q)$ where $T$ is kinetic energy and $V$ is potential energy, the <strong>symplectic Euler method</strong> alternates updates:</p>

                                <p><strong>Version A (position first):</strong></p>
                                <div class="math-block">
                                    $$q_{n+1} = q_n + h\frac{\partial H}{\partial p}(q_n, p_n) = q_n + h\frac{p_n}{m}$$
                                    $$p_{n+1} = p_n - h\frac{\partial H}{\partial q}(q_{n+1}, p_n) = p_n - h\frac{\partial V}{\partial q}(q_{n+1})$$
                                </div>

                                <p><strong>Version B (momentum first):</strong></p>
                                <div class="math-block">
                                    $$p_{n+1} = p_n - h\frac{\partial V}{\partial q}(q_n)$$
                                    $$q_{n+1} = q_n + h\frac{p_{n+1}}{m}$$
                                </div>

                                <p>Notice the crucial difference from regular Euler: we use the <em>updated</em> value in the second equation! This coupling makes it symplectic.</p>

                                <div class="example-box">
                                    <p><strong>Harmonic Oscillator Test:</strong> For $H = \frac{p^2}{2m} + \frac{1}{2}kq^2$:<br><br>
                                    <strong>Regular Euler:</strong> Energy grows exponentially - spiral outward, completely unphysical!<br>
                                    <strong>Symplectic Euler:</strong> Energy oscillates slightly but stays bounded - approximate ellipse in phase space<br>
                                    <strong>Verlet (symplectic):</strong> Energy oscillates with much smaller amplitude - nearly perfect circle<br><br>
                                    After 10,000 steps, regular Euler might have 1000√ó the initial energy, while symplectic Euler stays within ~1% and Verlet within ~0.01%!</p>
                                </div>

                                <h4>Verlet Algorithm (Leapfrog Method)</h4>

                                <p>The <strong>Verlet algorithm</strong> is the workhorse of molecular dynamics and gravitational simulations. It's second-order accurate and symplectic!</p>

                                <p><strong>Position Verlet:</strong></p>
                                <div class="math-block">
                                    $$q_{n+1} = q_n + h\frac{p_n}{m} + \frac{h^2}{2m}F(q_n)$$
                                    $$p_{n+1} = p_n + \frac{h}{2}[F(q_n) + F(q_{n+1})]$$
                                </div>

                                <p><strong>Velocity Verlet (more common form):</strong></p>
                                <div class="math-block">
                                    $$q_{n+1} = q_n + hv_n + \frac{h^2}{2m}F(q_n)$$
                                    $$v_{n+1} = v_n + \frac{h}{2m}[F(q_n) + F(q_{n+1})]$$
                                </div>

                                <p><strong>Why Verlet is amazing:</strong></p>
                                <ul>
                                    <li>Second-order accurate (error $\sim O(h^2)$)</li>
                                    <li>Symplectic - preserves phase space structure</li>
                                    <li>Time-reversible - can run simulation backward exactly</li>
                                    <li>Excellent long-term energy conservation</li>
                                    <li>Only requires one force evaluation per step</li>
                                </ul>

                                <div class="info-box">
                                    <p><strong>In Three-Body Simulations:</strong> The Gravitation¬≥ three-body problem could use Verlet or higher-order symplectic integrators. Over millions of orbits, symplectic methods maintain energy conservation to remarkable precision - regular methods would show artificial energy drift that creates orbits that slowly spiral in or out, completely wrong physics!</p>
                                </div>

                                <h4>Higher-Order Symplectic Integrators</h4>

                                <p>For even better accuracy, we can construct fourth-order or higher symplectic integrators using <strong>composition methods</strong>. The idea: combine multiple symplectic steps with carefully chosen coefficients.</p>

                                <p><strong>Fourth-Order Forest-Ruth Method:</strong></p>
                                <p>Apply a sequence of symplectic Euler steps with special coefficients $\theta_i$:</p>
                                <div class="math-block">
                                    $$\theta_1 = \theta_4 = \frac{1}{2(2-2^{1/3})}$$
                                    $$\theta_2 = \theta_3 = \frac{1-2^{1/3}}{2(2-2^{1/3})}$$
                                </div>

                                <p>This achieves fourth-order accuracy while remaining symplectic! The price is multiple force evaluations per step.</p>

                                <p><strong>Trade-off:</strong> Higher-order methods require more work per step but allow larger time steps for the same accuracy. Choose based on whether force evaluation or number of steps is the bottleneck.</p>

                                <h3>Spectral Methods for PDEs</h3>

                                <p><strong>Spectral methods</strong> represent functions as sums of basis functions (typically sines/cosines or polynomials) and solve PDEs in this transformed space. They achieve "exponential convergence" - error decreases faster than any power of grid spacing!</p>

                                <h4>The Fourier Spectral Method</h4>

                                <p>For periodic problems, represent the solution as a Fourier series:</p>

                                <div class="math-block">
                                    $$u(x, t) = \sum_{k=-N/2}^{N/2} \hat{u}_k(t)e^{ikx}$$
                                </div>

                                <p>where $\hat{u}_k(t)$ are the Fourier coefficients. In this representation:</p>

                                <p><strong>Spatial derivatives become algebraic:</strong></p>
                                <div class="math-block">
                                    $$\frac{\partial u}{\partial x} \leftrightarrow ik\hat{u}_k$$
                                    $$\frac{\partial^2 u}{\partial x^2} \leftrightarrow -k^2\hat{u}_k$$
                                </div>

                                <p>This transforms PDEs into ODEs for the Fourier coefficients!</p>

                                <div class="example-box">
                                    <p><strong>Example - Heat Equation:</strong> $\frac{\partial u}{\partial t} = \alpha\frac{\partial^2 u}{\partial x^2}$<br><br>
                                    In Fourier space: $\frac{d\hat{u}_k}{dt} = -\alpha k^2\hat{u}_k$<br><br>
                                    This is just exponential decay! Solution: $\hat{u}_k(t) = \hat{u}_k(0)e^{-\alpha k^2 t}$<br><br>
                                    Transform back to get $u(x, t)$. Each Fourier mode decays independently at rate $\alpha k^2$ - high frequencies (small scales) decay fastest!</p>
                                </div>

                                <h4>Why Spectral Methods Excel</h4>

                                <ul>
                                    <li><strong>Exponential accuracy:</strong> For smooth solutions, error decreases as $e^{-cN}$ where $N$ is the number of modes. Much faster than $N^{-p}$ for finite differences!</li>
                                    <li><strong>Exact differentiation:</strong> Derivatives are computed exactly (up to floating-point precision) in Fourier space</li>
                                    <li><strong>Global representation:</strong> Each mode contains information about the entire domain</li>
                                    <li><strong>Efficient with FFT:</strong> Fast Fourier Transform allows $O(N\log N)$ computation</li>
                                </ul>

                                <p><strong>Limitation:</strong> Requires periodic boundary conditions (or careful treatment of boundaries). For non-smooth solutions or shocks, spectral methods can show Gibbs phenomenon (oscillations near discontinuities).</p>

                                <h4>Pseudo-Spectral Methods for Nonlinear PDEs</h4>

                                <p>For nonlinear PDEs like Navier-Stokes, we can't solve everything in Fourier space. The <strong>pseudo-spectral method</strong> combines spectral and physical space:</p>

                                <ol>
                                    <li>Compute derivatives in Fourier space (spectral accuracy)</li>
                                    <li>Transform to physical space to evaluate nonlinear terms</li>
                                    <li>Transform back to Fourier space for time stepping</li>
                                </ol>

                                <p>This avoids expensive convolution in Fourier space while maintaining spectral accuracy for derivatives!</p>

                                <div class="info-box">
                                    <p><strong>In Fluid Simulations:</strong> Many Gravitation¬≥ fluid simulations could use spectral methods for high accuracy! The periodic boundary conditions in some simulations make them perfect for Fourier spectral methods. Turbulent flows benefit enormously - spectral methods resolve the wide range of scales in turbulence efficiently.</p>
                                </div>

                                <h4>Spectral Element Methods</h4>

                                <p><strong>Spectral element methods</strong> combine the accuracy of spectral methods with the flexibility of finite elements. The domain is divided into elements, and within each element, the solution is represented by high-order polynomials.</p>

                                <p><strong>Advantages:</strong></p>
                                <ul>
                                    <li>Handle complex geometries (like finite elements)</li>
                                    <li>Maintain spectral accuracy within each element</li>
                                    <li>Naturally handle non-periodic boundaries</li>
                                    <li>Allow local refinement where needed</li>
                                </ul>

                                <h3>Adaptive Mesh Refinement (AMR)</h3>

                                <p><strong>Adaptive Mesh Refinement</strong> dynamically adjusts grid resolution based on solution features. Why use fine resolution everywhere when you only need it where the action is?</p>

                                <h4>The AMR Philosophy</h4>

                                <p>Many physical phenomena have multi-scale structure:</p>
                                <ul>
                                    <li>Shock waves have thin transition layers</li>
                                    <li>Vortices have concentrated cores</li>
                                    <li>Combustion fronts are narrow</li>
                                    <li>Gravitational systems cluster in some regions</li>
                                </ul>

                                <p>A uniform fine grid everywhere wastes computation. AMR concentrates resolution where needed!</p>

                                <h4>AMR Algorithm (Berger-Oliger Approach)</h4>

                                <p><strong>1. Hierarchy of grids:</strong> Start with a coarse base grid. Add refined patches where needed.</p>

                                <p><strong>2. Error estimation:</strong> Compute local error indicators:</p>
                                <ul>
                                    <li>Solution gradients (large gradients ‚Üí refine)</li>
                                    <li>Truncation error estimates</li>
                                    <li>Physical criteria (e.g., vorticity magnitude)</li>
                                    <li>Richardson extrapolation</li>
                                </ul>

                                <p><strong>3. Refinement:</strong> Where error exceeds threshold, create finer grid patch (typically 2√ó or 4√ó finer).</p>

                                <p><strong>4. Time stepping:</strong> Advance each level with appropriate time step. Finer grids often use smaller time steps.</p>

                                <p><strong>5. Synchronization:</strong> After fine grid steps, update coarse grid with averaged fine grid data.</p>

                                <p><strong>6. Regrid:</strong> Periodically reassess refinement needs and adjust grid hierarchy.</p>

                                <div class="example-box">
                                    <p><strong>Example - Shock Tube:</strong> Consider a shock propagating through a tube:<br><br>
                                    <strong>Uniform grid:</strong> Need fine resolution everywhere to capture shock - millions of cells<br>
                                    <strong>AMR:</strong> Fine resolution only at shock front - thousands of cells at fine level, coarse elsewhere<br><br>
                                    As shock moves, refinement follows it! AMR might be 100√ó faster while maintaining accuracy at the shock.</p>
                                </div>

                                <h4>Block-Based vs. Cell-Based AMR</h4>

                                <p><strong>Block-based AMR:</strong> Refine rectangular patches (blocks) of cells</p>
                                <ul>
                                    <li>Easier to implement and parallelize</li>
                                    <li>Better cache performance</li>
                                    <li>May refine more cells than strictly needed</li>
                                </ul>

                                <p><strong>Cell-based (tree-based) AMR:</strong> Refine individual cells or use tree structures</p>
                                <ul>
                                    <li>More flexible refinement</li>
                                    <li>Minimal wasted computation</li>
                                    <li>More complex data structure and neighbor finding</li>
                                </ul>

                                <h4>Challenges in AMR</h4>

                                <ul>
                                    <li><strong>Conservation:</strong> Ensuring conserved quantities remain conserved across different grid levels</li>
                                    <li><strong>Stability:</strong> Refining/de-refining can introduce numerical artifacts</li>
                                    <li><strong>Load balancing:</strong> In parallel computing, refined regions create work imbalance</li>
                                    <li><strong>Interpolation:</strong> Need accurate interpolation between coarse and fine grids</li>
                                </ul>

                                <div class="info-box">
                                    <p><strong>In Astrophysical Simulations:</strong> AMR is crucial for simulating galaxy formation, star formation, and cosmological structure. Scales range from megaparsecs (large-scale structure) to AU (planetary systems) - a factor of $10^{14}$! Without AMR, such simulations would be impossible. Gravitation¬≥'s simulations are often uniform grids for simplicity, but production codes use sophisticated AMR.</p>
                                </div>

                                <h3>Multi-Scale Time Stepping</h3>

                                <p>Just as spatial scales vary, time scales can differ dramatically. <strong>Multi-scale time stepping</strong> uses different time steps for different components or regions.</p>

                                <h4>Hierarchical Time Stepping</h4>

                                <p>In AMR, finer grids often use smaller time steps:</p>
                                <ul>
                                    <li>Coarse grid: $\Delta t$</li>
                                    <li>First refinement level: $\Delta t/2$</li>
                                    <li>Second refinement level: $\Delta t/4$</li>
                                    <li>etc.</li>
                                </ul>

                                <p>This respects the CFL condition (Courant-Friedrichs-Lewy) at each level while avoiding tiny time steps on the coarse grid.</p>

                                <h4>Implicit-Explicit (IMEX) Methods</h4>

                                <p>For equations with both fast and slow processes, <strong>IMEX schemes</strong> treat stiff terms implicitly and non-stiff terms explicitly:</p>

                                <div class="math-block">
                                    $$\frac{du}{dt} = F_{\text{non-stiff}}(u) + F_{\text{stiff}}(u)$$
                                </div>

                                <p>Use explicit method for $F_{\text{non-stiff}}$ (cheap, avoids small time steps) and implicit for $F_{\text{stiff}}$ (stable, handles fast dynamics).</p>

                                <p><strong>Example:</strong> In combustion, chemical reactions (fast, stiff) treated implicitly; advection (slower) treated explicitly.</p>

                                <h3>Maintaining Physical Constraints</h3>

                                <p>Some equations have constraints that must be maintained:</p>

                                <h4>Divergence-Free Velocity (Incompressible Flow)</h4>

                                <p>For incompressible Navier-Stokes, $\nabla \cdot \vec{v} = 0$ must hold. <strong>Projection methods</strong> enforce this:</p>

                                <ol>
                                    <li>Compute provisional velocity $\vec{v}^*$ (may not be divergence-free)</li>
                                    <li>Decompose: $\vec{v}^* = \vec{v}^{n+1} + \nabla \phi$</li>
                                    <li>Take divergence: $\nabla \cdot \vec{v}^* = \nabla^2 \phi$ (Poisson equation)</li>
                                    <li>Solve for pressure-like field $\phi$</li>
                                    <li>Project: $\vec{v}^{n+1} = \vec{v}^* - \nabla \phi$</li>
                                </ol>

                                <p>Now $\nabla \cdot \vec{v}^{n+1} = 0$ exactly! This is the <strong>Chorin projection method</strong>.</p>

                                <h4>Positivity-Preserving Schemes</h4>

                                <p>Some quantities must remain non-negative (density, concentration, probability). <strong>Positivity-preserving schemes</strong> guarantee $\rho^{n+1} \geq 0$ even if standard methods would give negative values.</p>

                                <p>Techniques include:</p>
                                <ul>
                                    <li>Flux limiters (prevent overshoots)</li>
                                    <li>Maximum principle preserving discretizations</li>
                                    <li>Conservative, monotone schemes</li>
                                </ul>

                                <h3>Error Control and Adaptivity</h3>

                                <p>How do we know our simulation is accurate? <strong>Error control</strong> measures and controls discretization error.</p>

                                <h4>A Posteriori Error Estimates</h4>

                                <p>After computing a solution, estimate the error:</p>

                                <ul>
                                    <li><strong>Richardson extrapolation:</strong> Compare solutions with different resolutions</li>
                                    <li><strong>Residual-based:</strong> Check how well solution satisfies original equation</li>
                                    <li><strong>Adjoint-based:</strong> Use adjoint problem to estimate error in quantities of interest</li>
                                </ul>

                                <p>Based on error estimates, adapt mesh or time step to meet accuracy goals.</p>

                                <h4>Goal-Oriented Adaptivity</h4>

                                <p>Instead of minimizing error everywhere, minimize error in a <strong>quantity of interest</strong> (e.g., drag force on an airplane wing, heat flux through a boundary).</p>

                                <p>The adjoint equation identifies which regions most affect the goal - refine there preferentially!</p>

                                <div class="info-box">
                                    <p><strong>In Engineering:</strong> Goal-oriented AMR is crucial for practical simulations. You don't need perfect accuracy everywhere - you need accurate drag coefficient, lift, heat transfer, etc. By solving the adjoint problem, you discover where refinement most improves your answer. This can reduce computational cost by orders of magnitude while achieving target accuracy.</p>
                                </div>

                                <h3>Summary: Choosing the Right Method</h3>

                                <p>Select numerical methods based on problem characteristics:</p>

                                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                                    <tr style="background: rgba(255,255,255,0.1); border-bottom: 2px solid var(--border-color);">
                                        <th style="padding: 0.75rem; text-align: left; color: var(--text-primary);">Problem Type</th>
                                        <th style="padding: 0.75rem; text-align: left; color: var(--text-primary);">Recommended Method</th>
                                    </tr>
                                    <tr style="border-bottom: 1px solid var(--border-color);">
                                        <td style="padding: 0.75rem;">Hamiltonian systems (long-time)</td>
                                        <td style="padding: 0.75rem;">Symplectic integrators (Verlet, Forest-Ruth)</td>
                                    </tr>
                                    <tr style="border-bottom: 1px solid var(--border-color);">
                                        <td style="padding: 0.75rem;">Smooth periodic PDEs</td>
                                        <td style="padding: 0.75rem;">Fourier spectral methods</td>
                                    </tr>
                                    <tr style="border-bottom: 1px solid var(--border-color);">
                                        <td style="padding: 0.75rem;">PDEs with localized features</td>
                                        <td style="padding: 0.75rem;">Adaptive mesh refinement</td>
                                    </tr>
                                    <tr style="border-bottom: 1px solid var(--border-color);">
                                        <td style="padding: 0.75rem;">Multi-scale dynamics</td>
                                        <td style="padding: 0.75rem;">IMEX or hierarchical time stepping</td>
                                    </tr>
                                    <tr style="border-bottom: 1px solid var(--border-color);">
                                        <td style="padding: 0.75rem;">Incompressible fluids</td>
                                        <td style="padding: 0.75rem;">Projection methods + pseudo-spectral</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 0.75rem;">Shocks/discontinuities</td>
                                        <td style="padding: 0.75rem;">High-resolution methods (ENO, WENO)</td>
                                    </tr>
                                </table>

                                <div class="info-box">
                                    <p><strong>In Gravitation¬≥:</strong> Different simulations use different methods:<br>
                                    ‚Ä¢ <strong>Attractors:</strong> RK4 or adaptive RK45 (accuracy vs. speed trade-off)<br>
                                    ‚Ä¢ <strong>Three-body:</strong> Could use symplectic Verlet for energy conservation<br>
                                    ‚Ä¢ <strong>Fluids:</strong> Could benefit from spectral methods + AMR<br>
                                    ‚Ä¢ <strong>Vortex streets:</strong> Pseudo-spectral for accuracy in periodic domain<br><br>
                                    The right method depends on accuracy needs, computational budget, and physical properties being studied!</p>
                                </div>

                                <h3>The Art of Scientific Computing</h3>

                                <p>Numerical methods aren't just algorithms - they're computational physics. The best method:</p>

                                <ul>
                                    <li>Respects the physics (conserves what should be conserved)</li>
                                    <li>Balances accuracy and efficiency</li>
                                    <li>Handles the problem's specific challenges</li>
                                    <li>Provides diagnostics and error estimates</li>
                                    <li>Scales well to large problems</li>
                                </ul>

                                <p>Modern simulation is an interplay of mathematics, physics, and computer science. Understanding these advanced methods helps you create simulations that are not just visually appealing but physically accurate and computationally efficient!</p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 5.8 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">5.8</span>
                            <span>Solitons & Nonlinear Waves</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>What is a Soliton?</h2>
                                
                                <p>A <strong>soliton</strong> is a self-reinforcing solitary wave that maintains its shape while traveling at constant velocity. Unlike ordinary waves that spread out, interfere, and dissipate, solitons are remarkably stable - they can travel vast distances unchanged and even survive collisions with other solitons!</p>

                                <p>The word "soliton" was coined in 1965, but the phenomenon was first observed by Scottish engineer John Scott Russell in 1834. While riding horseback alongside a canal, he witnessed a large water wave that "rolled forward with great velocity, assuming the form of a large solitary elevation, a rounded, smooth and well-defined heap of water." He followed this wave on horseback for over a mile, watching it maintain its shape!</p>

                                <p>Russell called it a "wave of translation" and attempted to reproduce it experimentally. His observations were controversial - prevailing wave theory suggested such waves shouldn't exist. It took over a century for the mathematics to catch up.</p>

                                <h3>The Paradox of Wave Stability</h3>

                                <p>Why are solitons special? Ordinary waves face two competing effects:</p>

                                <ul>
                                    <li><strong>Dispersion:</strong> Different frequencies travel at different speeds, causing waves to spread out</li>
                                    <li><strong>Nonlinearity:</strong> Large amplitudes steepen waves, potentially causing breaking</li>
                                </ul>

                                <p>Separately, these effects destroy wave coherence. But in solitons, dispersion and nonlinearity <em>exactly balance</em>, creating a stable localized wave packet!</p>

                                <h3>The KdV Equation: The Soliton Equation</h3>

                                <p>The <strong>Korteweg-de Vries (KdV) equation</strong> is the prototypical soliton equation, describing shallow water waves:</p>

                                <div class="math-block">
                                    $$\frac{\partial u}{\partial t} + 6u\frac{\partial u}{\partial x} + \frac{\partial^3 u}{\partial x^3} = 0$$
                                </div>

                                <p>where $u(x, t)$ is the wave height. Let's decode the terms:</p>

                                <ul>
                                    <li><strong>$\frac{\partial u}{\partial t}$:</strong> Time evolution</li>
                                    <li><strong>$6u\frac{\partial u}{\partial x}$:</strong> Nonlinear term - wave steepening (larger amplitudes travel faster)</li>
                                    <li><strong>$\frac{\partial^3 u}{\partial x^3}$:</strong> Dispersion term - different wavelengths travel at different speeds</li>
                                </ul>

                                <p>The remarkable fact: these two opposing effects can perfectly balance!</p>

                                <h4>Single Soliton Solution</h4>

                                <p>The KdV equation has a <strong>traveling wave solution</strong> (soliton) of the form:</p>

                                <div class="math-block">
                                    $$u(x, t) = -\frac{c}{2}\text{sech}^2\left(\frac{\sqrt{c}}{2}(x - ct - x_0)\right)$$
                                </div>

                                <p>where $c > 0$ is the wave speed and $x_0$ is the initial position. This is a localized "hump" that travels without changing shape!</p>

                                <p><strong>Key properties:</strong></p>
                                <ul>
                                    <li><strong>Shape:</strong> $\text{sech}^2$ profile - smooth, bell-shaped pulse</li>
                                    <li><strong>Amplitude:</strong> Proportional to speed $c$ - taller waves travel faster!</li>
                                    <li><strong>Width:</strong> Proportional to $1/\sqrt{c}$ - taller waves are narrower</li>
                                    <li><strong>Stability:</strong> Exact balance between nonlinearity and dispersion</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Verification:</strong> We can verify this is a solution by substituting into the KdV equation. The nonlinear steepening exactly cancels the dispersive spreading - the wave maintains its shape! This is the mathematical miracle of solitons.</p>
                                </div>

                                <h3>Multiple Soliton Interactions</h3>

                                <p>The most remarkable property of solitons: when two solitons collide, they pass through each other and emerge unchanged! This is completely unlike ordinary waves, which interfere destructively or constructively and don't preserve their individual identities.</p>

                                <h4>Two-Soliton Solution</h4>

                                <p>The KdV equation has exact <strong>multi-soliton solutions</strong>. For two solitons with speeds $c_1 > c_2$:</p>

                                <ul>
                                    <li>Before collision: Two separate solitons approach each other</li>
                                    <li>During collision: Complex interference pattern, temporary distortion</li>
                                    <li>After collision: Both solitons re-emerge with <em>unchanged</em> shapes and speeds!</li>
                                </ul>

                                <p>The only lasting effect: each soliton experiences a <strong>phase shift</strong> - it emerges at a slightly different position than it would have without the collision. The faster soliton is delayed, the slower one is advanced.</p>

                                <div class="example-box">
                                    <p><strong>Physical Interpretation:</strong> Imagine two hikers at different speeds on a narrow trail. When they meet, they have to maneuver around each other (interaction), but afterward each continues at their original pace - just slightly ahead or behind where they would have been. Solitons behave similarly, but with perfect mathematical precision!</p>
                                </div>

                                <h4>Particle-Like Behavior</h4>

                                <p>The fact that solitons maintain their identity through collisions is astonishing - it's <em>particle-like</em> behavior! This is why they're called "solitons" (with the "-on" suffix like electron, photon, proton).</p>

                                <p><strong>Solitons vs. Particles:</strong></p>
                                <ul>
                                    <li>Both maintain identity through interactions</li>
                                    <li>Both can be labeled and tracked</li>
                                    <li>Solitons are classical (not quantum), extended objects (not point-like)</li>
                                    <li>Solitons emerge from wave equations, not particle mechanics</li>
                                </ul>

                                <div class="info-box">
                                    <p><strong>In Quantum Field Theory:</strong> There are quantum solitons! Magnetic monopoles, domain walls, cosmic strings, and instantons are topological solitons in field theories. Some physicists speculate fundamental particles themselves might be solitons in a deeper theory!</p>
                                </div>

                                <h3>Stability of Solitons</h3>

                                <p>Why do solitons maintain their shape? The answer lies in <strong>topological stability</strong> and conservation laws.</p>

                                <h4>Conservation Laws in KdV</h4>

                                <p>The KdV equation has <em>infinitely many</em> conservation laws! Examples include:</p>

                                <div class="math-block">
                                    $$I_1 = \int_{-\infty}^\infty u \, dx \quad \text{(mass)}$$
                                    $$I_2 = \int_{-\infty}^\infty u^2 \, dx \quad \text{(momentum)}$$
                                    $$I_3 = \int_{-\infty}^\infty \left(u^3 - \frac{1}{2}u_x^2\right) dx \quad \text{(energy)}$$
                                </div>

                                <p>These are all conserved: $\frac{dI_n}{dt} = 0$. The existence of infinitely many conserved quantities makes KdV <strong>integrable</strong> - we can solve it exactly!</p>

                                <p><strong>Integrability and Stability:</strong> The infinite conservation laws constrain the dynamics so tightly that solutions are stable. Small perturbations can't destroy a soliton - it radiates away the perturbation as dispersive waves and returns to its original shape!</p>

                                <h4>Inverse Scattering Transform</h4>

                                <p>The KdV equation can be solved using the <strong>inverse scattering transform (IST)</strong> - a powerful technique analogous to the Fourier transform but for nonlinear PDEs!</p>

                                <p><strong>How IST works:</strong></p>
                                <ol>
                                    <li>Map nonlinear PDE to a linear spectral problem</li>
                                    <li>Solve the linear problem (easy!)</li>
                                    <li>Use "inverse scattering" to reconstruct the solution</li>
                                </ol>

                                <p>IST reveals that solitons correspond to <strong>discrete eigenvalues</strong>, while dispersive waves correspond to the continuous spectrum. This explains why solitons are stable - discrete eigenvalues don't change under evolution!</p>

                                <h3>Other Soliton Equations</h3>

                                <p>Many important PDEs admit soliton solutions:</p>

                                <h4>Sine-Gordon Equation</h4>

                                <div class="math-block">
                                    $$\frac{\partial^2 \phi}{\partial t^2} - \frac{\partial^2 \phi}{\partial x^2} + \sin\phi = 0$$
                                </div>

                                <p>Originally from differential geometry, this equation appears in:</p>
                                <ul>
                                    <li>Josephson junctions (superconductivity)</li>
                                    <li>Crystal dislocations</li>
                                    <li>Domain walls in field theory</li>
                                </ul>

                                <p><strong>Kink solution:</strong> $\phi(x,t) = 4\arctan(e^{\gamma(x-vt)})$ where $\gamma = 1/\sqrt{1-v^2}$</p>

                                <p>This is a topological soliton - it connects two different vacuum states ($\phi = 0$ and $\phi = 2\pi$).</p>

                                <h4>Nonlinear Schr√∂dinger (NLS) Equation</h4>

                                <div class="math-block">
                                    $$i\frac{\partial \psi}{\partial t} + \frac{\partial^2 \psi}{\partial x^2} + |\psi|^2\psi = 0$$
                                </div>

                                <p>Describes wave packets in dispersive media:</p>
                                <ul>
                                    <li>Light pulses in optical fibers (fiber optics)</li>
                                    <li>Bose-Einstein condensates</li>
                                    <li>Water waves in deep water</li>
                                    <li>Plasma waves</li>
                                </ul>

                                <p><strong>Soliton solution:</strong> $\psi(x,t) = A\text{sech}(Ax)e^{i(A^2 t/2)}$ - a bright soliton (localized peak)</p>

                                <p>Also admits <strong>dark solitons</strong> (localized dips on constant background).</p>

                                <div class="info-box">
                                    <p><strong>Applications in Fiber Optics:</strong> Optical solitons in fiber optic cables can travel thousands of kilometers without spreading! This is crucial for long-distance telecommunications. The soliton pulse maintains its shape despite dispersion and nonlinearity in the fiber. Companies use soliton-based transmission to achieve high data rates over transoceanic cables.</p>
                                </div>

                                <h3>Solitons in Fluid Dynamics</h3>

                                <p>While our Gravitation¬≥ fluid simulations primarily show turbulent flows, solitons appear in specific fluid contexts:</p>

                                <h4>Internal Waves in Stratified Fluids</h4>

                                <p>In oceans and atmospheres with density stratification, <strong>internal solitary waves</strong> can form at interfaces between fluid layers. These are governed by KdV-like equations.</p>

                                <p><strong>Observations:</strong></p>
                                <ul>
                                    <li>Andaman Sea: Internal solitons visible from space (surface signature)</li>
                                    <li>Straits of Gibraltar: Generated by tidal flow over sills</li>
                                    <li>Atmosphere: Undular bores and morning glory clouds</li>
                                </ul>

                                <h4>Rossby Solitons (Atmospheric)</h4>

                                <p>Large-scale atmospheric waves (Rossby waves) can form soliton-like structures. These affect weather patterns - blocking highs that persist for days are related to Rossby wave dynamics!</p>

                                <div class="info-box">
                                    <p><strong>Connection to Our Simulations:</strong> While Gravitation¬≥ doesn't currently have a dedicated soliton simulation, soliton mathematics underpins understanding of coherent structures in fluids!<br><br>
                                    <strong>Coherent structures in turbulence:</strong> Vortices in turbulent flows (which you see in Von K√°rm√°n, Kelvin-Helmholtz, etc.) are "quasi-solitons" - they maintain coherence despite surrounding chaos.<br><br>
                                    <strong>Wave packets:</strong> Localized disturbances in fluid flows can behave soliton-like under certain conditions.<br><br>
                                    Understanding solitons helps explain why some structures persist in chaotic flows while others quickly dissipate!</p>
                                </div>

                                <h3>The Inverse Scattering Transform (IST)</h3>

                                <p>The <strong>inverse scattering transform</strong> is to nonlinear PDEs what the Fourier transform is to linear PDEs - a method for exactly solving integrable equations.</p>

                                <p><strong>IST Procedure for KdV:</strong></p>

                                <ol>
                                    <li><strong>Direct scattering:</strong> Given initial condition $u(x, 0)$, solve associated linear Schr√∂dinger equation to find scattering data</li>
                                    <li><strong>Time evolution:</strong> Scattering data evolves simply (linearly!) in time</li>
                                    <li><strong>Inverse scattering:</strong> Reconstruct $u(x, t)$ from evolved scattering data</li>
                                </ol>

                                <p>This transforms the nonlinear KdV into linear problems! The "nonlinear Fourier modes" are:</p>
                                <ul>
                                    <li><strong>Discrete spectrum:</strong> Bound states ‚Üí solitons (particle-like)</li>
                                    <li><strong>Continuous spectrum:</strong> Scattering states ‚Üí dispersive radiation (wave-like)</li>
                                </ul>

                                <p>This explains soliton-radiation decomposition: any initial condition evolves into a finite number of solitons plus dispersive tail!</p>

                                <h3>Applications and Modern Research</h3>

                                <p>Soliton theory has exploded into diverse fields:</p>

                                <h4>Fiber Optic Communications</h4>

                                <p>Optical solitons revolutionized telecommunications:</p>
                                <ul>
                                    <li>Pulses maintain shape over transoceanic distances</li>
                                    <li>No dispersion compensation needed</li>
                                    <li>Higher bit rates possible</li>
                                    <li>Soliton collisions don't corrupt data</li>
                                </ul>

                                <h4>Bose-Einstein Condensates</h4>

                                <p>Ultra-cold atoms in BECs support matter-wave solitons:</p>
                                <ul>
                                    <li>Bright solitons in attractive interactions</li>
                                    <li>Dark solitons in repulsive interactions</li>
                                    <li>Vortex solitons (rotating states)</li>
                                </ul>

                                <p>These can be created, controlled, and observed experimentally!</p>

                                <h4>Tsunami Waves</h4>

                                <p>Tsunamis in shallow water can behave as solitary waves governed by KdV-like equations. Understanding soliton dynamics helps predict tsunami propagation and impact.</p>

                                <h4>Condensed Matter Physics</h4>

                                <ul>
                                    <li><strong>Fluxons:</strong> Magnetic flux quanta in superconductors (Josephson junctions)</li>
                                    <li><strong>Spin waves:</strong> Solitons in magnetic chains</li>
                                    <li><strong>Polarons:</strong> Electron-phonon bound states</li>
                                    <li><strong>Domain walls:</strong> Topological defects separating regions</li>
                                </ul>

                                <h3>Why Solitons Matter</h3>

                                <p>Solitons represent a profound insight: <strong>nonlinearity need not mean chaos!</strong> While turbulence shows how nonlinearity creates complexity and randomness, solitons show how nonlinearity can create perfect order and stability.</p>

                                <p>Key lessons:</p>

                                <ul>
                                    <li><strong>Balance is possible:</strong> Competing effects (dispersion/nonlinearity) can achieve perfect equilibrium</li>
                                    <li><strong>Integrability exists:</strong> Some nonlinear systems are exactly solvable (infinite conservation laws)</li>
                                    <li><strong>Coherent structures:</strong> Localized, persistent features can emerge from fluid equations</li>
                                    <li><strong>Particle-wave duality:</strong> Classical waves can exhibit particle-like properties</li>
                                </ul>

                                <p>The mathematics of solitons - inverse scattering, infinite conservation laws, integrable systems - has enriched pure mathematics (algebraic geometry, representation theory) as much as applied physics!</p>

                                <div class="info-box">
                                    <p><strong>Connection to Fluid Simulations:</strong> Understanding solitons helps interpret coherent structures in Gravitation¬≥ fluid simulations:<br><br>
                                    <strong>Von K√°rm√°n vortex street:</strong> Vortices are "quasi-solitons" - coherent structures that persist despite turbulent background<br><br>
                                    <strong>Kelvin-Helmholtz instability:</strong> Initial waves evolve into vortex structures that maintain coherence<br><br>
                                    <strong>Jet flows:</strong> Can support solitary wave packets under certain conditions<br><br>
                                    While these aren't exact solitons (the Navier-Stokes equations aren't integrable), the stability mechanisms share conceptual similarities. The balance between nonlinear steepening, dispersion/diffusion, and other effects determines whether structures persist or dissolve.</p>
                                </div>

                                <h3>From Canals to Cosmos</h3>

                                <p>From John Scott Russell's 1834 observation in a canal to modern fiber optic networks spanning the globe, soliton theory has come full circle. What started as a curious anomaly became a central concept in nonlinear science.</p>

                                <p>Solitons teach us that the universe has richer possibilities than simple linear superposition. In the interplay between dispersion‚Äã and nonlinearity, nature finds exquisite balance - creating waves that neither spread nor break, but propagate eternally unchanged.</p>

                                <p>As mathematician Martin Kruskal, one of the pioneers of soliton theory, said: "The soliton phenomenon is one of the great discoveries of the twentieth century. It shows that deterministic nonlinearity need not imply chaotic complexity - it can instead produce remarkable order and simplicity."</p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 5.9 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">5.9</span>
                            <span>Ergodic Theory & Statistical Mechanics</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>Ergodic Theory: Bridging Dynamics and Statistics</h2>
                                
                                <p><strong>Ergodic theory</strong> is the mathematical framework connecting the microscopic dynamics of individual trajectories to the macroscopic behavior of statistical ensembles. It answers a fundamental question: When can time averages (following one trajectory) equal ensemble averages (over many initial conditions)?</p>

                                <p>The word "ergodic" comes from Greek: <em>ergon</em> (work) + <em>hodos</em> (path). It was introduced by physicist Ludwig Boltzmann in the 1870s while developing statistical mechanics. The ergodic hypothesis states that, over long times, a system explores all accessible microstates with equal probability.</p>

                                <h3>The Ergodic Hypothesis</h3>

                                <p>Consider a dynamical system with state space $\Omega$ and time evolution $T^t: \Omega \to \Omega$. For an observable $f: \Omega \to \mathbb{R}$, we can compute two types of averages:</p>

                                <p><strong>Time average</strong> (following one trajectory starting at $x$):</p>
                                <div class="math-block">
                                    $$\langle f \rangle_{\text{time}} = \lim_{T \to \infty} \frac{1}{T}\int_0^T f(T^t x) \, dt$$
                                </div>

                                <p><strong>Ensemble average</strong> (averaging over initial conditions with measure $\mu$):</p>
                                <div class="math-block">
                                    $$\langle f \rangle_{\text{ensemble}} = \int_\Omega f(x) \, d\mu(x)$$
                                </div>

                                <p>The system is <strong>ergodic</strong> if these averages are equal for almost all initial conditions $x$:</p>

                                <div class="math-block">
                                    $$\langle f \rangle_{\text{time}} = \langle f \rangle_{\text{ensemble}}$$
                                </div>

                                <p><strong>Physical meaning:</strong> A single long trajectory samples the entire accessible phase space in proportion to the equilibrium measure. You don't need to measure many particles - one particle observed over long time gives the same statistics!</p>

                                <div class="example-box">
                                    <p><strong>Example - Coin Flips:</strong> Flip a coin many times and record the fraction of heads:<br><br>
                                    <strong>Time average:</strong> Flip one coin 10,000 times ‚Üí ~50% heads<br>
                                    <strong>Ensemble average:</strong> Flip 10,000 coins once ‚Üí ~50% heads<br><br>
                                    The system is ergodic - both averages converge to the same value (1/2).</p>
                                </div>

                                <h3>Ergodicity and Mixing</h3>

                                <p>Ergodicity is the weakest form of randomness in deterministic systems. Stronger properties include:</p>

                                <h4>Weak Mixing</h4>

                                <p>A system is <strong>weakly mixing</strong> if correlations decay on average:</p>

                                <div class="math-block">
                                    $$\lim_{T \to \infty} \frac{1}{T}\int_0^T \left|\int_\Omega f(T^t x)g(x) \, d\mu - \int_\Omega f \, d\mu \int_\Omega g \, d\mu\right| dt = 0$$
                                </div>

                                <p>This means that over long times, knowing the initial state gives no information about the future state on average.</p>

                                <h4>Strong Mixing (Mixing)</h4>

                                <p>A system is <strong>mixing</strong> if correlations decay for every time:</p>

                                <div class="math-block">
                                    $$\lim_{t \to \infty} \int_\Omega f(T^t x)g(x) \, d\mu = \int_\Omega f \, d\mu \int_\Omega g \, d\mu$$
                                </div>

                                <p>This is stronger - correlations always decay, not just on average. Think of mixing cream in coffee: initially cream and coffee are separate (correlated), but after stirring (time evolution), they're uniformly mixed (uncorrelated).</p>

                                <p><strong>Hierarchy:</strong> Mixing ‚Üí Weak mixing ‚Üí Ergodic ‚Üí Recurrent</p>

                                <p>Each property is strictly stronger than the next. Most "chaotic" systems (like the Lorenz attractor) are mixing on their attractors.</p>

                                <div class="info-box">
                                    <p><strong>In Chaotic Attractors:</strong> The Lorenz and R√∂ssler attractors in Gravitation¬≥ are ergodic and mixing!<br><br>
                                    <strong>Ergodicity:</strong> One long trajectory densely fills the attractor - time average equals space average<br>
                                    <strong>Mixing:</strong> Two initially nearby points eventually become uncorrelated - their separation becomes independent of initial distance<br><br>
                                    This is why we can compute fractal dimensions and Lyapunov exponents from single long trajectories!</p>
                                </div>

                                <h3>Invariant Measures</h3>

                                <p>An <strong>invariant measure</strong> $\mu$ is a probability measure on phase space that doesn't change under time evolution:</p>

                                <div class="math-block">
                                    $$\mu(T^{-t}A) = \mu(A)$$
                                </div>

                                <p>for all measurable sets $A$ and times $t$. Physically, this represents an equilibrium probability distribution.</p>

                                <h4>Types of Invariant Measures</h4>

                                <p><strong>1. Microcanonical measure:</strong> Uniform over energy surface (isolated systems)</p>
                                <p><strong>2. Canonical measure:</strong> Boltzmann distribution $\propto e^{-\beta E}$ (systems in heat bath)</p>
                                <p><strong>3. Natural measure:</strong> The "physical" measure followed by typical trajectories</p>
                                <p><strong>4. SRB measure:</strong> Sinai-Ruelle-Bowen measure for dissipative systems</p>

                                <p>For Hamiltonian systems, <strong>Liouville's theorem</strong> says phase space volume is preserved, giving the natural invariant measure. For dissipative systems like the Lorenz attractor, the SRB measure concentrates on the attractor (zero volume in phase space) with fractal structure.</p>

                                <div class="example-box">
                                    <p><strong>Example - Circle Rotation:</strong> Consider the map $\theta_{n+1} = \theta_n + \alpha \pmod{2\pi}$ (rotation by angle $\alpha$):<br><br>
                                    <strong>If Œ±/2œÄ is irrational:</strong> The orbit densely fills the circle. The uniform measure $\mu(A) = |A|/2\pi$ is invariant and ergodic. Time average equals space average.<br><br>
                                    <strong>If Œ±/2œÄ is rational (Œ±/2œÄ = p/q):</strong> The orbit is periodic with period $q$. The system isn't ergodic - time average depends on starting point!</p>
                                </div>

                                <h3>Poincar√© Recurrence Theorem</h3>

                                <p><strong>Poincar√©'s recurrence theorem</strong>: For any measure-preserving system with finite total measure, almost every trajectory returns arbitrarily close to its initial condition infinitely often.</p>

                                <p>Mathematically, for any set $A$ with $\mu(A) > 0$, almost every point in $A$ returns to $A$ infinitely many times:</p>

                                <div class="math-block">
                                    $$\mu\{x \in A : T^n x \in A \text{ for infinitely many } n\} = \mu(A)$$
                                </div>

                                <p><strong>Physical implications:</strong></p>
                                <ul>
                                    <li>In a finite phase space, systems return arbitrarily close to initial state</li>
                                    <li>This happens for *almost all* initial conditions</li>
                                    <li>Recurrence time can be astronomically long!</li>
                                    <li>Explains why gases don't spontaneously unmix (but technically they could... eventually)</li>
                                </ul>

                                <div class="example-box">
                                    <p><strong>Poincar√© Recurrence Paradox:</strong> If a gas in a box must return arbitrarily close to its initial state, why does entropy increase?<br><br>
                                    <strong>Resolution:</strong> Recurrence time grows exponentially with system size. For macroscopic systems (Avogadro's number of particles), recurrence time exceeds the age of the universe by unimaginable factors! Practically, entropy increase is irreversible even though recurrence is guaranteed mathematically.</p>
                                </div>

                                <h3>Ergodic Theory in Statistical Mechanics</h3>

                                <p>Statistical mechanics relies on ergodic theory to justify using ensemble averages to predict experimental outcomes (which are time averages).</p>

                                <h4>Boltzmann's Ergodic Hypothesis</h4>

                                <p>Boltzmann's original hypothesis (too strong, actually false): A Hamiltonian system's trajectory passes through <em>every</em> point on the energy surface.</p>

                                <p><strong>Problem:</strong> Mathematically impossible! A continuous trajectory is 1-dimensional; energy surface is $(2N-1)$-dimensional. Measure-theoretically, the trajectory has measure zero!</p>

                                <p><strong>Modern formulation:</strong> The trajectory comes <em>arbitrarily close</em> to every point with probability one (ergodic hypothesis), and time averages equal microcanonical ensemble averages.</p>

                                <h4>Microcanonical vs. Canonical Ensembles</h4>

                                <p><strong>Microcanonical ensemble:</strong> Isolated system with fixed energy $E$. All microstates with energy between $E$ and $E + \delta E$ are equally probable. This is the natural setting for ergodic theory.</p>

                                <p><strong>Canonical ensemble:</strong> System in thermal contact with heat reservoir at temperature $T$. Probability $\propto e^{-E/k_BT}$ (Boltzmann distribution).</p>

                                <p>For large systems, both ensembles give equivalent predictions (ensemble equivalence). This is another manifestation of ergodicity - fluctuations become negligible!</p>

                                <h4>Time vs. Ensemble Averages in Practice</h4>

                                <p>In molecular dynamics simulations, we typically:</p>
                                <ol>
                                    <li>Compute one long trajectory (time average)</li>
                                    <li>Assume ergodicity</li>
                                    <li>Interpret results as ensemble averages</li>
                                    <li>Compare with experimental thermodynamic quantities</li>
                                </ol>

                                <p>This works because:
                                <ul>
                                    <li>Systems with many degrees of freedom are typically mixing</li>
                                    <li>Chaotic dynamics ensures thorough phase space exploration</li>
                                    <li>Recurrence times are long enough that system appears ergodic on observable timescales</li>
                                </ul>
                                </p>

                                <div class="info-box">
                                    <p><strong>Connection to Simulations:</strong> When computing average properties in our simulations, we're relying on ergodicity!<br><br>
                                    <strong>Three-body problem:</strong> To compute average distance between bodies, we could simulate one trajectory for long time or many trajectories for short time - ergodicity says they're equivalent<br><br>
                                    <strong>Chaotic attractors:</strong> Fractal dimension computed from one trajectory equals expectation over ensemble<br><br>
                                    <strong>Turbulent flows:</strong> Time-averaged flow statistics equal ensemble averages (Reynolds decomposition relies on this!)</p>
                                </div>

                                <h3>Breakdown of Ergodicity</h3>

                                <p>Not all systems are ergodic! Non-ergodic behavior occurs when:</p>

                                <h4>KAM Tori (Integrable Regions)</h4>

                                <p>In near-integrable Hamiltonian systems, KAM tori act as barriers. Trajectories starting on different tori never mix - the system isn't ergodic on the whole energy surface, only within connected regions.</p>

                                <h4>Glassy Systems</h4>

                                <p>In spin glasses and structural glasses at low temperature, the system gets trapped in local energy minima. Relaxation times exceed experimental (and computational) timescales - ergodicity is broken practically even if it holds theoretically.</p>

                                <h4>Many-Body Localization</h4>

                                <p>Recent discovery: certain strongly disordered quantum systems fail to thermalize. They remain in non-ergodic states, violating the eigenstate thermalization hypothesis.</p>

                                <h3>Long-Term Statistics of Chaotic Systems</h3>

                                <p>Chaotic systems, despite being deterministic and sensitive to initial conditions, have well-defined statistical properties!</p>

                                <h4>Natural Measure and Observable Statistics</h4>

                                <p>For chaotic attractors, there exists a <strong>natural measure</strong> (SRB measure) that describes long-term statistics. Any observable $f$ has well-defined statistics:</p>

                                <p><strong>Mean:</strong> $\langle f \rangle = \int f \, d\mu_{\text{SRB}}$</p>

                                <p><strong>Variance:</strong> $\sigma_f^2 = \int (f - \langle f \rangle)^2 \, d\mu_{\text{SRB}}$</p>

                                <p><strong>Probability distribution:</strong> Well-defined histogram of values</p>

                                <p>These are independent of initial conditions (for almost all initial conditions on the attractor)!</p>

                                <div class="example-box">
                                    <p><strong>Lorenz Attractor Statistics:</strong> For the Lorenz system with standard parameters:<br>
                                    ‚Ä¢ The $z$ coordinate has well-defined mean: $\langle z \rangle = \rho - 1 = 27$<br>
                                    ‚Ä¢ The switching time between lobes has an approximately exponential distribution<br>
                                    ‚Ä¢ All statistical properties are reproducible, despite chaos!<br>
                                    ‚Ä¢ Different initial conditions give same long-term statistics (ergodicity)</p>
                                </div>

                                <h4>Central Limit Theorem for Dynamical Systems</h4>

                                <p>Remarkably, chaotic systems often satisfy a <strong>central limit theorem</strong>: suitable observables, when summed along trajectories, converge to Gaussian distributions!</p>

                                <p>For mixing systems with sufficient decay of correlations:</p>

                                <div class="math-block">
                                    $$\frac{1}{\sqrt{N}}\sum_{i=0}^{N-1} \left[f(T^i x) - \langle f \rangle\right] \xrightarrow{d} \mathcal{N}(0, \sigma^2)$$
                                </div>

                                <p>This explains why macroscopic observables (sums over many particles/degrees of freedom) have Gaussian fluctuations, even though microscopic dynamics is deterministic chaos!</p>

                                <h3>Applications and Modern Developments</h3>

                                <p><strong>Climate Science:</strong> Long-term climate statistics (ensemble averages) can be computed from long time series (time averages) if climate system is ergodic on relevant timescales.</p>

                                <p><strong>Molecular Dynamics:</strong> Thermodynamic properties computed from single trajectories using ergodicity assumption.</p>

                                <p><strong>Turbulence:</strong> Reynolds averaging relies on ergodicity - time-averaged turbulent statistics equal ensemble averages.</p>

                                <p><strong>Financial Markets:</strong> Ergodicity economics questions whether financial time series are ergodic - crucial for risk assessment!</p>

                                <div class="info-box">
                                    <p><strong>The Deep Connection:</strong> Ergodic theory unifies:<br>
                                    ‚Ä¢ <strong>Dynamics:</strong> Individual trajectories evolving deterministically<br>
                                    ‚Ä¢ <strong>Statistics:</strong> Probability distributions and ensemble averages<br>
                                    ‚Ä¢ <strong>Thermodynamics:</strong> Macroscopic observables and equilibrium<br><br>
                                    It's the mathematical bridge explaining how deterministic microscopic laws give rise to statistical macroscopic behavior - the foundation of statistical mechanics!</p>
                                </div>

                                <h3>Summary: Why Ergodic Theory Matters</h3>

                                <p>Ergodic theory answers fundamental questions about deterministic systems:</p>

                                <ul>
                                    <li><strong>When can we replace time averages with ensemble averages?</strong> When the system is ergodic!</li>
                                    <li><strong>Why do chaotic systems have reproducible statistics?</strong> Because they're mixing - correlations decay!</li>
                                    <li><strong>Why does statistical mechanics work?</strong> Because physical systems are ergodic on energy surfaces!</li>
                                    <li><strong>How do we justify molecular dynamics simulations?</strong> By assuming ergodicity to connect finite-time simulations to infinite-time behavior!</li>
                                </ul>

                                <p>The mathematical framework of ergodic theory provides rigorous foundations for statistical physics, justifies computational methods in molecular simulations, and explains why deterministic chaos produces statistical regularity. It's one of the most beautiful and practically important areas of mathematics!</p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Subsection 5.10 -->
                <div class="subsection-accordion">
                    <div class="subsection-header">
                        <div class="subsection-title">
                            <span class="subsection-number">5.10</span>
                            <span>Perturbation Theory</span>
                        </div>
                        <span class="subsection-arrow">‚ñ∂</span>
                    </div>
                    <div class="subsection-content">
                        <div class="subsection-inner">
                            <div class="docs-section">
                                <h2>The Art of Small Parameters</h2>
                                
                                <p><strong>Perturbation theory</strong> is a powerful collection of techniques for studying systems that are "close" to simpler, exactly solvable problems. The key insight: if a system differs from a solvable one by a small amount, we can systematically approximate the solution as a series in the small parameter.</p>

                                <p>Think of it as asking: "I can solve problem A exactly. If problem B is like problem A but with a small difference, can I use my solution to A to understand B?" Perturbation theory provides a systematic framework for answering yes!</p>

                                <h3>The Basic Setup</h3>

                                <p>Consider a problem depending on a parameter $\epsilon$:</p>

                                <div class="math-block">
                                    $$L_\epsilon[u] = f$$
                                </div>

                                <p>where $L_\epsilon$ is some operator (differential, algebraic, etc.). We know how to solve the <strong>unperturbed problem</strong> ($\epsilon = 0$):</p>

                                <div class="math-block">
                                    $$L_0[u_0] = f$$
                                </div>

                                <p><strong>The perturbation question:</strong> For small $\epsilon \neq 0$, can we find $u_\epsilon$ as a series?</p>

                                <div class="math-block">
                                    $$u_\epsilon = u_0 + \epsilon u_1 + \epsilon^2 u_2 + \cdots$$
                                </div>

                                <p>where $u_0$ is the known solution and $u_1, u_2, \ldots$ are corrections we need to find.</p>

                                <div class="example-box">
                                    <p><strong>Physical Example - Planetary Orbits:</strong><br>
                                    <strong>Unperturbed:</strong> Two-body problem (Sun + one planet) ‚Üí exactly solvable, elliptical orbit<br>
                                    <strong>Perturbation:</strong> Add effect of Jupiter (small compared to Sun)<br>
                                    <strong>Solution:</strong> True orbit = Keplerian ellipse + perturbative corrections<br><br>
                                    This is how astronomers predict planetary positions to high accuracy!</p>
                                </div>

                                <h3>Regular Perturbation Theory</h3>

                                <p><strong>Regular perturbations</strong> are "well-behaved" - the perturbed solution remains smooth and close to the unperturbed one. The series expansion converges uniformly.</p>

                                <h4>Algebraic Example</h4>

                                <p>Solve: $(1 + \epsilon)x^2 - 1 = 0$ for small $\epsilon$</p>

                                <p><strong>Unperturbed solution</strong> ($\epsilon = 0$): $x_0 = 1$ (taking positive root)</p>

                                <p><strong>Perturbed solution:</strong> Assume $x = x_0 + \epsilon x_1 + \epsilon^2 x_2 + \cdots$</p>

                                <p>Substitute and collect powers of $\epsilon$:</p>

                                <div class="math-block">
                                    $$(1 + \epsilon)(x_0 + \epsilon x_1 + \cdots)^2 - 1 = 0$$
                                </div>

                                <p><strong>Order $\epsilon^0$:</strong> $x_0^2 - 1 = 0 \Rightarrow x_0 = 1$ ‚úì</p>

                                <p><strong>Order $\epsilon^1$:</strong> $2x_0 x_1 + x_0^2 = 0 \Rightarrow x_1 = -\frac{1}{2}$</p>

                                <p><strong>Order $\epsilon^2$:</strong> $2x_0 x_2 + x_1^2 + 2x_0 x_1 = 0 \Rightarrow x_2 = -\frac{1}{8}$</p>

                                <p>Solution: $x \approx 1 - \frac{\epsilon}{2} - \frac{\epsilon^2}{8} + \cdots$</p>

                                <p>Compare with exact solution: $x = \frac{1}{\sqrt{1 + \epsilon}} = 1 - \frac{\epsilon}{2} + \frac{3\epsilon^2}{8} - \cdots$ (Taylor series). First two terms match!</p>

                                <h4>Differential Equation Example</h4>

                                <p>Consider: $\frac{d^2 y}{dx^2} + (1 + \epsilon x)y = 0$ with $y(0) = 1$, $y'(0) = 0$</p>

                                <p><strong>Unperturbed</strong> ($\epsilon = 0$): $y'' + y = 0 \Rightarrow y_0 = \cos x$</p>

                                <p><strong>Perturbed solution:</strong> $y = y_0 + \epsilon y_1 + \epsilon^2 y_2 + \cdots$</p>

                                <p>Substitute and match powers of $\epsilon$:</p>

                                <p><strong>Order $\epsilon^1$:</strong> $y_1'' + y_1 = -xy_0 = -x\cos x$</p>

                                <p>Solve using variation of parameters: $y_1 = \frac{x\sin x}{2} - \frac{x^2\cos x}{4}$</p>

                                <p>First-order solution: $y \approx \cos x + \epsilon\left(\frac{x\sin x}{2} - \frac{x^2\cos x}{4}\right)$</p>

                                <div class="info-box">
                                    <p><strong>When Regular Perturbation Works:</strong> When the perturbation doesn't fundamentally change the character of the solution. Small changes to coefficients, source terms, or forcing usually yield regular perturbations.</p>
                                </div>

                                <h3>Singular Perturbation Theory</h3>

                                <p><strong>Singular perturbations</strong> occur when the small parameter multiplies the highest derivative or otherwise fundamentally changes the problem's character. The naive series expansion breaks down!</p>

                                <h4>The Boundary Layer Problem</h4>

                                <p>Classic example: $\epsilon \frac{d^2y}{dx^2} + \frac{dy}{dx} - y = 0$ on $x \in [0, 1]$ with $y(0) = 0$, $y(1) = 1$</p>

                                <p><strong>Problem:</strong> As $\epsilon \to 0$, this second-order ODE becomes first-order! We can't satisfy two boundary conditions with a first-order equation. The perturbation is <em>singular</em>.</p>

                                <p><strong>Exact solution:</strong> $y = \frac{e^{x/\epsilon} - 1}{e^{1/\epsilon} - 1} + \frac{1 - e^{(x-1)/\epsilon}}{1 - e^{-1/\epsilon}}$</p>

                                <p>For small $\epsilon$, this has two regions:</p>
                                <ul>
                                    <li><strong>Outer region:</strong> Most of domain, $y \approx 1$ (satisfies reduced equation)</li>
                                    <li><strong>Boundary layer near $x = 0$:</strong> Width $O(\epsilon)$, rapid transition from $y = 0$ to $y ‚âà 1$</li>
                                </ul>

                                <h4>Method of Matched Asymptotic Expansions</h4>

                                <p>To solve singular perturbation problems:</p>

                                <ol>
                                    <li><strong>Outer expansion:</strong> Series valid away from boundary layer
                                        $$y_{\text{outer}} = y_0(x) + \epsilon y_1(x) + \cdots$$
                                    </li>
                                    <li><strong>Inner expansion:</strong> Series in stretched coordinate $\xi = x/\epsilon$
                                        $$y_{\text{inner}} = Y_0(\xi) + \epsilon Y_1(\xi) + \cdots$$
                                    </li>
                                    <li><strong>Matching:</strong> Require inner and outer solutions to agree in overlap region
                                        $$\lim_{\xi \to \infty} Y(\xi) = \lim_{x \to 0} y(x)$$
                                    </li>
                                    <li><strong>Composite solution:</strong> Combine inner and outer, subtract overlap
                                        $$y_{\text{composite}} = y_{\text{inner}} + y_{\text{outer}} - y_{\text{overlap}}$$
                                    </li>
                                </ol>

                                <div class="example-box">
                                    <p><strong>Physical Example - Fluid Boundary Layers:</strong> Flow past an airplane wing at high Reynolds number ($Re = VL/\nu \gg 1$):<br><br>
                                    ‚Ä¢ <strong>Outer flow:</strong> Inviscid (Euler equations), smooth<br>
                                    ‚Ä¢ <strong>Boundary layer:</strong> Thin region near wing where viscosity matters, thickness $\sim L/\sqrt{Re}$<br>
                                    ‚Ä¢ <strong>Matching:</strong> Velocity in outer flow must match velocity at edge of boundary layer<br><br>
                                    This is Prandtl's boundary layer theory - one of the great triumphs of singular perturbation theory!</p>
                                </div>

                                <h3>Multiple Scale Analysis</h3>

                                <p><strong>Multiple scales</strong> arise when a system has phenomena occurring on different timescales (fast oscillations + slow drift, rapid transients + slow evolution, etc.).</p>

                                <p><strong>Key idea:</strong> Introduce multiple independent time variables:
                                <ul>
                                    <li>Fast time: $\tau = t$ (oscillations)</li>
                                    <li>Slow time: $T = \epsilon t$ (amplitude modulation)</li>
                                </ul>
                                </p>

                                <p>Treat $\tau$ and $T$ as independent variables, even though $T = \epsilon t$:</p>

                                <div class="math-block">
                                    $$\frac{d}{dt} = \frac{\partial}{\partial \tau} + \epsilon \frac{\partial}{\partial T}$$
                                </div>

                                <h4>Example: Weakly Damped Oscillator</h4>

                                <p>Consider: $\frac{d^2x}{dt^2} + \omega^2 x = -\epsilon \frac{dx}{dt}$ (weak damping)</p>

                                <p>Naive perturbation fails (secular terms grow unbounded). Use multiple scales:</p>

                                <p>Let $x(t) = x_0(\tau, T) + \epsilon x_1(\tau, T) + \cdots$ where $\tau = t$, $T = \epsilon t$</p>

                                <p><strong>Order $\epsilon^0$:</strong> $\frac{\partial^2 x_0}{\partial \tau^2} + \omega^2 x_0 = 0$</p>

                                <p>Solution: $x_0 = A(T)e^{i\omega\tau} + \text{c.c.}$ where $A(T)$ is amplitude (slowly varying)</p>

                                <p><strong>Order $\epsilon^1$:</strong> Solvability condition eliminates secular terms, giving evolution equation for $A(T)$:
                                $$\frac{dA}{dT} = -\frac{A}{2}$$
                                </p>

                                <p>Solution: $A(T) = A_0 e^{-T/2} = A_0 e^{-\epsilon t/2}$</p>

                                <p>Final solution: $x(t) \approx A_0 e^{-\epsilon t/2}\cos(\omega t)$</p>

                                <p>This describes oscillations with slowly decaying amplitude - exactly what we expect from weak damping! The multiple scale analysis avoids secular terms and gives a uniformly valid approximation.</p>

                                <div class="info-box">
                                    <p><strong>When to Use Multiple Scales:</strong> When the problem involves phenomena on widely separated timescales or length scales. Examples include:<br>
                                    ‚Ä¢ Slowly varying amplitude in oscillators<br>
                                    ‚Ä¢ Nonlinear wave modulation<br>
                                    ‚Ä¢ Boundary layers with multiple regions<br>
                                    ‚Ä¢ Climate dynamics (fast weather + slow climate drift)</p>
                                </div>

                                <h3>Weakly Nonlinear Theory</h3>

                                <p><strong>Weakly nonlinear theory</strong> studies systems that are nearly linear, with small nonlinearities. These appear throughout physics when systems operate near equilibrium or with small amplitudes.</p>

                                <h4>Amplitude Equations</h4>

                                <p>For weakly nonlinear oscillators, the <strong>Stuart-Landau equation</strong> describes amplitude evolution:</p>

                                <div class="math-block">
                                    $$\frac{dA}{dt} = \sigma A - \ell|A|^2 A$$
                                </div>

                                <p>where $A$ is the complex amplitude, $\sigma$ is the linear growth rate, and $\ell$ is the nonlinear saturation coefficient.</p>

                                <p><strong>Behavior:</strong></p>
                                <ul>
                                    <li>$\sigma < 0$: Amplitude decays to zero (stable fixed point)</li>
                                    <li>$\sigma > 0$: Amplitude grows then saturates at $|A| = \sqrt{\sigma/\text{Re}(\ell)}$ (limit cycle)</li>
                                </ul>

                                <p>This describes the Hopf bifurcation! The weakly nonlinear analysis captures amplitude saturation that linear theory misses.</p>

                                <h4>Near-Equilibrium Behavior</h4>

                                <p>Many systems exhibit interesting dynamics when slightly displaced from equilibrium. Weakly nonlinear theory systematically expands around the equilibrium state.</p>

                                <p><strong>Example - Rayleigh-B√©nard Convection:</strong> Fluid heated from below undergoes convection when temperature difference exceeds a critical value.</p>

                                <p>Rayleigh number: $Ra = \frac{g\alpha\Delta T L^3}{\nu\kappa}$ (controls heating strength)</p>

                                <ul>
                                    <li>$Ra < Ra_c$: No convection (conduction only)</li>
                                    <li>$Ra$ slightly above $Ra_c$: Weakly nonlinear convection rolls</li>
                                    <li>$Ra \gg Ra_c$: Fully nonlinear turbulent convection</li>
                                </ul>

                                <p>Weakly nonlinear theory near $Ra_c$ predicts roll pattern, wavelength, and heat transfer!</p>

                                <div class="info-box">
                                    <p><strong>In Gravitation¬≥ Simulations:</strong> The Rayleigh-B√©nard simulation shows this transition! At low Rayleigh numbers, you'd see regular convection rolls (weakly nonlinear regime). At high Rayleigh numbers, the flow becomes chaotic and turbulent. Perturbation theory helps understand the transition!</p>
                                </div>

                                <h3>Applications of Perturbation Theory</h3>

                                <p>Perturbation methods appear throughout physics and engineering:</p>

                                <h4>Quantum Mechanics</h4>

                                <p>Time-independent perturbation theory for perturbed Hamiltonian $H = H_0 + \lambda V$:</p>

                                <p>Energy corrections: $E_n = E_n^{(0)} + \lambda E_n^{(1)} + \lambda^2 E_n^{(2)} + \cdots$</p>

                                <p>First-order: $E_n^{(1)} = \langle n^{(0)}|V|n^{(0)}\rangle$</p>

                                <p>This is how we calculate atomic energy levels in real atoms (unperturbed = hydrogen, perturbation = electron-electron interactions)!</p>

                                <h4>Celestial Mechanics</h4>

                                <p>Orbital perturbations from additional planets, non-spherical planets, general relativity:</p>
                                <ul>
                                    <li>Mercury's perihelion precession (GR perturbation)</li>
                                    <li>Satellite orbit perturbations (Earth's oblateness)</li>
                                    <li>Asteroid orbit evolution (planetary perturbations)</li>
                                </ul>

                                <h4>Fluid Dynamics</h4>

                                <p>Weakly nonlinear theory near instability thresholds:</p>
                                <ul>
                                    <li>Pattern formation in Rayleigh-B√©nard convection</li>
                                    <li>Waves on fluid interfaces</li>
                                    <li>Transition to turbulence</li>
                                </ul>

                                <h4>Nonlinear Optics</h4>

                                <p>Weak nonlinearity in optical media leads to:</p>
                                <ul>
                                    <li>Self-focusing and self-phase modulation</li>
                                    <li>Four-wave mixing</li>
                                    <li>Harmonic generation</li>
                                </ul>

                                <h3>Limitations and Breakdown</h3>

                                <p>Perturbation series don't always converge! They may be:</p>

                                <p><strong>Asymptotic series:</strong> Useful for small $\epsilon$ but diverge for any fixed $\epsilon$ as order increases. Still provide good approximations if truncated appropriately!</p>

                                <p><strong>Radius of convergence:</strong> Series may converge only for $\epsilon < \epsilon_{\text{max}}$. Beyond this, higher-order corrections become unreliable.</p>

                                <p><strong>Secular terms:</strong> Terms growing with time violate the assumption that perturbation stays small. Multiple scales or renormalization group methods needed!</p>

                                <div class="example-box>
                                    <p><strong>When Perturbation Theory Fails:</strong><br>
                                    ‚Ä¢ Near bifurcations or phase transitions (system changes character)<br>
                                    ‚Ä¢ Long-time behavior with secular terms<br>
                                    ‚Ä¢ Strong perturbations ($\epsilon \sim O(1)$)<br>
                                    ‚Ä¢ Chaotic regions where small changes have large effects<br>
                                    ‚Ä¢ Resonances where small forcing produces large response</p>
                                </div>

                                <h3>Summary: The Power and Limits of Perturbation</h3>

                                <p>Perturbation theory is powerful when:</p>
                                <ul>
                                    <li>You have a solvable reference problem</li>
                                    <li>The perturbation is genuinely small</li>
                                    <li>The solution character doesn't change</li>
                                    <li>You're interested in near-equilibrium behavior</li>
                                </ul>

                                <p>But remember:</p>
                                <ul>
                                    <li>Not all interesting physics is perturbative!</li>
                                    <li>Small parameters can have large effects (singular perturbations)</li>
                                    <li>Series may diverge (use asymptotically, not literally)</li>
                                    <li>Need different techniques for strongly nonlinear, far-from-equilibrium phenomena</li>
                                </ul>

                                <div class="info-box">
                                    <p><strong>Connection to Simulations:</strong> Perturbation theory helps understand:<br><br>
                                    <strong>Three-body problem:</strong> Restricted problem uses perturbation theory - small third body perturbs two-body solution<br><br>
                                    <strong>Fluid instabilities:</strong> Weakly nonlinear theory explains pattern selection when instability first develops<br><br>
                                    <strong>Near-equilibrium dynamics:</strong> Small oscillations around equilibrium points in attractor simulations<br><br>
                                    While our simulations often show strongly nonlinear phenomena, perturbation theory provides insight into how complexity emerges from simple starting points!</p>
                                </div>

                                <h3>From Small Changes to Big Insights</h3>

                                <p>Perturbation theory embodies a fundamental scientific principle: understand the simple before tackling the complex. By systematically building on exactly solvable problems, we gain insight into realistic systems.</p>

                                <p>Whether calculating planetary orbits, designing optical fibers, predicting instability thresholds, or understanding quantum mechanics, perturbation theory provides a bridge from idealized models to messy reality. It's a testament to how far careful approximation based on small parameters can take us!</p>

                                <p>As physicist Freeman Dyson noted: "The most important thing a graduate student can learn is: most problems can't be solved exactly, but many can be solved perturbatively. The art lies in knowing when to perturb and how."</p>
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
    </div>

    <!-- JavaScript for Tab and Accordion Functionality -->
    <script>
        const tabs = document.querySelectorAll('.tutorial-tab');
        const contents = document.querySelectorAll('.tutorial-content');

        tabs.forEach(tab => {
            tab.addEventListener('click', () => {
                // Remove active class from all tabs and contents
                tabs.forEach(t => t.classList.remove('active'));
                contents.forEach(c => c.classList.remove('active'));

                // Add active class to clicked tab
                tab.classList.add('active');

                // Show corresponding content
                const targetId = tab.getAttribute('data-tutorial');
                const targetContent = document.getElementById(targetId);
                if (targetContent) {
                    targetContent.classList.add('active');
                }
            });
        });

        // Accordion functionality
        const accordionHeaders = document.querySelectorAll('.subsection-header');

        accordionHeaders.forEach(header => {
            header.addEventListener('click', () => {
                const content = header.nextElementSibling;
                const isActive = header.classList.contains('active');

                // Toggle active state
                header.classList.toggle('active');
                content.classList.toggle('active');
            });
        });

    </script>
</body>
</html>
